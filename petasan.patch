diff -uprN a/drivers/block/rbd.c b/drivers/block/rbd.c
--- a/drivers/block/rbd.c	2024-04-24 01:22:34.568815811 +0200
+++ b/drivers/block/rbd.c	2024-04-24 01:23:17.731512539 +0200
@@ -47,6 +47,7 @@
 #include <linux/slab.h>
 #include <linux/idr.h>
 #include <linux/workqueue.h>
+#include <linux/delay.h>
 
 #include "rbd_types.h"
 
@@ -237,6 +238,7 @@ struct rbd_obj_request {
 enum img_req_flags {
 	IMG_REQ_CHILD,		/* initiator: block = 0, child image = 1 */
 	IMG_REQ_LAYERED,	/* ENOENT handling: normal = 0, layered = 1 */
+	IMG_REQ_LIO, /* normal = 0 , LIO = 1 */  		
 };
 
 #define for_each_obj_request(ireq, oreq) \
@@ -618,7 +620,7 @@ struct rbd_options {
 };
 
 #define RBD_QUEUE_DEPTH_DEFAULT	BLKDEV_MAX_RQ
-#define RBD_ALLOC_SIZE_DEFAULT	(64 * 1024)
+#define RBD_ALLOC_SIZE_DEFAULT	(4 * 1024 * 1024)
 #define RBD_LOCK_TIMEOUT_DEFAULT 0  /* no timeout */
 #define RBD_READ_ONLY_DEFAULT	false
 #define RBD_LOCK_ON_READ_DEFAULT false
@@ -1376,6 +1378,21 @@ static void rbd_img_request_init(struct
 	img_request->callback = end_cb;
 }
 
+static void rbd_img_capture_header(struct rbd_img_request *img_req)
+{
+	struct rbd_device *rbd_dev = img_req->rbd_dev;
+
+	lockdep_assert_held(&rbd_dev->header_rwsem);
+
+	if (rbd_img_is_write(img_req))
+		img_req->snapc = ceph_get_snap_context(rbd_dev->header.snapc);
+	else
+		img_req->snap_id = rbd_dev->spec->snap_id;
+
+	if (rbd_dev_parent_get(rbd_dev))
+		img_request_layered_set(img_req);
+}
+
 /* This is for use by LIO RBD so we don't export the caches directly */
 struct rbd_img_request *rbd_img_request_create(
 					struct rbd_device *rbd_dev,
@@ -1389,25 +1406,12 @@ struct rbd_img_request *rbd_img_request_
 		return NULL;
 
 	rbd_img_request_init(img_request, rbd_dev, op_type, end_cb);
+	rbd_img_capture_header(img_request);
+	set_bit(IMG_REQ_LIO, &img_request->flags);
 	return img_request;
 }
 EXPORT_SYMBOL(rbd_img_request_create);
 
-static void rbd_img_capture_header(struct rbd_img_request *img_req)
-{
-	struct rbd_device *rbd_dev = img_req->rbd_dev;
-
-	lockdep_assert_held(&rbd_dev->header_rwsem);
-
-	if (rbd_img_is_write(img_req))
-		img_req->snapc = ceph_get_snap_context(rbd_dev->header.snapc);
-	else
-		img_req->snap_id = rbd_dev->spec->snap_id;
-
-	if (rbd_dev_parent_get(rbd_dev))
-		img_request_layered_set(img_req);
-}
-
 void rbd_img_request_destroy(struct rbd_img_request *img_request)
 {
 	struct rbd_obj_request *obj_request;
@@ -1425,7 +1429,7 @@ void rbd_img_request_destroy(struct rbd_
 	if (rbd_img_is_write(img_request))
 		ceph_put_snap_context(img_request->snapc);
 
-	if (test_bit(IMG_REQ_CHILD, &img_request->flags))
+	if (test_bit(IMG_REQ_CHILD, &img_request->flags) || test_bit(IMG_REQ_LIO, &img_request->flags))
 		kmem_cache_free(rbd_img_request_cache, img_request);
 }
 EXPORT_SYMBOL(rbd_img_request_destroy);
@@ -2968,6 +2972,7 @@ again:
 		if (*result == -ENOENT) {
 			rbd_obj_zero_range(obj_req, 0, obj_req->ex.oe_len);
 			*result = 0;
+			obj_req->img_request->obj_read_enoent++;
 		} else if (*result >= 0) {
 			if (*result < obj_req->ex.oe_len)
 				rbd_obj_zero_range(obj_req, *result,
@@ -3606,6 +3611,12 @@ again:
 	switch (img_req->state) {
 	case RBD_IMG_START:
 		rbd_assert(!*result);
+		
+		if(img_req->op_type != OBJ_OP_READ ) {
+			atomic_inc(&rbd_dev->inflight_write_requests);
+			pr_debug("%s rbd_dev %p inflight_write_requests %u",__func__, rbd_dev, 
+				atomic_read(&rbd_dev->inflight_write_requests));
+		}
 
 		ret = rbd_img_exclusive_lock(img_req);
 		if (ret < 0) {
@@ -3697,6 +3708,11 @@ again:
 			goto again;
 		}
 	} else {
+		if(img_req->op_type != OBJ_OP_READ ) {
+			atomic_dec(&img_req->rbd_dev->inflight_write_requests);
+			pr_debug("%s rbd_dev %p inflight_write_requests %u",__func__, img_req->rbd_dev, 
+				atomic_read(&img_req->rbd_dev->inflight_write_requests));
+		}
 		img_req->callback(img_req, result);
 	}
 }
@@ -4435,6 +4451,11 @@ static void rbd_acknowledge_notify_resul
 	__rbd_acknowledge_notify(rbd_dev, notify_id, cookie, &result);
 }
 
+static void rbd_handle_scsi_pr_update(struct rbd_device *rbd_dev, u8 struct_v,
+				     void **p);
+static void rbd_handle_pre_snap(struct rbd_device *rbd_dev, u8 struct_v,
+				     void **p);
+
 static void rbd_watch_cb(void *arg, u64 notify_id, u64 cookie,
 			 u64 notifier_id, void *data, size_t data_len)
 {
@@ -4483,12 +4504,25 @@ static void rbd_watch_cb(void *arg, u64
 			rbd_acknowledge_notify(rbd_dev, notify_id, cookie);
 		break;
 	case RBD_NOTIFY_OP_HEADER_UPDATE:
+		pr_info("%s rbd_dev %p RBD_NOTIFY_OP_HEADER_UPDATE", 
+			__func__, rbd_dev);
 		ret = rbd_dev_refresh(rbd_dev);
 		if (ret)
 			rbd_warn(rbd_dev, "refresh failed: %d", ret);
 
+		atomic_set(&rbd_dev->block_writes, 0);
 		rbd_acknowledge_notify(rbd_dev, notify_id, cookie);
 		break;
+	case RBD_NOTIFY_OP_SCSI_PR_UPDATE:
+		rbd_handle_scsi_pr_update(rbd_dev, struct_v, &p);
+		rbd_acknowledge_notify(rbd_dev, notify_id, cookie);
+		break;		
+	case RBD_NOTIFY_OP_PRE_SNAP:
+		pr_info("%s rbd_dev %p RBD_NOTIFY_OP_PRE_SNAP", 
+			__func__, rbd_dev);
+		rbd_handle_pre_snap(rbd_dev, struct_v, &p);
+ 		rbd_acknowledge_notify(rbd_dev, notify_id, cookie);
+ 		break;		
 	default:
 		if (rbd_is_lock_owner(rbd_dev))
 			rbd_acknowledge_notify_result(rbd_dev, notify_id,
@@ -4764,6 +4798,9 @@ static void rbd_queue_workfn(struct work
 
 	blk_mq_start_request(rq);
 
+	if (op_type != OBJ_OP_READ && rbd_dev_block_writes(rbd_dev)) 
+		rbd_dev_wait_on_bloked_writes(rbd_dev, 20);
+	
 	down_read(&rbd_dev->header_rwsem);
 	mapping_size = rbd_dev->mapping.size;
 	rbd_img_capture_header(img_request);
@@ -5456,6 +5493,14 @@ static struct rbd_device *__rbd_dev_crea
 	rbd_dev->rbd_client = rbdc;
 	rbd_dev->spec = spec;
 
+	mutex_init(&rbd_dev->pr_mutex);
+	rbd_dev->pr_cached = NULL;
+	rbd_dev->pr_dirty = true;
+	rbd_dev->pr_cache_ts = jiffies;
+
+	atomic_set(&rbd_dev->inflight_write_requests, 0);
+	atomic_set(&rbd_dev->block_writes, 0);
+		
 	return rbd_dev;
 }
 
@@ -7443,6 +7488,275 @@ static void __exit rbd_exit(void)
 	rbd_slab_exit();
 }
 
+static void rbd_handle_scsi_pr_update(struct rbd_device *rbd_dev, u8 struct_v,
+				     void **p)
+{
+	struct rbd_client_id my_cid = rbd_get_cid(rbd_dev);
+	struct rbd_client_id cid = { 0 };
+	if (struct_v >= 2) {
+		cid.gid = ceph_decode_64(p);
+		cid.handle = ceph_decode_64(p);
+	}
+	if (rbd_cid_equal(&cid, &my_cid))
+		return;
+
+	dout("%s rbd_dev %p cid %llu-%llu\n", __func__, rbd_dev, cid.gid,
+	     cid.handle);
+
+	mutex_lock(&rbd_dev->pr_mutex);
+	/* dirty pr cache */
+	rbd_dev->pr_dirty = true;
+	mutex_unlock(&rbd_dev->pr_mutex);
+}
+
+static void rbd_handle_pre_snap(struct rbd_device *rbd_dev, u8 struct_v,
+				     void **p)
+{
+	u32 max_wait_sec = 5;
+	u32 sleep_ms = 100;
+	u32 count = max_wait_sec * 1000 / sleep_ms;
+	struct rbd_client_id cid = { 0 };
+	u32 flush_delay;
+
+	if (struct_v >= 2) {
+		cid.gid = ceph_decode_64(p);
+		cid.handle = ceph_decode_64(p);
+	}
+
+	flush_delay = ceph_decode_32(p);			
+	if (rbd_dev->header.features & RBD_FEATURE_EXCLUSIVE_LOCK)
+		return;
+	atomic_set(&rbd_dev->block_writes, 1);
+
+	while(0 < atomic_read(&rbd_dev->inflight_write_requests) ) {
+
+		pr_info("%s rbd_dev %p inflight_write_requests %u",
+			__func__, rbd_dev, 
+			atomic_read(&rbd_dev->inflight_write_requests));
+
+		msleep(sleep_ms);
+		if(--count < 1) {
+			if(0 < atomic_read(&rbd_dev->inflight_write_requests) )
+				rbd_warn(rbd_dev,"%s timeout  inflight_write_requests %u",__func__,
+					atomic_read(&rbd_dev->inflight_write_requests) );
+			break;
+		}
+	}
+}
+
+void rbd_dev_wait_on_bloked_writes(struct rbd_device *rbd_dev, u32 sec)
+{
+	u32 max_wait_sec = sec;
+	u32 sleep_ms = 100;
+	u32 count = max_wait_sec * 1000 / sleep_ms;
+
+	while (rbd_dev_block_writes(rbd_dev)) {
+
+		pr_info("%s rbd_dev %p blocking", __func__, rbd_dev);
+
+		msleep(sleep_ms);
+		if(--count < 1) {
+			rbd_warn(rbd_dev,"%s timeout", __func__);
+			atomic_set(&rbd_dev->block_writes, 0);
+			break;
+		}
+	}
+}
+EXPORT_SYMBOL(rbd_dev_wait_on_bloked_writes);
+
+static int __rbd_notify(struct rbd_device *rbd_dev,
+				enum rbd_notify_op notify_op,
+				struct page ***preply_pages,
+				size_t *preply_len)
+{
+	struct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;
+	struct rbd_client_id cid = rbd_get_cid(rbd_dev);
+	char buf[4 + 8 + 8 + CEPH_ENCODING_START_BLK_LEN];
+	int buf_size = sizeof(buf);	
+	void *p = buf;
+
+	dout("%s rbd_dev %p notify_op %d\n", __func__, rbd_dev, notify_op);
+
+	/* encode *LockPayload NotifyMessage (op + ClientId) */
+	ceph_start_encoding(&p, 2, 1, buf_size - CEPH_ENCODING_START_BLK_LEN);
+	ceph_encode_32(&p, notify_op);
+	ceph_encode_64(&p, cid.gid);
+	ceph_encode_64(&p, cid.handle);
+
+	return ceph_osdc_notify(osdc, &rbd_dev->header_oid,
+				&rbd_dev->header_oloc, buf, buf_size,
+				RBD_NOTIFY_TIMEOUT, preply_pages, preply_len);
+}
+
+void rbd_notify_scsi_pr_update(struct rbd_device *rbd_dev)
+{
+	struct page **reply_pages;
+	size_t reply_len;
+	int ret;
+
+	ret = __rbd_notify(rbd_dev,RBD_NOTIFY_OP_SCSI_PR_UPDATE, &reply_pages, &reply_len);
+	ceph_release_page_vector(reply_pages, calc_pages_for(0, reply_len));
+
+	if(ret) {
+		if(ret == -ETIMEDOUT)
+			rbd_warn(rbd_dev, "pr update notify timeout");
+		else {
+			rbd_warn(rbd_dev, "pr update notify failed, waiting for watchers to expire cache");
+			msleep(RBD_NOTIFY_TIMEOUT * 1000);
+		}
+	}
+}
+EXPORT_SYMBOL(rbd_notify_scsi_pr_update);
+
+int rbd_dev_setxattr(struct rbd_device *rbd_dev, char *key, void *val,
+		     int val_len)
+{
+	struct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;
+	struct ceph_osd_request *req;
+	int ret;
+
+	req = ceph_osdc_alloc_request(osdc, NULL, 1, false, GFP_KERNEL);
+	if (!req)
+		return -ENOMEM;
+
+	ceph_oid_copy(&req->r_base_oid, &rbd_dev->header_oid);
+	ceph_oloc_copy(&req->r_base_oloc, &rbd_dev->header_oloc);
+	req->r_flags = CEPH_OSD_FLAG_WRITE;
+
+	ret = osd_req_op_xattr_init(req, 0, CEPH_OSD_OP_SETXATTR,
+				    key, val, val_len, 0, 0);
+	if (ret)
+		goto out;
+
+	ret = ceph_osdc_alloc_messages(req, GFP_KERNEL);
+	if (ret)
+		goto out;
+
+	ceph_osdc_start_request(osdc, req, false);
+	ret = ceph_osdc_wait_request(osdc, req);
+	if (ret < 0)
+		goto out;
+
+	ret = 0;
+out:
+	ceph_osdc_put_request(req);
+	return ret;
+}
+EXPORT_SYMBOL(rbd_dev_setxattr);
+
+int rbd_dev_cmpsetxattr(struct rbd_device *rbd_dev, char *key, void *oldval,
+			int oldval_len, void *newval, int newval_len)
+{
+	struct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;
+	struct ceph_osd_request *req;
+	int ret;
+
+	req = ceph_osdc_alloc_request(osdc, NULL, 2, false, GFP_KERNEL);
+	if (!req)
+		return -ENOMEM;
+
+	ceph_oid_copy(&req->r_base_oid, &rbd_dev->header_oid);
+	ceph_oloc_copy(&req->r_base_oloc, &rbd_dev->header_oloc);
+	req->r_flags = CEPH_OSD_FLAG_WRITE;
+
+	ret = osd_req_op_xattr_init(req, 0,
+				    CEPH_OSD_OP_CMPXATTR,
+				    key, oldval, oldval_len,
+				    CEPH_OSD_CMPXATTR_OP_EQ,
+				    CEPH_OSD_CMPXATTR_MODE_STRING);
+	if (ret)
+		goto out;
+
+	ret = osd_req_op_xattr_init(req, 1,
+				    CEPH_OSD_OP_SETXATTR,
+				    key, newval, newval_len, 0, 0);
+	if (ret)
+		goto out;
+
+	ret = ceph_osdc_alloc_messages(req, GFP_KERNEL);
+	if (ret)
+		goto out;
+
+	ceph_osdc_start_request(osdc, req, false);
+	ret = ceph_osdc_wait_request(osdc, req);
+	if (ret < 0)
+		goto out;
+
+	ret = 0;
+out:
+	ceph_osdc_put_request(req);
+	return ret;
+}
+EXPORT_SYMBOL(rbd_dev_cmpsetxattr);
+
+int rbd_dev_getxattr(struct rbd_device *rbd_dev, char *key, int max_val_len,
+		     void **_val, int *val_len)
+{
+	struct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;
+	struct ceph_osd_request *req;
+	int page_count;
+	struct page **pages = NULL;
+	void *val;
+	int ret;
+
+	BUG_ON(!key);
+	BUG_ON(!_val || !val_len);
+
+	req = ceph_osdc_alloc_request(osdc, NULL, 1, false, GFP_KERNEL);
+	if (!req)
+		return -ENOMEM;
+
+	ceph_oid_copy(&req->r_base_oid, &rbd_dev->header_oid);
+	ceph_oloc_copy(&req->r_base_oloc, &rbd_dev->header_oloc);
+	req->r_flags = CEPH_OSD_FLAG_READ;
+
+	ret = osd_req_op_xattr_init(req, 0,
+				    CEPH_OSD_OP_GETXATTR,
+				    key, NULL, 0, 0, 0);
+	if (ret)
+		goto out_req;
+
+	page_count = calc_pages_for(0, max_val_len);
+	pages = ceph_alloc_page_vector(page_count, GFP_KERNEL);
+	if (IS_ERR(pages)) {
+		ret = PTR_ERR(pages);
+		goto out_req;
+	}
+
+	osd_req_op_xattr_response_data_pages(req, 0,
+					     pages, max_val_len,
+					     0, false, true);
+
+	ret = ceph_osdc_alloc_messages(req, GFP_KERNEL);
+	if (ret)
+		goto out_req;
+
+	ceph_osdc_start_request(osdc, req, false);
+	ret = ceph_osdc_wait_request(osdc, req);
+	if (ret < 0)
+		goto out_req;
+
+	rbd_assert(ret <= (u64)max_val_len);
+
+	val = kmalloc(ret, GFP_KERNEL);
+	if (!val) {
+		ret = -ENOMEM;
+		goto out_req;
+	}
+
+	ceph_copy_from_page_vector(pages, val, 0, ret);
+	*_val = val;
+	*val_len = ret;
+
+	ret = 0;
+
+out_req:
+	ceph_osdc_put_request(req);
+	return ret;
+}
+EXPORT_SYMBOL(rbd_dev_getxattr);
+
+
 module_init(rbd_init);
 module_exit(rbd_exit);
 
diff -uprN a/drivers/block/rbd_types.h b/drivers/block/rbd_types.h
--- a/drivers/block/rbd_types.h	2024-04-24 01:22:34.568815811 +0200
+++ b/drivers/block/rbd_types.h	2024-04-24 01:23:17.495497987 +0200
@@ -39,6 +39,8 @@ enum rbd_notify_op {
 	RBD_NOTIFY_OP_RELEASED_LOCK      = 1,
 	RBD_NOTIFY_OP_REQUEST_LOCK       = 2,
 	RBD_NOTIFY_OP_HEADER_UPDATE      = 3,
+	RBD_NOTIFY_OP_SCSI_PR_UPDATE	 = 100,	
+	RBD_NOTIFY_OP_PRE_SNAP           = 101,	
 };
 
 #define OBJECT_NONEXISTENT	0
diff -uprN a/drivers/md/dm-writecache.c b/drivers/md/dm-writecache.c
--- a/drivers/md/dm-writecache.c	2024-04-24 01:22:34.568815811 +0200
+++ b/drivers/md/dm-writecache.c	2024-04-24 01:23:17.603504647 +0200
@@ -22,7 +22,7 @@
 
 #define HIGH_WATERMARK			50
 #define LOW_WATERMARK			45
-#define MAX_WRITEBACK_JOBS		min(0x10000000 / PAGE_SIZE, totalram_pages() / 16)
+#define MAX_WRITEBACK_JOBS		65536
 #define ENDIO_LATENCY			16
 #define WRITEBACK_LATENCY		64
 #define AUTOCOMMIT_BLOCKS_SSD		65536
@@ -30,9 +30,9 @@
 #define AUTOCOMMIT_MSEC			1000
 #define MAX_AGE_DIV			16
 #define MAX_AGE_UNSPECIFIED		-1UL
-#define PAUSE_WRITEBACK			(HZ * 3)
+#define PAUSE_WRITEBACK			0
 
-#define BITMAP_GRANULARITY	65536
+#define BITMAP_GRANULARITY	4096
 #if BITMAP_GRANULARITY < PAGE_SIZE
 #undef BITMAP_GRANULARITY
 #define BITMAP_GRANULARITY	PAGE_SIZE
@@ -206,8 +206,15 @@ struct dm_writecache {
 
 	struct bio_set bio_set;
 	mempool_t copy_pool;
+	bool legacy_shadow_sb;
+	bool legacy_read_shadow_seq_count;
 };
 
+#define LEGACY_SHADOW_SUPERBLOCK_MAGIC  0xcafecafe
+#define LEGACY_SHADOW_SUPERBLOCK_VERSION 2
+static bool legacy_detect_start_sector(struct dm_writecache *wc);
+static uint64_t legacy_read_last_seq_count(struct dm_writecache *wc) ;
+
 #define WB_LIST_INLINE		16
 
 struct writeback_struct {
@@ -1011,15 +1018,19 @@ static void writecache_resume(struct dm_
 		INIT_LIST_HEAD(&wc->freelist);
 	}
 	wc->freelist_size = 0;
-
-	r = copy_mc_to_kernel(&sb_seq_count, &sb(wc)->seq_count,
-			      sizeof(uint64_t));
-	if (r) {
-		writecache_error(wc, r, "hardware memory error when reading superblock: %d", r);
-		sb_seq_count = cpu_to_le64(0);
+	
+	if(wc->legacy_read_shadow_seq_count) {
+		wc->seq_count = legacy_read_last_seq_count(wc) + 1;	
 	}
-	wc->seq_count = le64_to_cpu(sb_seq_count);
-
+	else {
+		r = copy_mc_to_kernel(&sb_seq_count, &sb(wc)->seq_count, sizeof(uint64_t));
+		if (r) {
+			writecache_error(wc, r, "hardware memory error when reading superblock: %d", r);
+			sb_seq_count = cpu_to_le64(0);
+		}
+		wc->seq_count = le64_to_cpu(sb_seq_count);	
+	}	
+	
 #ifdef DM_WRITECACHE_HANDLE_HARDWARE_ERRORS
 	for (b = 0; b < wc->n_blocks; b++) {
 		struct wc_entry *e = &wc->entries[b];
@@ -1157,6 +1168,81 @@ static int process_cleaner_mesg(unsigned
 	return 0;
 }
 
+static int set_config_value(struct dm_writecache *wc, char *key, char *val)
+{
+	unsigned v,x;
+	if (sscanf(val, "%u", &v) != 1)
+		return -EINVAL;
+	if (!strcasecmp(key, "high_watermark")) {
+		if (v < 0 || v > 100)
+			return -EINVAL;
+		wc_lock(wc);
+		x = (uint64_t)wc->n_blocks * (100 - v);
+		x += 50;
+		do_div(x, 100);
+		if (wc->freelist_low_watermark < x) {
+			wc_unlock(wc);
+			return -EINVAL;
+		}
+		wc->freelist_high_watermark = x;
+		wc->high_wm_percent_value = v;
+		wc->high_wm_percent_set = true;
+		if (wc->freelist_size + wc->writeback_size 
+			<= wc->freelist_high_watermark)
+			queue_work(wc->writeback_wq, &wc->writeback_work);
+		wc_unlock(wc);
+	}
+	else if (!strcasecmp(key, "low_watermark")) {
+		if (v < 0 || v > 100)
+			return -EINVAL;
+		wc_lock(wc);
+		x = (uint64_t)wc->n_blocks * (100 - v);
+		x += 50;
+		do_div(x, 100);
+		if (x < wc->freelist_high_watermark) {
+			wc_unlock(wc);
+			return -EINVAL;
+		}
+		wc->freelist_low_watermark = x;
+		wc->low_wm_percent_value = v;
+		wc->low_wm_percent_set = true;
+		wc_unlock(wc);
+	}
+	else if (!strcasecmp(key, "writeback_jobs")) {
+		wc_lock(wc);
+		wc->max_writeback_jobs = v;
+		wc->max_writeback_jobs_set = true;
+		wc_unlock(wc);
+	}
+	else if (!strcasecmp(key, "autocommit_blocks")) {
+		wc_lock(wc);
+		wc->autocommit_blocks = v;
+		wc->autocommit_blocks_set = true;
+		wc_unlock(wc);
+	}
+	else if (!strcasecmp(key, "autocommit_time")) {
+		if (v < 0 || v > 3600000)
+			return -EINVAL;
+		wc_lock(wc);
+		wc->autocommit_jiffies = msecs_to_jiffies(v);
+		wc->autocommit_time_value = v;
+		wc->autocommit_time_set = true;
+		wc_unlock(wc);
+	}
+	else if (!strcasecmp(key, "pause_writeback")) {
+		if (v < 0 || v > 60000)
+			return -EINVAL;
+		wc_lock(wc);
+		wc->pause = msecs_to_jiffies(v);
+		wc->pause_value = v;
+		wc->pause_set = true;
+		wc_unlock(wc);
+	}	
+	else
+		return -EINVAL;
+	return 0;
+}
+
 static int writecache_message(struct dm_target *ti, unsigned argc, char **argv,
 			      char *result, unsigned maxlen)
 {
@@ -1169,6 +1255,8 @@ static int writecache_message(struct dm_
 		r = process_flush_on_suspend_mesg(argc, argv, wc);
 	else if (!strcasecmp(argv[0], "cleaner"))
 		r = process_cleaner_mesg(argc, argv, wc);
+	else if (argc==2)
+		r = set_config_value(wc, argv[0], argv[1]);	
 	else
 		DMERR("unrecognised message received: %s", argv[0]);
 
@@ -2402,6 +2490,7 @@ invalid_optional:
 		size_t n_blocks, n_metadata_blocks;
 		uint64_t n_bitmap_bits;
 
+		legacy_detect_start_sector(wc);
 		wc->memory_map_size -= (uint64_t)wc->start_sector << SECTOR_SHIFT;
 
 		bio_list_init(&wc->flush_list);
@@ -2486,13 +2575,19 @@ invalid_optional:
 		r = -EINVAL;
 		goto bad;
 	}
-
-	if (le32_to_cpu(s.version) != MEMORY_SUPERBLOCK_VERSION) {
-		ti->error = "Invalid version in the superblock";
-		r = -EINVAL;
-		goto bad;
+			
+	wc->legacy_read_shadow_seq_count = false;
+	if (le32_to_cpu(s.version) != MEMORY_SUPERBLOCK_VERSION ) {
+		if( wc->legacy_shadow_sb && le32_to_cpu(s.version) == LEGACY_SHADOW_SUPERBLOCK_VERSION ) {
+			wc->legacy_read_shadow_seq_count = true;
+		}
+		else {
+			ti->error = "Invalid version in the superblock";
+			r = -EINVAL;
+			goto bad;
+		}
 	}
-
+	
 	if (le32_to_cpu(s.block_size) != wc->block_size) {
 		ti->error = "Block size does not match superblock";
 		r = -EINVAL;
@@ -2564,7 +2659,6 @@ static void writecache_status(struct dm_
 			      unsigned status_flags, char *result, unsigned maxlen)
 {
 	struct dm_writecache *wc = ti->private;
-	unsigned extra_args;
 	unsigned sz = 0;
 
 	switch (type) {
@@ -2576,57 +2670,99 @@ static void writecache_status(struct dm_
 	case STATUSTYPE_TABLE:
 		DMEMIT("%c %s %s %u ", WC_MODE_PMEM(wc) ? 'p' : 's',
 				wc->dev->name, wc->ssd_dev->name, wc->block_size);
-		extra_args = 0;
-		if (wc->start_sector_set)
-			extra_args += 2;
-		if (wc->high_wm_percent_set)
-			extra_args += 2;
-		if (wc->low_wm_percent_set)
-			extra_args += 2;
-		if (wc->max_writeback_jobs_set)
-			extra_args += 2;
-		if (wc->autocommit_blocks_set)
-			extra_args += 2;
-		if (wc->autocommit_time_set)
-			extra_args += 2;
-		if (wc->max_age_set)
-			extra_args += 2;
-		if (wc->cleaner_set)
-			extra_args++;
-		if (wc->writeback_fua_set)
-			extra_args++;
-		if (wc->metadata_only)
-			extra_args++;
-		if (wc->pause_set)
-			extra_args += 2;
-
-		DMEMIT("%u", extra_args);
-		if (wc->start_sector_set)
-			DMEMIT(" start_sector %llu", (unsigned long long)wc->start_sector);
-		if (wc->high_wm_percent_set)
-			DMEMIT(" high_watermark %u", wc->high_wm_percent_value);
-		if (wc->low_wm_percent_set)
-			DMEMIT(" low_watermark %u", wc->low_wm_percent_value);
-		if (wc->max_writeback_jobs_set)
-			DMEMIT(" writeback_jobs %u", wc->max_writeback_jobs);
-		if (wc->autocommit_blocks_set)
-			DMEMIT(" autocommit_blocks %u", wc->autocommit_blocks);
-		if (wc->autocommit_time_set)
-			DMEMIT(" autocommit_time %u", wc->autocommit_time_value);
-		if (wc->max_age_set)
-			DMEMIT(" max_age %u", wc->max_age_value);
-		if (wc->cleaner_set)
-			DMEMIT(" cleaner");
-		if (wc->writeback_fua_set)
-			DMEMIT(" %sfua", wc->writeback_fua ? "" : "no");
+		DMEMIT(" start_sector %llu", (unsigned long long)wc->start_sector);
+		DMEMIT(" high_watermark %u", wc->high_wm_percent_value);
+		DMEMIT(" low_watermark %u", wc->low_wm_percent_value);
+		DMEMIT(" writeback_jobs %u", wc->max_writeback_jobs);
+		DMEMIT(" autocommit_blocks %u", wc->autocommit_blocks);
+		DMEMIT(" autocommit_time %u", wc->autocommit_time_value);
+		DMEMIT(" %sfua", wc->writeback_fua ? "" : "no");
 		if (wc->metadata_only)
 			DMEMIT(" metadata_only");
-		if (wc->pause_set)
-			DMEMIT(" pause_writeback %u", wc->pause_value);
+		DMEMIT(" pause_writeback %u", wc->pause_value);
+		DMEMIT(" max_age %u", wc->max_age_value);
+
 		break;
 	}
 }
 
+static uint64_t legacy_read_last_seq_count(struct dm_writecache *wc) 
+{
+	size_t b = wc->n_blocks;
+	uint64_t last_seq_count = 0;
+	uint64_t seq_count;
+	struct wc_memory_entry *p = &sb(wc)->entries[0];
+	__le64 empty = cpu_to_le64(-1);
+	while (0 < b) {
+		if (p->seq_count != empty) {
+			seq_count = le64_to_cpu(p->seq_count);
+			if (last_seq_count < seq_count)
+				last_seq_count = seq_count;		
+		}
+		p++;		
+		b--;
+	} 
+	
+	DMINFO("legacy read last sequence count: %llu", last_seq_count);
+	return last_seq_count;
+}
+	
+struct legacy_shadow_superblock {
+	union {
+		struct {
+			__le32 magic;
+			__le32 version;
+			__le64 origin_start_sector;
+			__u8 meta_state;
+		};
+		__le64 padding[64];
+	};
+};	
+
+
+static bool legacy_detect_start_sector(struct dm_writecache *wc)
+{
+	struct dm_io_region region;
+	struct dm_io_request req;
+	struct legacy_shadow_superblock *sb;
+	uint64_t len;	
+	bool ret; 
+	
+	wc->legacy_shadow_sb = false;
+	ret = false; 
+	
+	len =  max((size_t)4096U, sizeof(struct legacy_shadow_superblock)); 
+	sb = alloc_pages_exact(len, GFP_KERNEL);
+	
+	region.bdev = wc->ssd_dev->bdev;
+	region.sector = 0;
+	region.count = len >> SECTOR_SHIFT;
+	req.bi_op = REQ_OP_READ;
+	req.bi_op_flags = 0;
+	req.mem.type = DM_IO_KMEM;
+	req.mem.ptr.addr = sb;
+	req.client = wc->dm_io;
+	req.notify.fn = NULL;
+	
+	if( dm_io(&req, 1, &region, NULL) ) {
+		goto end;
+	}
+	
+	if (le32_to_cpu(sb->magic) != LEGACY_SHADOW_SUPERBLOCK_MAGIC || 
+				le32_to_cpu(sb->version) != LEGACY_SHADOW_SUPERBLOCK_VERSION ) {
+		goto end;
+	}
+	wc->legacy_shadow_sb = true;	
+	wc->start_sector = le64_to_cpu(sb->origin_start_sector) ;
+	DMINFO("detected legacy shadow with start sector: %llu",wc->start_sector);
+	ret = true;
+end:
+	free_pages_exact(sb, len);
+	return ret;
+
+}
+
+
 static struct target_type writecache_target = {
 	.name			= "writecache",
 	.version		= {1, 5, 0},
diff -uprN a/drivers/target/iscsi/iscsi_target.c b/drivers/target/iscsi/iscsi_target.c
--- a/drivers/target/iscsi/iscsi_target.c	2024-04-24 01:22:34.568815811 +0200
+++ b/drivers/target/iscsi/iscsi_target.c	2024-04-24 01:23:17.515499221 +0200
@@ -3349,6 +3349,7 @@ iscsit_build_sendtargets_response(struct
 	unsigned char buf[ISCSI_IQN_LEN+12]; /* iqn + "TargetName=" + \0 */
 	unsigned char *text_in = cmd->text_in_ptr, *text_ptr = NULL;
 	bool active;
+	int matched_portal;
 
 	buffer_len = min(conn->conn_ops->MaxRecvDataSegmentLength,
 			 SENDTARGETS_BUF_LIMIT);
@@ -3384,6 +3385,36 @@ iscsit_build_sendtargets_response(struct
 
 		target_name_printed = 0;
 
+		/* PetaSAN send targets with matching portal ip */ 
+		matched_portal =0;
+		spin_lock(&tiqn->tiqn_tpg_lock);
+		list_for_each_entry(tpg, &tiqn->tiqn_tpg_list, tpg_list) {
+			spin_lock(&tpg->tpg_np_lock);		
+			list_for_each_entry(tpg_np, &tpg->tpg_gnp_list,tpg_np_list) {
+				struct iscsi_np *np = tpg_np->tpg_np;
+/*
+				if (  np->np_ip != NULL && strcmp(conn->local_ip , np->np_ip) == 0 )
+					matched_portal = 1;
+*/
+				if (conn->local_sockaddr.ss_family == AF_INET) {
+					struct sockaddr_in* local_sockaddr_in = (struct sockaddr_in *)&conn->local_sockaddr;
+					struct sockaddr_in* np_sockaddr_in    = (struct sockaddr_in *)&np->np_sockaddr;
+
+					if( memcmp((char *)&local_sockaddr_in->sin_addr,(char *)&np_sockaddr_in->sin_addr ,sizeof(struct in_addr) ) == 0)
+						matched_portal = 1;
+				}
+
+				if( matched_portal == 1 )
+					break;
+			}
+			spin_unlock(&tpg->tpg_np_lock);
+			if( matched_portal == 1 )
+				break;
+		}
+		spin_unlock(&tiqn->tiqn_tpg_lock);
+		if (matched_portal !=1)
+			continue; 
+		
 		spin_lock(&tiqn->tiqn_tpg_lock);
 		list_for_each_entry(tpg, &tiqn->tiqn_tpg_list, tpg_list) {
 
diff -uprN a/drivers/target/target_core_alua.c b/drivers/target/target_core_alua.c
--- a/drivers/target/target_core_alua.c	2024-04-24 01:22:34.572816065 +0200
+++ b/drivers/target/target_core_alua.c	2024-04-24 01:23:17.527499960 +0200
@@ -1646,10 +1646,10 @@ struct t10_alua_tg_pt_gp *core_alua_allo
 	tg_pt_gp->tg_pt_gp_alua_access_state =
 			ALUA_ACCESS_STATE_ACTIVE_OPTIMIZED;
 	/*
-	 * Enable both explicit and implicit ALUA support by default
+	 * Default to symmetric LUA using ALUA semantics: implicit-only TPGs in A/O
 	 */
-	tg_pt_gp->tg_pt_gp_alua_access_type =
-			TPGS_EXPLICIT_ALUA | TPGS_IMPLICIT_ALUA;
+	tg_pt_gp->tg_pt_gp_alua_access_type = TPGS_IMPLICIT_ALUA;
+
 	/*
 	 * Set the default Active/NonOptimized Delay in milliseconds
 	 */
diff -uprN a/drivers/target/target_core_device.c b/drivers/target/target_core_device.c
--- a/drivers/target/target_core_device.c	2024-04-24 01:22:34.572816065 +0200
+++ b/drivers/target/target_core_device.c	2024-04-24 01:23:17.543500947 +0200
@@ -817,7 +817,7 @@ struct se_device *target_alloc_device(st
 	xcopy_lun->lun_tpg = &xcopy_pt_tpg;
 
 	/* Preload the default INQUIRY const values */
-	strlcpy(dev->t10_wwn.vendor, "LIO-ORG", sizeof(dev->t10_wwn.vendor));
+	strlcpy(dev->t10_wwn.vendor, "PETASAN", sizeof(dev->t10_wwn.vendor));
 	strlcpy(dev->t10_wwn.model, dev->transport->inquiry_prod,
 		sizeof(dev->t10_wwn.model));
 	strlcpy(dev->t10_wwn.revision, dev->transport->inquiry_rev,
diff -uprN a/drivers/target/target_core_pr.c b/drivers/target/target_core_pr.c
--- a/drivers/target/target_core_pr.c	2024-04-24 01:22:34.572816065 +0200
+++ b/drivers/target/target_core_pr.c	2024-04-24 01:23:17.427493794 +0200
@@ -1,3 +1,4 @@
+
 // SPDX-License-Identifier: GPL-2.0-or-later
 /*******************************************************************************
  * Filename:  target_core_pr.c
@@ -197,16 +198,14 @@ void target_release_reservation(struct s
 	}
 }
 
-sense_reason_t
-target_scsi2_reservation_release(struct se_cmd *cmd)
+static sense_reason_t
+target_scsi2_reservation_release_execute(struct se_cmd *cmd)
 {
 	struct se_device *dev = cmd->se_dev;
 	struct se_session *sess = cmd->se_sess;
 	struct se_portal_group *tpg;
 	int rc;
 
-	if (!sess || !sess->se_tpg)
-		goto out;
 	rc = target_check_scsi2_reservation_conflict(cmd);
 	if (rc == 1)
 		goto out;
@@ -234,30 +233,38 @@ target_scsi2_reservation_release(struct
 out_unlock:
 	spin_unlock(&dev->dev_reservation_lock);
 out:
-	target_complete_cmd(cmd, SAM_STAT_GOOD);
 	return 0;
 }
 
 sense_reason_t
-target_scsi2_reservation_reserve(struct se_cmd *cmd)
+target_scsi2_reservation_release(struct se_cmd *cmd)
 {
 	struct se_device *dev = cmd->se_dev;
 	struct se_session *sess = cmd->se_sess;
-	struct se_portal_group *tpg;
 	sense_reason_t ret = 0;
-	int rc;
 
-	if ((cmd->t_task_cdb[1] & 0x01) &&
-	    (cmd->t_task_cdb[1] & 0x02)) {
-		pr_err("LongIO and Obsolete Bits set, returning ILLEGAL_REQUEST\n");
-		return TCM_UNSUPPORTED_SCSI_OPCODE;
-	}
-	/*
-	 * This is currently the case for target_core_mod passthrough struct se_cmd
-	 * ops
-	 */
 	if (!sess || !sess->se_tpg)
 		goto out;
+
+	if (dev->transport->pr_ops && dev->transport->pr_ops->scsi2_release)
+		ret = dev->transport->pr_ops->scsi2_release(cmd);
+	else
+		ret = target_scsi2_reservation_release_execute(cmd);
+out:
+	if (!ret)
+		target_complete_cmd(cmd, SAM_STAT_GOOD);
+	return ret;
+}
+
+static sense_reason_t
+target_scsi2_reservation_reserve_execute(struct se_cmd *cmd)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct se_session *sess = cmd->se_sess;
+	struct se_portal_group *tpg;
+	sense_reason_t ret = 0;
+	int rc;
+
 	rc = target_check_scsi2_reservation_conflict(cmd);
 	if (rc == 1)
 		goto out;
@@ -296,44 +303,58 @@ target_scsi2_reservation_reserve(struct
 out_unlock:
 	spin_unlock(&dev->dev_reservation_lock);
 out:
+	return ret;
+}
+
+sense_reason_t
+target_scsi2_reservation_reserve(struct se_cmd *cmd)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct se_session *sess = cmd->se_sess;
+	sense_reason_t ret = 0;
+
+	if ((cmd->t_task_cdb[1] & 0x01) &&
+	    (cmd->t_task_cdb[1] & 0x02)) {
+		pr_err("LongIO and Obselete Bits set, returning"
+				" ILLEGAL_REQUEST\n");
+		return TCM_UNSUPPORTED_SCSI_OPCODE;
+	}
+	/*
+	 * This is currently the case for target_core_mod passthrough struct se_cmd
+	 * ops
+	 */
+	if (!sess || !sess->se_tpg)
+		goto out;
+
+	if (dev->transport->pr_ops && dev->transport->pr_ops->scsi2_reserve)
+		ret = dev->transport->pr_ops->scsi2_reserve(cmd);
+	else
+		ret = target_scsi2_reservation_reserve_execute(cmd);
+out:
 	if (!ret)
 		target_complete_cmd(cmd, SAM_STAT_GOOD);
 	return ret;
 }
 
 
+
 /*
  * Begin SPC-3/SPC-4 Persistent Reservations emulation support
  *
  * This function is called by those initiator ports who are *NOT*
  * the active PR reservation holder when a reservation is present.
  */
-static int core_scsi3_pr_seq_non_holder(struct se_cmd *cmd, u32 pr_reg_type,
-					bool isid_mismatch)
+int core_scsi3_pr_seq_non_holder(struct se_cmd *cmd, u32 pr_reg_type,
+				 char *dbg_nexus, bool registered_nexus)
 {
 	unsigned char *cdb = cmd->t_task_cdb;
-	struct se_session *se_sess = cmd->se_sess;
-	struct se_node_acl *nacl = se_sess->se_node_acl;
 	int other_cdb = 0;
-	int registered_nexus = 0, ret = 1; /* Conflict by default */
+	int ret = 1; /* Conflict by default */
 	int all_reg = 0, reg_only = 0; /* ALL_REG, REG_ONLY */
 	int we = 0; /* Write Exclusive */
 	int legacy = 0; /* Act like a legacy device and return
 			 * RESERVATION CONFLICT on some CDBs */
 
-	if (isid_mismatch) {
-		registered_nexus = 0;
-	} else {
-		struct se_dev_entry *se_deve;
-
-		rcu_read_lock();
-		se_deve = target_nacl_find_deve(nacl, cmd->orig_fe_lun);
-		if (se_deve)
-			registered_nexus = test_bit(DEF_PR_REG_ACTIVE,
-						    &se_deve->deve_flags);
-		rcu_read_unlock();
-	}
-
 	switch (pr_reg_type) {
 	case PR_TYPE_WRITE_EXCLUSIVE:
 		we = 1;
@@ -502,7 +523,7 @@ static int core_scsi3_pr_seq_non_holder(
 			pr_debug("%s Conflict for unregistered nexus"
 				" %s CDB: 0x%02x to %s reservation\n",
 				transport_dump_cmd_direction(cmd),
-				se_sess->se_node_acl->initiatorname, cdb[0],
+				dbg_nexus, cdb[0],
 				core_scsi3_pr_dump_type(pr_reg_type));
 			return 1;
 		} else {
@@ -554,11 +575,12 @@ static int core_scsi3_pr_seq_non_holder(
 	pr_debug("%s Conflict for %sregistered nexus %s CDB: 0x%2x"
 		" for %s reservation\n", transport_dump_cmd_direction(cmd),
 		(registered_nexus) ? "" : "un",
-		se_sess->se_node_acl->initiatorname, cdb[0],
+		dbg_nexus, cdb[0],
 		core_scsi3_pr_dump_type(pr_reg_type));
 
 	return 1; /* Conflict by default */
 }
+EXPORT_SYMBOL(core_scsi3_pr_seq_non_holder);
 
 static sense_reason_t
 target_scsi3_pr_reservation_check(struct se_cmd *cmd)
@@ -567,6 +589,7 @@ target_scsi3_pr_reservation_check(struct
 	struct se_session *sess = cmd->se_sess;
 	u32 pr_reg_type;
 	bool isid_mismatch = false;
+	bool registered_nexus = false;
 
 	if (!dev->dev_pr_res_holder)
 		return 0;
@@ -587,7 +610,21 @@ target_scsi3_pr_reservation_check(struct
 	return 0;
 
 check_nonholder:
-	if (core_scsi3_pr_seq_non_holder(cmd, pr_reg_type, isid_mismatch))
+	if (!isid_mismatch) {
+		struct se_node_acl *nacl = sess->se_node_acl;
+		struct se_dev_entry *se_deve;
+
+		rcu_read_lock();
+		se_deve = target_nacl_find_deve(nacl, cmd->orig_fe_lun);
+		if (se_deve)
+			registered_nexus = test_bit(DEF_PR_REG_ACTIVE,
+						    &se_deve->deve_flags);
+		rcu_read_unlock();
+	}
+
+	if (core_scsi3_pr_seq_non_holder(cmd, pr_reg_type,
+					 sess->se_node_acl->initiatorname,
+					 registered_nexus))
 		return TCM_RESERVATION_CONFLICT;
 	return 0;
 }
@@ -2043,8 +2080,10 @@ retry:
 }
 
 static sense_reason_t
-core_scsi3_emulate_pro_register(struct se_cmd *cmd, u64 res_key, u64 sa_res_key,
-		bool aptpl, bool all_tg_pt, bool spec_i_pt, enum register_type register_type)
+core_scsi3_emulate_pro_register_execute(struct se_cmd *cmd, u64 res_key,
+					u64 sa_res_key, bool aptpl,
+					bool all_tg_pt, bool spec_i_pt,
+					enum register_type register_type)
 {
 	struct se_session *se_sess = cmd->se_sess;
 	struct se_device *dev = cmd->se_dev;
@@ -2245,6 +2284,34 @@ out:
 	return ret;
 }
 
+
+
+static sense_reason_t
+core_scsi3_emulate_pro_register(struct se_cmd *cmd, u64 res_key, u64 sa_res_key,
+		bool aptpl, bool all_tg_pt, bool spec_i_pt,
+		enum register_type register_type)
+{
+	struct se_device *dev = cmd->se_dev;
+	sense_reason_t ret;
+
+	if (dev->transport->pr_ops && dev->transport->pr_ops->pr_register) {
+		bool ignore_existing;
+		if (register_type == REGISTER_AND_IGNORE_EXISTING_KEY)
+			ignore_existing = true;
+		else
+			ignore_existing = false;
+		ret = dev->transport->pr_ops->pr_register(cmd, res_key,
+			sa_res_key, aptpl, all_tg_pt, spec_i_pt,
+			ignore_existing);
+	} else {
+		ret = core_scsi3_emulate_pro_register_execute(cmd,
+			res_key, sa_res_key, aptpl, all_tg_pt, spec_i_pt,
+			register_type);
+	}
+
+	return ret;
+}
+
 unsigned char *core_scsi3_pr_dump_type(int type)
 {
 	switch (type) {
@@ -2425,6 +2492,9 @@ static sense_reason_t
 core_scsi3_emulate_pro_reserve(struct se_cmd *cmd, int type, int scope,
 		u64 res_key)
 {
+	struct se_device *dev = cmd->se_dev;
+	sense_reason_t ret;
+
 	switch (type) {
 	case PR_TYPE_WRITE_EXCLUSIVE:
 	case PR_TYPE_EXCLUSIVE_ACCESS:
@@ -2432,7 +2502,30 @@ core_scsi3_emulate_pro_reserve(struct se
 	case PR_TYPE_EXCLUSIVE_ACCESS_REGONLY:
 	case PR_TYPE_WRITE_EXCLUSIVE_ALLREG:
 	case PR_TYPE_EXCLUSIVE_ACCESS_ALLREG:
-		return core_scsi3_pro_reserve(cmd, type, scope, res_key);
+		/*
+		 * From spc4r17 Section 5.7.9: Reserving:
+		 *
+		 * From above:
+		 *  b) TYPE field and SCOPE field set to the persistent
+		 *     reservation being created.
+		 *
+		 * Only one persistent reservation is allowed at a time per
+		 * logical unit and that persistent reservation has a scope of
+		 * LU_SCOPE.
+		 */
+		if (scope != PR_SCOPE_LU_SCOPE) {
+			pr_err("SPC-3 PR: Illegal SCOPE: 0x%02x\n", scope);
+			return TCM_INVALID_PARAMETER_LIST;
+		}
+
+		if (dev->transport->pr_ops
+					&& dev->transport->pr_ops->pr_reserve) {
+			ret = dev->transport->pr_ops->pr_reserve(cmd, type,
+								 res_key);
+		} else {
+			ret = core_scsi3_pro_reserve(cmd, type, scope, res_key);
+		}
+		return ret;
 	default:
 		pr_err("SPC-3 PR: Unknown Service Action RESERVE Type:"
 			" 0x%02x\n", type);
@@ -2508,7 +2601,7 @@ out:
 }
 
 static sense_reason_t
-core_scsi3_emulate_pro_release(struct se_cmd *cmd, int type, int scope,
+core_scsi3_emulate_pro_release_execute(struct se_cmd *cmd, int type, int scope,
 		u64 res_key)
 {
 	struct se_device *dev = cmd->se_dev;
@@ -2665,7 +2758,27 @@ out_put_pr_reg:
 }
 
 static sense_reason_t
-core_scsi3_emulate_pro_clear(struct se_cmd *cmd, u64 res_key)
+core_scsi3_emulate_pro_release(struct se_cmd *cmd, int type, int scope,
+		u64 res_key)
+{
+	struct se_device *dev = cmd->se_dev;
+	sense_reason_t ret;
+
+	if (dev->transport->pr_ops && dev->transport->pr_ops->pr_release) {
+		if (scope != PR_SCOPE_LU_SCOPE) {
+			pr_err("SPC-3 PR: Illegal SCOPE: 0x%02x\n", scope);
+			return TCM_INVALID_PARAMETER_LIST;
+		}
+		ret = dev->transport->pr_ops->pr_release(cmd, type, res_key);
+	} else {
+		ret = core_scsi3_emulate_pro_release_execute(cmd, type, scope,
+							     res_key);
+	}
+	return ret;
+}
+
+static sense_reason_t
+core_scsi3_emulate_pro_clear_execute(struct se_cmd *cmd, u64 res_key)
 {
 	struct se_device *dev = cmd->se_dev;
 	struct se_node_acl *pr_reg_nacl;
@@ -2748,6 +2861,20 @@ core_scsi3_emulate_pro_clear(struct se_c
 	return 0;
 }
 
+static sense_reason_t
+core_scsi3_emulate_pro_clear(struct se_cmd *cmd, u64 res_key)
+{
+	struct se_device *dev = cmd->se_dev;
+	sense_reason_t ret;
+
+	if (dev->transport->pr_ops && dev->transport->pr_ops->pr_clear) {
+		ret = dev->transport->pr_ops->pr_clear(cmd, res_key);
+	} else {
+		ret = core_scsi3_emulate_pro_clear_execute(cmd, res_key);
+	}
+	return ret;
+}
+
 static void __core_scsi3_complete_pro_preempt(
 	struct se_device *dev,
 	struct t10_pr_registration *pr_reg,
@@ -3115,6 +3242,9 @@ static sense_reason_t
 core_scsi3_emulate_pro_preempt(struct se_cmd *cmd, int type, int scope,
 		u64 res_key, u64 sa_res_key, enum preempt_type preempt_type)
 {
+	struct se_device *dev = cmd->se_dev;
+	sense_reason_t ret;
+	
 	switch (type) {
 	case PR_TYPE_WRITE_EXCLUSIVE:
 	case PR_TYPE_EXCLUSIVE_ACCESS:
@@ -3122,8 +3252,26 @@ core_scsi3_emulate_pro_preempt(struct se
 	case PR_TYPE_EXCLUSIVE_ACCESS_REGONLY:
 	case PR_TYPE_WRITE_EXCLUSIVE_ALLREG:
 	case PR_TYPE_EXCLUSIVE_ACCESS_ALLREG:
-		return core_scsi3_pro_preempt(cmd, type, scope, res_key,
-					      sa_res_key, preempt_type);
+		if (dev->transport->pr_ops
+					&& dev->transport->pr_ops->pr_preempt) {
+			bool abort;
+			if (preempt_type == PREEMPT_AND_ABORT)
+				abort = true;
+			else
+				abort = false;
+
+			if (scope != PR_SCOPE_LU_SCOPE) {
+				pr_err("SPC-3 PR: Illegal SCOPE: 0x%02x\n", scope);
+				return TCM_INVALID_PARAMETER_LIST;
+			}
+			ret = dev->transport->pr_ops->pr_preempt(cmd, res_key,
+								 sa_res_key,
+								 type, abort);
+		} else {
+			ret = core_scsi3_pro_preempt(cmd, type, scope, res_key,
+						     sa_res_key, preempt_type);
+		}
+		return ret;
 	default:
 		pr_err("SPC-3 PR: Unknown Service Action PREEMPT%s"
 			" Type: 0x%02x\n", (preempt_type == PREEMPT_AND_ABORT) ? "_AND_ABORT" : "", type);
@@ -3133,7 +3281,7 @@ core_scsi3_emulate_pro_preempt(struct se
 
 
 static sense_reason_t
-core_scsi3_emulate_pro_register_and_move(struct se_cmd *cmd, u64 res_key,
+core_scsi3_emulate_pro_register_and_move_execute(struct se_cmd *cmd, u64 res_key,
 		u64 sa_res_key, int aptpl, int unreg)
 {
 	struct se_session *se_sess = cmd->se_sess;
@@ -3531,6 +3679,25 @@ out_put_pr_reg:
 	return ret;
 }
 
+static sense_reason_t
+core_scsi3_emulate_pro_register_and_move(struct se_cmd *cmd, u64 res_key,
+		u64 sa_res_key, int aptpl, int unreg)
+{
+	struct se_device *dev = cmd->se_dev;
+	sense_reason_t ret;
+
+	if (dev->transport->pr_ops
+			&& dev->transport->pr_ops->pr_register_and_move) {
+		ret = dev->transport->pr_ops->pr_register_and_move(cmd, res_key,
+			sa_res_key, aptpl, unreg);
+	} else {
+		ret = core_scsi3_emulate_pro_register_and_move_execute(cmd,
+			res_key, sa_res_key, aptpl, unreg);
+	}
+
+	return ret;
+}
+
 /*
  * See spc4r17 section 6.14 Table 170
  */
@@ -3680,29 +3847,14 @@ target_scsi3_emulate_pr_out(struct se_cm
 	return ret;
 }
 
-/*
- * PERSISTENT_RESERVE_IN Service Action READ_KEYS
- *
- * See spc4r17 section 5.7.6.2 and section 6.13.2, Table 160
- */
 static sense_reason_t
-core_scsi3_pri_read_keys(struct se_cmd *cmd)
+core_scsi3_pri_read_keys_execute(struct se_cmd *cmd, unsigned char *buf,
+				 u32 buf_len)
 {
 	struct se_device *dev = cmd->se_dev;
 	struct t10_pr_registration *pr_reg;
-	unsigned char *buf;
 	u32 add_len = 0, off = 8;
 
-	if (cmd->data_length < 8) {
-		pr_err("PRIN SA READ_KEYS SCSI Data Length: %u"
-			" too small\n", cmd->data_length);
-		return TCM_INVALID_CDB_FIELD;
-	}
-
-	buf = transport_kmap_data_sg(cmd);
-	if (!buf)
-		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
-
 	put_unaligned_be32(dev->t10_pr.pr_generation, buf);
 
 	spin_lock(&dev->t10_pr.registration_lock);
@@ -3729,27 +3881,24 @@ core_scsi3_pri_read_keys(struct se_cmd *
 	put_unaligned_be32(add_len, &buf[4]);
 	target_set_cmd_data_length(cmd, 8 + add_len);
 
-	transport_kunmap_data_sg(cmd);
-
-	return 0;
+	return TCM_NO_SENSE;
 }
 
+
 /*
- * PERSISTENT_RESERVE_IN Service Action READ_RESERVATION
+ * PERSISTENT_RESERVE_IN Service Action READ_KEYS
  *
- * See spc4r17 section 5.7.6.3 and section 6.13.3.2 Table 161 and 162
+ * See spc4r17 section 5.7.6.2 and section 6.13.2, Table 160
  */
 static sense_reason_t
-core_scsi3_pri_read_reservation(struct se_cmd *cmd)
+core_scsi3_pri_read_keys(struct se_cmd *cmd)
 {
 	struct se_device *dev = cmd->se_dev;
-	struct t10_pr_registration *pr_reg;
 	unsigned char *buf;
-	u64 pr_res_key;
-	u32 add_len = 0;
+	sense_reason_t ret;
 
 	if (cmd->data_length < 8) {
-		pr_err("PRIN SA READ_RESERVATIONS SCSI Data Length: %u"
+		pr_err("PRIN SA READ_KEYS SCSI Data Length: %u"
 			" too small\n", cmd->data_length);
 		return TCM_INVALID_CDB_FIELD;
 	}
@@ -3758,6 +3907,33 @@ core_scsi3_pri_read_reservation(struct s
 	if (!buf)
 		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
 
+	if (dev->transport->pr_ops && dev->transport->pr_ops->pr_read_keys) {
+		ret = dev->transport->pr_ops->pr_read_keys(cmd, buf,
+							   cmd->data_length);
+	} else {
+		ret = core_scsi3_pri_read_keys_execute(cmd, buf,
+						       cmd->data_length);
+	}
+	if (ret)
+		goto err_unmap;
+
+	ret = TCM_NO_SENSE;
+err_unmap:
+	transport_kunmap_data_sg(cmd);
+
+	return ret;
+}
+
+
+static sense_reason_t
+core_scsi3_pri_read_reservation_execute(struct se_cmd *cmd, unsigned char *buf,
+					u32 buf_len)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct t10_pr_registration *pr_reg;
+	u64 pr_res_key;
+	u32 add_len = 0;
+
 	put_unaligned_be32(dev->t10_pr.pr_generation, &buf[0]);
 
 	spin_lock(&dev->dev_reservation_lock);
@@ -3806,27 +3982,24 @@ core_scsi3_pri_read_reservation(struct s
 
 err:
 	spin_unlock(&dev->dev_reservation_lock);
-	transport_kunmap_data_sg(cmd);
-
 	return 0;
 }
 
 /*
- * PERSISTENT_RESERVE_IN Service Action REPORT_CAPABILITIES
+ * PERSISTENT_RESERVE_IN Service Action READ_RESERVATION
  *
- * See spc4r17 section 6.13.4 Table 165
+ * See spc4r17 section 5.7.6.3 and section 6.13.3.2 Table 161 and 162
  */
 static sense_reason_t
-core_scsi3_pri_report_capabilities(struct se_cmd *cmd)
+core_scsi3_pri_read_reservation(struct se_cmd *cmd)
 {
 	struct se_device *dev = cmd->se_dev;
-	struct t10_reservation *pr_tmpl = &dev->t10_pr;
 	unsigned char *buf;
-	u16 len = 8; /* Hardcoded to 8. */
+	sense_reason_t ret;
 
-	if (cmd->data_length < 6) {
-		pr_err("PRIN SA REPORT_CAPABILITIES SCSI Data Length:"
-			" %u too small\n", cmd->data_length);
+	if (cmd->data_length < 8) {
+		pr_err("PRIN SA READ_RESERVATIONS SCSI Data Length: %u"
+			" too small\n", cmd->data_length);
 		return TCM_INVALID_CDB_FIELD;
 	}
 
@@ -3834,6 +4007,38 @@ core_scsi3_pri_report_capabilities(struc
 	if (!buf)
 		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
 
+	if (dev->transport->pr_ops
+			&& dev->transport->pr_ops->pr_read_reservation) {
+		ret = dev->transport->pr_ops->pr_read_reservation(cmd, buf,
+							   cmd->data_length);
+	} else {
+		ret = core_scsi3_pri_read_reservation_execute(cmd, buf,
+						       cmd->data_length);
+	}
+	if (ret)
+		goto err_unmap;
+
+	ret = TCM_NO_SENSE;
+err_unmap:
+	transport_kunmap_data_sg(cmd);
+
+	return ret;
+}
+
+
+/*
+ * PERSISTENT_RESERVE_IN Service Action REPORT_CAPABILITIES
+ *
+ * See spc4r17 section 6.13.4 Table 165
+ */
+static sense_reason_t
+core_scsi3_pri_report_capabilities_execute(struct se_cmd *cmd,
+					   unsigned char *buf, u32 buf_len)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct t10_reservation *pr_tmpl = &dev->t10_pr;
+	u16 len = 8; /* Hardcoded to 8. */
+
 	put_unaligned_be16(len, &buf[0]);
 	buf[2] |= 0x10; /* CRH: Compatible Reservation Hanlding bit. */
 	buf[2] |= 0x08; /* SIP_C: Specify Initiator Ports Capable bit */
@@ -3865,41 +4070,68 @@ core_scsi3_pri_report_capabilities(struc
 
 	target_set_cmd_data_length(cmd, len);
 
-	transport_kunmap_data_sg(cmd);
-
 	return 0;
 }
 
 /*
+ * PERSISTENT_RESERVE_IN Service Action REPORT_CAPABILITIES
+ *
+ * See spc4r17 section 6.13.4 Table 165
+ */
+static sense_reason_t
+core_scsi3_pri_report_capabilities(struct se_cmd *cmd)
+{
+	struct se_device *dev = cmd->se_dev;
+	unsigned char *buf;
+	sense_reason_t ret;
+
+	if (cmd->data_length < 6) {
+		pr_err("PRIN SA REPORT_CAPABILITIES SCSI Data Length:"
+			" %u too small\n", cmd->data_length);
+		return TCM_INVALID_CDB_FIELD;
+	}
+
+	buf = transport_kmap_data_sg(cmd);
+	if (!buf)
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+
+	if (dev->transport->pr_ops
+			&& dev->transport->pr_ops->pr_report_capabilities) {
+		ret = dev->transport->pr_ops->pr_report_capabilities(cmd, buf,
+							   cmd->data_length);
+	} else {
+		ret = core_scsi3_pri_report_capabilities_execute(cmd, buf,
+						       cmd->data_length);
+	}
+	if (ret)
+		goto err_unmap;
+
+	ret = TCM_NO_SENSE;
+err_unmap:
+	transport_kunmap_data_sg(cmd);
+
+	return ret;
+}
+/*
  * PERSISTENT_RESERVE_IN Service Action READ_FULL_STATUS
  *
  * See spc4r17 section 6.13.5 Table 168 and 169
  */
 static sense_reason_t
-core_scsi3_pri_read_full_status(struct se_cmd *cmd)
+core_scsi3_pri_read_full_status_execute(struct se_cmd *cmd, unsigned char *buf,
+					u32 buf_len)
 {
 	struct se_device *dev = cmd->se_dev;
 	struct se_node_acl *se_nacl;
 	struct se_portal_group *se_tpg;
 	struct t10_pr_registration *pr_reg, *pr_reg_tmp;
 	struct t10_reservation *pr_tmpl = &dev->t10_pr;
-	unsigned char *buf;
 	u32 add_desc_len = 0, add_len = 0;
 	u32 off = 8; /* off into first Full Status descriptor */
 	int format_code = 0, pr_res_type = 0, pr_res_scope = 0;
 	int exp_desc_len, desc_len;
 	bool all_reg = false;
 
-	if (cmd->data_length < 8) {
-		pr_err("PRIN SA READ_FULL_STATUS SCSI Data Length: %u"
-			" too small\n", cmd->data_length);
-		return TCM_INVALID_CDB_FIELD;
-	}
-
-	buf = transport_kmap_data_sg(cmd);
-	if (!buf)
-		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
-
 	put_unaligned_be32(dev->t10_pr.pr_generation, &buf[0]);
 
 	spin_lock(&dev->dev_reservation_lock);
@@ -4027,14 +4259,53 @@ core_scsi3_pri_read_full_status(struct s
 	put_unaligned_be32(add_len, &buf[4]);
 	target_set_cmd_data_length(cmd, 8 + add_len);
 
+	return 0;
+}
+
+/*
+ * PERSISTENT_RESERVE_IN Service Action READ_FULL_STATUS
+ *
+ * See spc4r17 section 6.13.5 Table 168 and 169
+ */
+static sense_reason_t
+core_scsi3_pri_read_full_status(struct se_cmd *cmd)
+{
+	struct se_device *dev = cmd->se_dev;
+	unsigned char *buf;
+	sense_reason_t ret;
+
+	if (cmd->data_length < 8) {
+		pr_err("PRIN SA READ_FULL_STATUS SCSI Data Length: %u"
+			" too small\n", cmd->data_length);
+		return TCM_INVALID_CDB_FIELD;
+	}
+
+	buf = transport_kmap_data_sg(cmd);
+	if (!buf)
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+
+	if (dev->transport->pr_ops
+			&& dev->transport->pr_ops->pr_read_full_status) {
+		ret = dev->transport->pr_ops->pr_read_full_status(cmd, buf,
+							   cmd->data_length);
+	} else {
+		ret = core_scsi3_pri_read_full_status_execute(cmd, buf,
+							      cmd->data_length);
+	}
+	if (ret)
+		goto err_unmap;
+
+	ret = TCM_NO_SENSE;
+err_unmap:
 	transport_kunmap_data_sg(cmd);
 
-	return 0;
+	return ret;
 }
 
 sense_reason_t
 target_scsi3_emulate_pr_in(struct se_cmd *cmd)
 {
+	struct se_device *dev = cmd->se_dev;
 	sense_reason_t ret;
 
 	/*
@@ -4046,7 +4317,13 @@ target_scsi3_emulate_pr_in(struct se_cmd
 	 * initiator or service action and shall terminate with a RESERVATION
 	 * CONFLICT status.
 	 */
-	if (cmd->se_dev->dev_reservation_flags & DRF_SPC2_RESERVATIONS) {
+	if (dev->transport->pr_ops && dev->transport->pr_ops->check_conflict) {
+		ret = dev->transport->pr_ops->check_conflict(cmd,
+						TARGET_PR_CHECK_SCSI2_ANY);
+		if (ret) {
+			return ret;
+		}
+	} else if (cmd->se_dev->dev_reservation_flags & DRF_SPC2_RESERVATIONS) {
 		pr_err("Received PERSISTENT_RESERVE CDB while legacy"
 			" SPC-2 reservation is held, returning"
 			" RESERVATION_CONFLICT\n");
@@ -4077,6 +4354,22 @@ target_scsi3_emulate_pr_in(struct se_cmd
 	return ret;
 }
 
+static sense_reason_t
+target_check_reservation_execute(struct se_cmd *cmd)
+{
+	struct se_device *dev = cmd->se_dev;
+	sense_reason_t ret;
+
+	spin_lock(&dev->dev_reservation_lock);
+	if (dev->dev_reservation_flags & DRF_SPC2_RESERVATIONS)
+		ret = target_scsi2_reservation_check(cmd);
+	else
+		ret = target_scsi3_pr_reservation_check(cmd);
+	spin_unlock(&dev->dev_reservation_lock);
+
+	return ret;
+}
+
 sense_reason_t
 target_check_reservation(struct se_cmd *cmd)
 {
@@ -4092,12 +4385,12 @@ target_check_reservation(struct se_cmd *
 	if (dev->transport_flags & TRANSPORT_FLAG_PASSTHROUGH_PGR)
 		return 0;
 
-	spin_lock(&dev->dev_reservation_lock);
-	if (dev->dev_reservation_flags & DRF_SPC2_RESERVATIONS)
-		ret = target_scsi2_reservation_check(cmd);
+	if (dev->transport->pr_ops && dev->transport->pr_ops->check_conflict)
+		ret = dev->transport->pr_ops->check_conflict(cmd,
+						TARGET_PR_CHECK_SCSI2_SCSI3);
 	else
-		ret = target_scsi3_pr_reservation_check(cmd);
-	spin_unlock(&dev->dev_reservation_lock);
+		ret = target_check_reservation_execute(cmd);
 
 	return ret;
 }
+
diff -uprN a/drivers/target/target_core_pr.h b/drivers/target/target_core_pr.h
--- a/drivers/target/target_core_pr.h	2024-04-24 01:22:34.572816065 +0200
+++ b/drivers/target/target_core_pr.h	2024-04-24 01:23:17.431494041 +0200
@@ -73,6 +73,8 @@ extern void core_scsi3_free_pr_reg_from_
 extern void core_scsi3_free_all_registrations(struct se_device *);
 extern unsigned char *core_scsi3_pr_dump_type(int);
 
+extern int core_scsi3_pr_seq_non_holder(struct se_cmd *, u32, char *, bool);
+
 extern sense_reason_t target_scsi3_emulate_pr_in(struct se_cmd *);
 extern sense_reason_t target_scsi3_emulate_pr_out(struct se_cmd *);
 extern sense_reason_t target_check_reservation(struct se_cmd *);
diff -uprN a/drivers/target/target_core_rbd.c b/drivers/target/target_core_rbd.c
--- a/drivers/target/target_core_rbd.c	2024-04-24 01:22:34.572816065 +0200
+++ b/drivers/target/target_core_rbd.c	2024-04-24 01:23:17.711511306 +0200
@@ -40,6 +40,10 @@
 
 #include "target_core_rbd.h"
 
+#define TCM_RBD_QUEUE_DEPTH 512
+
+static void tcm_rbd_warn(struct rbd_device *rbd_dev, const char *fmt, ...);
+
 static inline struct tcm_rbd_dev *TCM_RBD_DEV(struct se_device *dev)
 {
 	return container_of(dev, struct tcm_rbd_dev, dev);
@@ -75,6 +79,10 @@ static struct se_device *tcm_rbd_alloc_d
 	return &tcm_rbd_dev->dev;
 }
 
+
+static int tcm_rbd_detect_legacy_capacity(struct se_device *dev);
+static int tcm_rbd_read_legacy_capacity_xattr(struct se_device *dev);
+
 static int tcm_rbd_configure_device(struct se_device *dev)
 {
 	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
@@ -107,7 +115,7 @@ static int tcm_rbd_configure_device(stru
 
 	dev->dev_attrib.hw_block_size = bdev_logical_block_size(bd);
 	dev->dev_attrib.hw_max_sectors = queue_max_hw_sectors(q);
-	dev->dev_attrib.hw_queue_depth = q->nr_requests;
+	dev->dev_attrib.hw_queue_depth = TCM_RBD_QUEUE_DEPTH;
 
 	if (target_configure_unmap_from_queue(&dev->dev_attrib, q))
 		pr_debug("RBD: BLOCK Discard support available,"
@@ -132,22 +140,35 @@ static int tcm_rbd_configure_device(stru
 		/* blkdev_put() called in destroy_device */
 		return -EINVAL;
 	}
-	if (tcm_rbd_dev->emulate_legacy_capacity
-	    /* (1ULL<<3) = RBD_FEATURE_OBJECT_MAP */
-	 && tcm_rbd_dev->rbd_dev->header.features & (1ULL<<3)) {
+	    
+	if (tcm_rbd_dev->rbd_dev->header.features & (1ULL<<3)) {
+		/* (1ULL<<3) = RBD_FEATURE_OBJECT_MAP */
 		/*
 		 * bsc#1177109 and bsc#1181366: we permit image overrun for
 		 * legacy (non object-map) images when emulate_legacy_capacity
 		 * is enabled. Image overrun musn't be permitted alongside
 		 * RBD_FEATURE_OBJECT_MAP.
-		 */
-		pr_err("RBD: emulate_legacy_capacity must be disabled for "
-		       "RBD_FEATURE_OBJECT_MAP images\n");
-		return -EINVAL;
+		 */	
+		tcm_rbd_dev->emulate_legacy_capacity = false;
 	}
-
-	/* disable standalone reservation handling */
-	dev->dev_attrib.emulate_pr = 0;
+	else {
+		/* legacy (non object-map) images */
+		int legacy_capacity_detected = tcm_rbd_detect_legacy_capacity(dev);
+		if(legacy_capacity_detected < 0) {
+			pr_err("%s legacy capacity detection error\n",__func__);   
+			return -EINVAL;
+		}	
+		if(tcm_rbd_read_legacy_capacity_xattr(dev) < 0) {
+			pr_err("%s legacy capacity xattr read error\n",__func__);   
+			return -EINVAL;
+		}			
+		if(legacy_capacity_detected==1 && !tcm_rbd_dev->emulate_legacy_capacity) {
+			pr_err("%s Detected legacy capacity but incorrectly disabled.\n",__func__);
+			return -EINVAL;
+		}
+	}
+	
+	dev->dev_attrib.emulate_pr = 1;
 
 	return 0;
 }
@@ -290,7 +311,6 @@ tcm_rbd_execute_cmd(struct se_cmd *cmd,
 	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(cmd->se_dev);
 	struct tcm_rbd_cmd *trc;
 	struct rbd_img_request *img_request;
-	struct ceph_snap_context *snapc = NULL;
 	u64 mapping_size;
 	struct tcm_rbd_sync_notify sync_notify = {
 		0,
@@ -303,13 +323,13 @@ tcm_rbd_execute_cmd(struct se_cmd *cmd,
 
 	if (!length) {
 		dout("%s: zero-length request\n", __func__);
-		goto err;
+		goto err_1;
 	}
 
 	if (op_type != OBJ_OP_READ && rbd_dev->spec->snap_id != CEPH_NOSNAP) {
 		pr_warn("write or %d on read-only snapshot", op_type);
 		sense = TCM_WRITE_PROTECTED;
-		goto err;
+		goto err_1;
 	}
 
 	/*
@@ -322,52 +342,44 @@ tcm_rbd_execute_cmd(struct se_cmd *cmd,
 		pr_warn("request for non-existent snapshot");
 		BUG_ON(rbd_dev->spec->snap_id == CEPH_NOSNAP);
 		sense = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
-		goto err;
+		goto err_1;
 	}
 
 	if (offset && length > U64_MAX - offset + 1) {
 		pr_warn("bad request range (%llu~%llu)", offset, length);
 		sense = TCM_INVALID_CDB_FIELD;
-		goto err;	/* Shouldn't happen */
+		goto err_1;	/* Shouldn't happen */
 	}
 
-	/* See rbd_img_capture_header */
 	down_read(&rbd_dev->header_rwsem);
 	mapping_size = rbd_dev->mapping.size;
-	if (op_type != OBJ_OP_READ) {
-		snapc = rbd_dev->header.snapc;
-		ceph_get_snap_context(snapc);
-	}
 	up_read(&rbd_dev->header_rwsem);
-
+	
 	if (offset + length > mapping_size) {
-		pr_warn("beyond EOD (%llu~%llu > %llu)", offset,
+		tcm_rbd_warn(rbd_dev, "beyond EOD (%llu~%llu > %llu)", offset,
 			length, mapping_size);
 		if (!tcm_rbd_dev->emulate_legacy_capacity) {
 			sense = TCM_ADDRESS_OUT_OF_RANGE;
-			goto err_snapc;
+			goto err_1;
 		}
 	}
 
-	trc = kzalloc(sizeof(struct tcm_rbd_cmd), GFP_KERNEL);
-	if (!trc) {
-		sense = TCM_OUT_OF_RESOURCES;
-		goto err_snapc;
-	}
-
 	img_request = rbd_img_request_create(rbd_dev, op_type,
 			sync ? tcm_rbd_sync_callback : tcm_rbd_async_callback);
 	if (!img_request) {
 		sense = TCM_OUT_OF_RESOURCES;
-		goto err_trc;
+		goto err_1;
 	}
-	/* snapc is now owned by img_request - see rbd_img_request_destroy */
-	img_request->snapc = snapc;
-	snapc = NULL; /* img_request consumes a ref */
-	trc->img_request = img_request;
-
+	
 	pr_debug("rbd_dev %p img_req %p %d %llu~%llu\n", rbd_dev,
-	     img_request, op_type, offset, length);
+	     img_request, op_type, offset, length);	
+	
+	trc = kzalloc(sizeof(struct tcm_rbd_cmd), GFP_KERNEL);
+	if (!trc) {
+		sense = TCM_OUT_OF_RESOURCES;
+		goto err_2;
+	}
+	trc->img_request = img_request;
 
 	if (op_type == OBJ_OP_DISCARD || op_type == OBJ_OP_ZEROOUT)
 		result = rbd_img_fill_nodata(img_request, offset, length);
@@ -385,10 +397,10 @@ tcm_rbd_execute_cmd(struct se_cmd *cmd,
 	}
 	if (result == -ENOMEM) {
 		sense = TCM_OUT_OF_RESOURCES;
-		goto err_img_request;
+		goto err_3;
 	} else if (result) {
 		sense = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
-		goto err_img_request;
+		goto err_3;
 	}
 
 	if (sync) {
@@ -406,22 +418,23 @@ tcm_rbd_execute_cmd(struct se_cmd *cmd,
 			sense = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
 		else
 			sense = TCM_NO_SENSE;
-		goto err_img_request;
+		goto err_3;
 	}
 
 	return TCM_NO_SENSE;
 
-err_img_request:
+err_3:	
+	if (trc) {
+		if (trc->bvecs)
+			kfree(trc->bvecs);
+		kfree(trc);		
+	}
+err_2:
 	rbd_img_request_destroy(img_request);
-err_trc:
-	kfree(trc->bvecs);
-	kfree(trc);
-err_snapc:
+err_1:
 	if (sense)
 		pr_warn("RBD op type %d %llx at %llx sense %d",
 			op_type, length, offset, sense);
-	ceph_put_snap_context(snapc);
-err:
 	return sense;
 }
 
@@ -570,7 +583,6 @@ tcm_rbd_execute_cmp_and_write(struct se_
 	struct rbd_device *rbd_dev = tcm_rbd_dev->rbd_dev;
 	struct tcm_rbd_cmd *trc;
 	struct rbd_img_request *img_request;
-	struct ceph_snap_context *snapc = NULL;
 	u64 mapping_size;
 	sense_reason_t sense = TCM_NO_SENSE;
 	u64 offset = rbd_lba_shift(dev, cmd->t_task_lba);
@@ -586,27 +598,27 @@ tcm_rbd_execute_cmp_and_write(struct se_
 	if (rbd_dev->spec->snap_id != CEPH_NOSNAP) {
 		pr_warn("compare-and-write on read-only snapshot");
 		sense = TCM_WRITE_PROTECTED;
-		goto err;
+		goto err_1;
 	}
 
 	if (!test_bit(RBD_DEV_FLAG_EXISTS, &rbd_dev->flags)) {
 		pr_warn("request for non-existent snapshot");
 		BUG_ON(rbd_dev->spec->snap_id == CEPH_NOSNAP);
 		sense = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
-		goto err;
+		goto err_1;
 	}
 
 	if (offset && length > U64_MAX - offset + 1) {
 		pr_warn("bad request range (%llu~%llu)", offset, length);
 		sense = TCM_INVALID_CDB_FIELD;
-		goto err;	/* Shouldn't happen */
+		goto err_1;	/* Shouldn't happen */
 	}
-
-	/* See rbd_img_capture_header */
+	
+	if (rbd_dev_block_writes(rbd_dev)) 
+		rbd_dev_wait_on_bloked_writes(rbd_dev, 20);
+	
 	down_read(&rbd_dev->header_rwsem);
 	mapping_size = rbd_dev->mapping.size;
-	snapc = rbd_dev->header.snapc;
-	ceph_get_snap_context(snapc);
 	up_read(&rbd_dev->header_rwsem);
 
 	/*
@@ -619,36 +631,33 @@ tcm_rbd_execute_cmp_and_write(struct se_
 			length, mapping_size);
 		if (!tcm_rbd_dev->emulate_legacy_capacity) {
 			sense = TCM_ADDRESS_OUT_OF_RANGE;
-			goto err_snapc;
+			goto err_1;
 		}
 	}
 
 	/* need twice as much data for each compare & write operation */
 	if (cmd->data_length < length * 2) {
 		sense = TCM_INVALID_CDB_FIELD;
-		goto err_snapc;
-	}
-
-	trc = kzalloc(sizeof(struct tcm_rbd_cmd), GFP_KERNEL);
-	if (!trc) {
-		sense = TCM_OUT_OF_RESOURCES;
-		goto err_snapc;
+		goto err_1;
 	}
 
 	img_request = rbd_img_request_create(rbd_dev, OBJ_OP_CMP_AND_WRITE,
 					     tcm_rbd_cmp_and_write_callback);
 	if (!img_request) {
 		sense = TCM_OUT_OF_RESOURCES;
-		goto err_trc;
+		goto err_1;
 	}
-	/* snapc is now owned by img_request - see rbd_img_request_destroy */
-	img_request->snapc = snapc;
-	snapc = NULL;
-	trc->img_request = img_request;
-
+	
 	pr_debug("rbd_dev %p compare-and-write img_req %p %llu~%llu\n",
 		 rbd_dev, img_request, offset, length);
-
+	
+	trc = kzalloc(sizeof(struct tcm_rbd_cmd), GFP_KERNEL);
+	if (!trc) {
+		sense = TCM_OUT_OF_RESOURCES;
+		goto err_2;
+	}
+	trc->img_request = img_request;
+	
 	/*
 	 * data in cmd->t_data_sg is arrange as:
 	 * [len * data for compare | len * data for write]
@@ -667,10 +676,10 @@ tcm_rbd_execute_cmp_and_write(struct se_
 
 	if (result == -ENOMEM) {
 		sense = TCM_OUT_OF_RESOURCES;
-		goto err_img_request;
+		goto err_3;
 	} else if (result) {
 		sense = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
-		goto err_img_request;
+		goto err_3;
 	}
 
 	img_request->lio_cmd_data = cmd;
@@ -680,18 +689,20 @@ tcm_rbd_execute_cmp_and_write(struct se_
 
 	return TCM_NO_SENSE;
 
-err_img_request:
+	
+err_3:	
+	if (trc) {
+		if (trc->bvecs)
+			kfree(trc->bvecs);
+		kfree(trc);		
+	}
+err_2:
 	rbd_img_request_destroy(img_request);
-err_trc:
-	kfree(trc->bvecs);
-	kfree(trc);
-err_snapc:
+err_1:
 	if (sense)
 		pr_warn("RBD compare-and-write %llx at %llx sense %d",
 			length, offset, sense);
-	ceph_put_snap_context(snapc);
-err:
-	return sense;
+	return sense;	
 }
 
 enum {
@@ -892,6 +903,2481 @@ static bool tcm_rbd_get_write_cache(stru
 	return false;
 }
 
+
+
+#include <target/target_core_fabric.h>
+#include "target_core_pr.h"
+
+#define TCM_RBD_PR_INFO_XATTR_KEY "pr_info"
+#define TCM_RBD_PR_INFO_XATTR_VERS 1
+
+#define TCM_RBD_PR_INFO_XATTR_FIELD_VER		0
+#define TCM_RBD_PR_INFO_XATTR_FIELD_SEQ		1
+#define TCM_RBD_PR_INFO_XATTR_FIELD_SCSI2_RSV	2
+#define TCM_RBD_PR_INFO_XATTR_FIELD_GEN		3
+#define TCM_RBD_PR_INFO_XATTR_FIELD_SCSI3_RSV	4
+#define TCM_RBD_PR_INFO_XATTR_FIELD_NUM_REGS	5
+#define TCM_RBD_PR_INFO_XATTR_FIELD_REGS_START	6
+
+#define TCM_RBD_PR_INFO_XATTR_VAL_SCSI3_RSV_ABSENT	"No SPC-3 Reservation holder"
+#define TCM_RBD_PR_INFO_XATTR_VAL_SCSI2_RSV_ABSENT	"No SPC-2 Reservation holder"
+
+/* don't allow encoded PR info to exceed 80K */
+#define TCM_RBD_PR_INFO_XATTR_MAX_SIZE 81920
+
+/*
+ * TRANSPORT_IQN_LEN + strlen(",i,0x") + sizeof(u64) * 2 + strlen(",")
+ *	+ TRANSPORT_IQN_LEN + strlen(",t,0x") + sizeof(u32) * 2 + sizeof("\0") =
+ */
+#define TCM_RBD_PR_IT_NEXUS_MAXLEN	484
+
+/* number of retries amid concurrent PR info changes from other nodes */
+#define TCM_RBD_PR_REG_MAX_RETRIES	5
+
+/*
+ * Persistent reservation info. This structure is converted to and from a
+ * string for storage within an RBD object xattr. String based storage allows
+ * us to use xattr compare and write operations for atomic PR info updates.
+ */
+struct tcm_rbd_pr_rsv {
+	u64 key;		/* registered key */
+	/*
+	 * I-T nexus for reservation. Separate to reg, so that all_tg_pt flag
+	 * can be supported in future.
+	 */
+	char it_nexus[TCM_RBD_PR_IT_NEXUS_MAXLEN];
+	int type;		/* PR_TYPE_... */
+	/* scope is always PR_SCOPE_LU_SCOPE */
+};
+#define TCM_RBD_PR_INFO_XATTR_ENCODED_PR_RSV_MAXLEN		\
+	((sizeof("0x") + sizeof(u64) * 2) + sizeof(" ") +	\
+	 TCM_RBD_PR_IT_NEXUS_MAXLEN + sizeof(" ") +		\
+	 (sizeof("0x") + sizeof(u64) * 2) + sizeof("\n"))
+
+struct tcm_rbd_pr_reg {
+	struct list_head regs_node;
+	u64 key;		/* registered key */
+	/* I-T nexus for registration */
+	char it_nexus[TCM_RBD_PR_IT_NEXUS_MAXLEN];
+};
+#define TCM_RBD_PR_INFO_XATTR_ENCODED_PR_REG_MAXLEN		\
+	((sizeof("0x") + sizeof(u64) * 2) + sizeof(" ") +	\
+	 TCM_RBD_PR_IT_NEXUS_MAXLEN + sizeof("\n"))
+
+struct tcm_rbd_scsi2_rsv {
+	/*
+	 * I-T nexus for SCSI2 (RESERVE/RELEASE) reservation.
+	 */
+	char it_nexus[TCM_RBD_PR_IT_NEXUS_MAXLEN];
+};
+#define TCM_RBD_PR_INFO_XATTR_ENCODED_SCSI2_RSV_MAXLEN		\
+	(TCM_RBD_PR_IT_NEXUS_MAXLEN + sizeof("\n"))
+
+struct tcm_rbd_pr_info {
+	u32 vers;		/* on disk format version number */
+	u32 seq; 		/* sequence number bumped every xattr write */
+	struct tcm_rbd_scsi2_rsv *scsi2_rsv; /* SCSI2 reservation if any */
+	u32 gen; 		/* PR generation number */
+	struct tcm_rbd_pr_rsv *rsv;	/* SCSI3 reservation if any */
+	u32 num_regs;		/* number of registrations */
+	struct list_head regs;	/* list of registrations */
+};
+#define TCM_RBD_PR_INFO_XATTR_ENCODED_MAXLEN(_num_regs)			\
+	((sizeof("0x") + sizeof(u32) * 2) + sizeof("\n") +		\
+	 (sizeof("0x") + sizeof(u32) * 2) + sizeof("\n") +		\
+	 TCM_RBD_PR_INFO_XATTR_ENCODED_SCSI2_RSV_MAXLEN + 		\
+	 (sizeof("0x") + sizeof(u32) * 2) + sizeof("\n") +		\
+	 TCM_RBD_PR_INFO_XATTR_ENCODED_PR_RSV_MAXLEN +	 		\
+	 (sizeof("0x") + sizeof(u32) * 2) + sizeof("\n") +		\
+	 (TCM_RBD_PR_INFO_XATTR_ENCODED_PR_REG_MAXLEN * _num_regs) +	\
+	 sizeof("\0"))
+
+#define PR_CACHE_TIMEOUT_SEC  3
+	
+static int
+tcm_rbd_gen_it_nexus(struct se_session *se_sess,
+		     char *nexus_buf,
+		     size_t buflen)
+{
+	struct se_portal_group *se_tpg;
+	const struct target_core_fabric_ops *tfo;
+	u32 tpg_tag = 0;
+	char *tpg_wwn = "";
+	int rc;
+
+	if (!se_sess || !se_sess->se_node_acl || !se_sess->se_tpg
+					|| !se_sess->se_tpg->se_tpg_tfo) {
+		pr_warn("invalid session for IT nexus generation\n");
+		return -EINVAL;
+	}
+
+	se_tpg = se_sess->se_tpg;
+	tfo = se_tpg->se_tpg_tfo;
+
+	/*
+	 * nexus generation may be coming from an xcopy, in which case tfo
+	 * refers to xcopy_pt_tfo (tpg_get_wwn and tpg_get_tag are NULL).
+	 */
+	if (tfo->tpg_get_tag) {
+		tpg_tag = tfo->tpg_get_tag(se_tpg);
+	}
+	if (tfo->tpg_get_wwn) {
+		tpg_wwn = tfo->tpg_get_wwn(se_tpg);
+	}
+
+	rc = snprintf(nexus_buf, buflen, "%s,i,0x%llx,%s,t,0x%x",
+		      se_sess->se_node_acl->initiatorname,
+		      se_sess->sess_bin_isid,
+		      tpg_wwn,
+		      tpg_tag);
+	if ((rc < 0) || (rc >= buflen)) {
+		pr_err("error formatting reserve cookie\n");
+		return -EINVAL;
+	}
+
+	pr_debug("generated nexus: %s\n", nexus_buf);
+
+	return 0;
+}
+
+static void
+tcm_rbd_pr_info_free(struct tcm_rbd_pr_info *pr_info)
+{
+	struct tcm_rbd_pr_reg *reg;
+	struct tcm_rbd_pr_reg *reg_n;
+
+	kfree(pr_info->scsi2_rsv);
+	kfree(pr_info->rsv);
+	list_for_each_entry_safe(reg, reg_n, &pr_info->regs, regs_node) {
+		kfree(reg);
+	}
+	kfree(pr_info);
+}
+
+static bool
+tcm_rbd_is_rsv_holder(struct tcm_rbd_pr_rsv *rsv, struct tcm_rbd_pr_reg *reg,
+		      bool *rsv_is_all_reg)
+{
+	BUG_ON(!rsv);
+	BUG_ON(!reg);
+	if ((rsv->type == PR_TYPE_WRITE_EXCLUSIVE_ALLREG)
+			|| (rsv->type == PR_TYPE_EXCLUSIVE_ACCESS_ALLREG)) {
+		/* any registeration is a reservation holder */
+		if (rsv_is_all_reg)
+			*rsv_is_all_reg = true;
+		return true;
+	}
+	if (rsv_is_all_reg)
+		*rsv_is_all_reg = false;
+
+	if ((rsv->key == reg->key)
+	 && !strncmp(rsv->it_nexus, reg->it_nexus, ARRAY_SIZE(rsv->it_nexus))) {
+		return true;
+	}
+
+	return false;
+}
+
+static int
+tcm_rbd_pr_info_rsv_set(struct tcm_rbd_pr_info *pr_info, u64 key, char *nexus,
+			int type)
+{
+	struct tcm_rbd_pr_rsv *rsv;
+
+	if (pr_info->rsv != NULL) {
+		pr_err("rsv_set called with existing reservation\n");
+		return -EINVAL;
+	}
+
+	rsv = kmalloc(sizeof(*rsv), GFP_KERNEL);
+	if (!rsv) {
+		return -ENOMEM;
+	}
+
+	rsv->key = key;
+	strlcpy(rsv->it_nexus, nexus, ARRAY_SIZE(rsv->it_nexus));
+	rsv->type = type;
+
+	pr_info->rsv = rsv;
+
+	dout("pr_info rsv set: 0x%llx %s %d\n", key, nexus, type);
+
+	return 0;
+}
+
+static void
+tcm_rbd_pr_info_rsv_clear(struct tcm_rbd_pr_info *pr_info)
+{
+	kfree(pr_info->rsv);
+	pr_info->rsv = NULL;
+
+	dout("pr_info rsv cleared\n");
+}
+
+static int
+tcm_rbd_pr_info_append_reg(struct tcm_rbd_pr_info *pr_info, char *nexus,
+			   u64 key)
+{
+	struct tcm_rbd_pr_reg *reg;
+
+	reg = kmalloc(sizeof(*reg), GFP_KERNEL);
+	if (!reg) {
+		return -ENOMEM;
+	}
+
+	reg->key = key;
+	strlcpy(reg->it_nexus, nexus, ARRAY_SIZE(reg->it_nexus));
+
+	list_add_tail(&reg->regs_node, &pr_info->regs);
+	pr_info->num_regs++;
+
+	dout("appended pr_info reg: 0x%llx\n", reg->key);
+
+	return 0;
+}
+
+static void
+tcm_rbd_pr_info_clear_reg(struct tcm_rbd_pr_info *pr_info,
+			  struct tcm_rbd_pr_reg *reg)
+{
+	list_del(&reg->regs_node);
+	pr_info->num_regs--;
+
+	dout("deleted pr_info reg: 0x%llx\n", reg->key);
+
+	kfree(reg);
+}
+
+static int
+tcm_rbd_pr_info_unregister_reg(struct tcm_rbd_pr_info *pr_info,
+			       struct tcm_rbd_pr_reg *reg)
+{
+	struct tcm_rbd_pr_rsv *rsv;
+	bool all_reg = false;
+
+	rsv = pr_info->rsv;
+	if (rsv && tcm_rbd_is_rsv_holder(rsv, reg, &all_reg)) {
+		/*
+		 * If the persistent reservation holder is more than one I_T
+		 * nexus, the reservation shall not be released until the
+		 * registrations for all persistent reservation holder I_T
+		 * nexuses are removed.
+		 */
+		if (!all_reg || (pr_info->num_regs == 1)) {
+			pr_warn("implicitly releasing PR of type %d on "
+				"unregister from %s\n",
+				rsv->type, reg->it_nexus);
+			tcm_rbd_pr_info_rsv_clear(pr_info);
+		}
+	}
+
+	tcm_rbd_pr_info_clear_reg(pr_info, reg);
+
+	return 0;
+}
+
+static int
+tcm_rbd_pr_info_scsi2_rsv_set(struct tcm_rbd_pr_info *pr_info, char *nexus)
+{
+	struct tcm_rbd_scsi2_rsv *scsi2_rsv;
+
+	if (pr_info->scsi2_rsv != NULL) {
+		pr_err("rsv_set called with existing SCSI2 reservation\n");
+		return -EINVAL;
+	}
+
+	scsi2_rsv = kmalloc(sizeof(*scsi2_rsv), GFP_KERNEL);
+	if (!scsi2_rsv) {
+		return -ENOMEM;
+	}
+
+	strlcpy(scsi2_rsv->it_nexus, nexus, ARRAY_SIZE(scsi2_rsv->it_nexus));
+
+	pr_info->scsi2_rsv = scsi2_rsv;
+
+	dout("pr_info scsi2_rsv set: %s\n", nexus);
+
+	return 0;
+}
+
+static void
+tcm_rbd_pr_info_scsi2_rsv_clear(struct tcm_rbd_pr_info *pr_info)
+{
+	dout("pr_info scsi2_rsv clearing: %s\n", pr_info->scsi2_rsv->it_nexus);
+	kfree(pr_info->scsi2_rsv);
+	pr_info->scsi2_rsv = NULL;
+}
+
+static int
+tcm_rbd_pr_info_vers_decode(char *str, u32 *vers)
+{
+	int rc;
+
+	BUG_ON(!vers);
+	rc = sscanf(str, "0x%08x", vers);
+	if (rc != 1) {
+		pr_err("failed to decode PR info version in: %s\n", str);
+		return -EINVAL;
+	}
+
+	if (*vers != TCM_RBD_PR_INFO_XATTR_VERS) {
+		pr_err("unsupported PR info version: %u\n", *vers);
+		return -EINVAL;
+	}
+
+	dout("processed pr_info version: %u\n", *vers);
+	return 0;
+}
+
+static int
+tcm_rbd_pr_info_seq_decode(char *str, u32 *seq)
+{
+	int rc;
+
+	BUG_ON(!seq);
+	rc = sscanf(str, "0x%08x", seq);
+	if (rc != 1) {
+		pr_err("failed to decode PR info seqnum in: %s\n", str);
+		return -EINVAL;
+	}
+
+	dout("processed pr_info seqnum: %u\n", *seq);
+	return 0;
+}
+
+static int
+tcm_rbd_pr_info_scsi2_rsv_decode(char *str,
+				 struct tcm_rbd_scsi2_rsv **_scsi2_rsv)
+{
+	struct tcm_rbd_scsi2_rsv *scsi2_rsv;
+
+	BUG_ON(!_scsi2_rsv);
+	if (!strncmp(str, TCM_RBD_PR_INFO_XATTR_VAL_SCSI2_RSV_ABSENT,
+		     sizeof(TCM_RBD_PR_INFO_XATTR_VAL_SCSI2_RSV_ABSENT))) {
+		scsi2_rsv = NULL;
+	} else {
+		size_t n;
+
+		scsi2_rsv = kzalloc(sizeof(*scsi2_rsv), GFP_KERNEL);
+		if (!scsi2_rsv) {
+			return -ENOMEM;
+		}
+
+		n = strlcpy(scsi2_rsv->it_nexus, str,
+			    TCM_RBD_PR_IT_NEXUS_MAXLEN);
+		if (n >= TCM_RBD_PR_IT_NEXUS_MAXLEN) {
+			kfree(scsi2_rsv);
+			return -EINVAL;
+		}
+	}
+
+	dout("processed pr_info SCSI2 rsv: %s\n", str);
+	*_scsi2_rsv = scsi2_rsv;
+	return 0;
+}
+
+static int
+tcm_rbd_pr_info_gen_decode(char *str, u32 *gen)
+{
+	int rc;
+
+	BUG_ON(!gen);
+	rc = sscanf(str, "0x%08x", gen);
+	if (rc != 1) {
+		pr_err("failed to parse PR gen: %s\n", str);
+		return -EINVAL;
+	}
+	dout("processed pr_info generation: %s\n", str);
+	return 0;
+}
+
+static int
+tcm_rbd_pr_info_num_regs_decode(char *str, u32 *num_regs)
+{
+	int rc;
+
+	BUG_ON(!num_regs);
+	rc = sscanf(str, "0x%08x", num_regs);
+	if (rc != 1) {
+		pr_err("failed to parse PR num regs: %s\n", str);
+		return -EINVAL;
+	}
+	dout("processed pr_info num_regs: %s\n", str);
+	return 0;
+}
+
+static int
+tcm_rbd_pr_info_rsv_decode(char *str, struct tcm_rbd_pr_rsv **_rsv)
+{
+	struct tcm_rbd_pr_rsv *rsv;
+	int rc;
+
+	BUG_ON(!_rsv);
+	if (!strncmp(str, TCM_RBD_PR_INFO_XATTR_VAL_SCSI3_RSV_ABSENT,
+		    sizeof(TCM_RBD_PR_INFO_XATTR_VAL_SCSI3_RSV_ABSENT))) {
+		/* no reservation. Ensure pr_info->rsv is NULL */
+		rsv = NULL;
+	} else {
+		rsv = kzalloc(sizeof(*rsv), GFP_KERNEL);
+		if (!rsv) {
+			return -ENOMEM;
+		}
+
+		/* reservation key, I-T nexus, and type with space separators */
+		rc = sscanf(str, "0x%016llx %"
+			    __stringify(TCM_RBD_PR_IT_NEXUS_MAXLEN)
+			    "s 0x%08x", &rsv->key, rsv->it_nexus, &rsv->type);
+		if (rc != 3) {
+			pr_err("failed to parse PR rsv: %s\n", str);
+			kfree(rsv);
+			return -EINVAL;
+		}
+	}
+
+	dout("processed pr_info rsv: %s\n", str);
+	*_rsv = rsv;
+	return 0;
+}
+
+static int
+tcm_rbd_pr_info_reg_decode(char *str, struct tcm_rbd_pr_reg **_reg)
+{
+	struct tcm_rbd_pr_reg *reg;
+	int rc;
+
+	BUG_ON(!_reg);
+	reg = kzalloc(sizeof(*reg), GFP_KERNEL);
+	if (!reg) {
+		return -ENOMEM;
+	}
+
+	/* registration key and I-T nexus with space separator */
+	rc = sscanf(str, "0x%016llx %" __stringify(TCM_RBD_PR_IT_NEXUS_MAXLEN)
+		    "s", &reg->key, reg->it_nexus);
+	if (rc != 2) {
+		pr_err("failed to parse PR reg: %s\n", str);
+		kfree(reg);
+		return -EINVAL;
+	}
+
+	dout("processed pr_info reg: %s\n", str);
+	*_reg = reg;
+	return 0;
+}
+
+static int
+tcm_rbd_pr_info_decode(char *pr_xattr,
+		       int pr_xattr_len,
+		       struct tcm_rbd_pr_info **_pr_info)
+{
+	struct tcm_rbd_pr_info *pr_info;
+	int rc;
+	int field;
+	int i;
+	char *p;
+	char *str;
+	char *end;
+
+	BUG_ON(!_pr_info);
+
+	if (!pr_xattr_len) {
+		pr_err("zero length PR xattr\n");
+		return -EINVAL;
+	}
+
+	dout("decoding PR xattr: %s\n", pr_xattr);
+
+	pr_info = kzalloc(sizeof(*pr_info), GFP_KERNEL);
+	if (!pr_info) {
+		return -ENOMEM;
+	}
+
+	INIT_LIST_HEAD(&pr_info->regs);
+
+	p = pr_xattr;
+	end = pr_xattr + pr_xattr_len;
+	field = 0;
+	i = 0;
+	/*
+	 * '\n' separator between header fields and each reg entry.
+	 * reg subfields are further separated by ' '.
+	 */
+	for (str = strsep(&p, "\n"); str && *str != '\0' && (p < end);
+						str = strsep(&p, "\n")) {
+
+		if (field == TCM_RBD_PR_INFO_XATTR_FIELD_VER) {
+			rc = tcm_rbd_pr_info_vers_decode(str, &pr_info->vers);
+			if (rc < 0) {
+				goto err_info_free;
+			}
+		} else if (field == TCM_RBD_PR_INFO_XATTR_FIELD_SEQ) {
+			rc = tcm_rbd_pr_info_seq_decode(str, &pr_info->seq);
+			if (rc < 0) {
+				goto err_info_free;
+			}
+		} else if (field == TCM_RBD_PR_INFO_XATTR_FIELD_SCSI2_RSV) {
+			rc = tcm_rbd_pr_info_scsi2_rsv_decode(str,
+							&pr_info->scsi2_rsv);
+			if (rc < 0) {
+				goto err_info_free;
+			}
+		} else if (field == TCM_RBD_PR_INFO_XATTR_FIELD_GEN) {
+			rc = tcm_rbd_pr_info_gen_decode(str, &pr_info->gen);
+			if (rc < 0) {
+				goto err_info_free;
+			}
+		} else if (field == TCM_RBD_PR_INFO_XATTR_FIELD_SCSI3_RSV) {
+			rc = tcm_rbd_pr_info_rsv_decode(str, &pr_info->rsv);
+			if (rc < 0) {
+				goto err_info_free;
+			}
+		} else if (field == TCM_RBD_PR_INFO_XATTR_FIELD_NUM_REGS) {
+			rc = tcm_rbd_pr_info_num_regs_decode(str,
+							    &pr_info->num_regs);
+			if (rc < 0) {
+				goto err_info_free;
+			}
+		} else if (field >= TCM_RBD_PR_INFO_XATTR_FIELD_REGS_START) {
+			struct tcm_rbd_pr_reg *reg;
+			rc = tcm_rbd_pr_info_reg_decode(str, &reg);
+			if (rc < 0) {
+				goto err_info_free;
+			}
+			list_add_tail(&reg->regs_node, &pr_info->regs);
+			i++;
+		} else {
+			dout("skipping parsing of field %d\n", field);
+		}
+
+		field++;
+	}
+
+	if (field <= TCM_RBD_PR_INFO_XATTR_FIELD_NUM_REGS) {
+		pr_err("pr_info missing basic fields, stopped at %d\n", field);
+		rc = -EINVAL;
+		goto err_info_free;
+	}
+
+	if (i != pr_info->num_regs) {
+		pr_err("processed %d registrations, expected %d\n",
+			 i, pr_info->num_regs);
+		rc = -EINVAL;
+		goto err_info_free;
+	}
+
+	dout("successfully processed all PR data\n");
+	*_pr_info = pr_info;
+
+	return 0;
+
+err_info_free:
+	tcm_rbd_pr_info_free(pr_info);
+	return rc;
+}
+
+static int
+tcm_rbd_pr_info_vers_seq_encode(char *buf, size_t buf_remain, u32 vers, u32 seq)
+{
+	int rc;
+
+	rc = snprintf(buf, buf_remain, "0x%08x\n0x%08x\n",
+		      vers, seq);
+	if ((rc < 0) || (rc >= buf_remain)) {
+		pr_err("failed to encode PR vers and seq\n");
+		return -EINVAL;
+	}
+
+	return rc;
+}
+
+static int
+tcm_rbd_pr_info_scsi2_rsv_encode(char *buf, size_t buf_remain,
+				 struct tcm_rbd_scsi2_rsv *scsi2_rsv)
+{
+	int rc;
+
+	if (!scsi2_rsv) {
+		/* no reservation */
+		rc = snprintf(buf, buf_remain, "%s\n",
+			      TCM_RBD_PR_INFO_XATTR_VAL_SCSI2_RSV_ABSENT);
+	} else {
+		rc = snprintf(buf, buf_remain, "%s\n", scsi2_rsv->it_nexus);
+	}
+	if ((rc < 0) || (rc >= buf_remain)) {
+		pr_err("failed to encode SCSI2 reservation\n");
+		return -EINVAL;
+	}
+
+	return rc;
+}
+
+static int
+tcm_rbd_pr_info_gen_encode(char *buf, size_t buf_remain, u32 gen)
+{
+	int rc;
+
+	rc = snprintf(buf, buf_remain, "0x%08x\n", gen);
+	if ((rc < 0) || (rc >= buf_remain)) {
+		pr_err("failed to encode PR gen\n");
+		return -EINVAL;
+	}
+
+	return rc;
+}
+
+static int
+tcm_rbd_pr_info_rsv_encode(char *buf, size_t buf_remain,
+			   struct tcm_rbd_pr_rsv *rsv)
+{
+	int rc;
+
+	if (!rsv) {
+		/* no reservation */
+		rc = snprintf(buf, buf_remain, "%s\n",
+			      TCM_RBD_PR_INFO_XATTR_VAL_SCSI3_RSV_ABSENT);
+	} else {
+		rc = snprintf(buf, buf_remain, "0x%016llx %s 0x%08x\n",
+			      rsv->key, rsv->it_nexus, rsv->type);
+	}
+	if ((rc < 0) || (rc >= buf_remain)) {
+		pr_err("failed to encode PR reservation\n");
+		return -EINVAL;
+	}
+
+	return rc;
+}
+
+static int
+tcm_rbd_pr_info_num_regs_encode(char *buf, size_t buf_remain, u32 num_regs)
+{
+	int rc;
+
+	rc = snprintf(buf, buf_remain, "0x%08x\n", num_regs);
+	if ((rc < 0) || (rc >= buf_remain)) {
+		pr_err("failed to encode PR num_regs\n");
+		return -EINVAL;
+	}
+
+	return rc;
+}
+
+static int
+tcm_rbd_pr_info_reg_encode(char *buf, size_t buf_remain,
+			   struct tcm_rbd_pr_reg *reg)
+{
+	int rc;
+
+	rc = snprintf(buf, buf_remain, "0x%016llx %s\n", reg->key, reg->it_nexus);
+	if ((rc < 0) || (rc >= buf_remain)) {
+		pr_err("failed to encode PR registration\n");
+		return -EINVAL;
+	}
+
+	return rc;
+}
+
+static int
+tcm_rbd_pr_info_encode(struct tcm_rbd_pr_info *pr_info,
+		       char **_pr_xattr,
+		       int *pr_xattr_len)
+{
+	struct tcm_rbd_pr_reg *reg;
+	char *pr_xattr;
+	char *p;
+	size_t buf_remain;
+	int rc;
+	int i;
+
+	if (pr_info->vers != TCM_RBD_PR_INFO_XATTR_VERS) {
+		pr_err("unsupported PR info version: %u\n", pr_info->vers);
+		return -EINVAL;
+	}
+
+	buf_remain = TCM_RBD_PR_INFO_XATTR_ENCODED_MAXLEN(pr_info->num_regs);
+	if (buf_remain > TCM_RBD_PR_INFO_XATTR_MAX_SIZE) {
+		pr_err("PR info too large for encoding: %zd\n", buf_remain);
+		return -EINVAL;
+	}
+
+	dout("encoding PR info: vers=%u, seq=%u, gen=%u, num regs=%u into %zd "
+	     "bytes\n", pr_info->vers, pr_info->seq, pr_info->gen,
+	     pr_info->num_regs, buf_remain);
+
+	pr_xattr = kmalloc(buf_remain, GFP_KERNEL);
+	if (!pr_xattr) {
+		return -ENOMEM;
+	}
+
+	p = pr_xattr;
+	rc = tcm_rbd_pr_info_vers_seq_encode(p, buf_remain, pr_info->vers,
+					     pr_info->seq);
+	if (rc < 0) {
+		rc = -EINVAL;
+		goto err_xattr_free;
+	}
+
+	p += rc;
+	buf_remain -= rc;
+
+	rc = tcm_rbd_pr_info_scsi2_rsv_encode(p, buf_remain,
+					      pr_info->scsi2_rsv);
+	 if (rc < 0) {
+		rc = -EINVAL;
+		goto err_xattr_free;
+	}
+
+	p += rc;
+	buf_remain -= rc;
+
+	rc = tcm_rbd_pr_info_gen_encode(p, buf_remain, pr_info->gen);
+	if (rc < 0) {
+		rc = -EINVAL;
+		goto err_xattr_free;
+	}
+
+	p += rc;
+	buf_remain -= rc;
+
+	rc = tcm_rbd_pr_info_rsv_encode(p, buf_remain, pr_info->rsv);
+	 if (rc < 0) {
+		rc = -EINVAL;
+		goto err_xattr_free;
+	}
+
+	p += rc;
+	buf_remain -= rc;
+
+	rc = tcm_rbd_pr_info_num_regs_encode(p, buf_remain, pr_info->num_regs);
+	if (rc < 0) {
+		rc = -EINVAL;
+		goto err_xattr_free;
+	}
+
+	p += rc;
+	buf_remain -= rc;
+
+	i = 0;
+	list_for_each_entry(reg, &pr_info->regs, regs_node) {
+		rc = tcm_rbd_pr_info_reg_encode(p, buf_remain, reg);
+		 if (rc < 0) {
+			rc = -EINVAL;
+			goto err_xattr_free;
+		}
+
+		p += rc;
+		buf_remain -= rc;
+		i++;
+	}
+
+	if (i != pr_info->num_regs) {
+		pr_err("mismatch between PR num_regs and list entries!\n");
+		rc = -EINVAL;
+		goto err_xattr_free;
+	}
+
+	*_pr_xattr = pr_xattr;
+	/* +1 to include null term */
+	*pr_xattr_len = (p - pr_xattr) + 1;
+
+	dout("successfully encoded all %d PR regs into %d bytes: %s\n",
+	     pr_info->num_regs, *pr_xattr_len, pr_xattr);
+
+	return 0;
+
+err_xattr_free:
+	kfree(pr_xattr);
+	return rc;
+}
+
+static int
+tcm_rbd_pr_info_mock_empty(struct tcm_rbd_dev *tcm_rbd_dev,
+			   struct tcm_rbd_pr_info **_pr_info)
+{
+	struct tcm_rbd_pr_info *pr_info;
+
+	pr_info = kzalloc(sizeof(*pr_info), GFP_KERNEL);
+	if (!pr_info) {
+		return -ENOMEM;
+	}
+
+	pr_info->vers = TCM_RBD_PR_INFO_XATTR_VERS;
+	INIT_LIST_HEAD(&pr_info->regs);
+
+	*_pr_info = pr_info;
+	dout("successfully initialized mock PR info\n");
+
+	return 0;
+}
+
+static int
+tcm_rbd_pr_info_init(struct tcm_rbd_dev *tcm_rbd_dev,
+		     struct tcm_rbd_pr_info **_pr_info,
+		     char **_pr_xattr, int *_pr_xattr_len)
+{
+	struct tcm_rbd_pr_info *pr_info;
+	char *pr_xattr = NULL;
+	int pr_xattr_len = 0;
+	int rc;
+	struct rbd_device *rbd_dev = tcm_rbd_dev->rbd_dev;
+
+	pr_info = kzalloc(sizeof(*pr_info), GFP_KERNEL);
+	if (!pr_info) {
+		return -ENOMEM;
+	}
+
+	pr_info->vers = TCM_RBD_PR_INFO_XATTR_VERS;
+	INIT_LIST_HEAD(&pr_info->regs);
+	pr_info->seq = 1;
+
+	rc = tcm_rbd_pr_info_encode(pr_info, &pr_xattr,
+				    &pr_xattr_len);
+	if (rc) {
+		pr_warn("failed to encode PR xattr: %d\n", rc);
+		goto err_info_free;
+	}
+
+	mutex_lock(&rbd_dev->pr_mutex);
+	if( rbd_dev->pr_cached )
+		kfree(rbd_dev->pr_cached);
+	rbd_dev->pr_cached = NULL;
+	rbd_dev->pr_dirty = true;
+	mutex_unlock(&rbd_dev->pr_mutex);
+
+	rc = rbd_dev_setxattr(tcm_rbd_dev->rbd_dev,
+			      TCM_RBD_PR_INFO_XATTR_KEY,
+			      pr_xattr, pr_xattr_len);
+	if (rc) {
+		pr_warn("failed to set PR xattr: %d\n", rc);
+		goto err_xattr_free;
+	}
+
+	mutex_lock(&rbd_dev->pr_mutex);
+	rbd_dev->pr_cached = kstrdup(pr_xattr, GFP_KERNEL);
+	if (rbd_dev->pr_cached) {
+		rbd_dev->pr_dirty = false;
+		rbd_dev->pr_cache_ts = jiffies;
+	}
+	rbd_notify_scsi_pr_update(rbd_dev);
+
+	mutex_unlock(&rbd_dev->pr_mutex);
+
+	*_pr_info = pr_info;
+	if (_pr_xattr) {
+		BUG_ON(!_pr_xattr_len);
+		*_pr_xattr = pr_xattr;
+		*_pr_xattr_len = pr_xattr_len;
+	} else {
+		kfree(pr_xattr);
+	}
+	dout("successfully initialized PR info\n");
+
+	return 0;
+
+err_xattr_free:
+	kfree(pr_xattr);
+err_info_free:
+	kfree(pr_info);
+	return rc;
+}
+
+static int
+tcm_rbd_pr_info_get(struct tcm_rbd_dev *tcm_rbd_dev,
+		    struct tcm_rbd_pr_info **_pr_info,
+		    char **_pr_xattr, int *_pr_xattr_len)
+{
+	int rc;
+	char *pr_xattr = NULL;
+	char *dup_xattr = NULL;
+	int pr_xattr_len = 0;
+	struct tcm_rbd_pr_info *pr_info = NULL;
+	struct rbd_device *rbd_dev = tcm_rbd_dev->rbd_dev;
+
+	BUG_ON(!_pr_info);
+
+	mutex_lock(&rbd_dev->pr_mutex);
+	if( !rbd_dev->pr_dirty  && time_before(jiffies,rbd_dev->pr_cache_ts + HZ * PR_CACHE_TIMEOUT_SEC) ) {
+		if( !rbd_dev->pr_cached ) {
+			mutex_unlock(&rbd_dev->pr_mutex);
+			return  -ENODATA;
+		}
+		pr_xattr = kstrdup(rbd_dev->pr_cached, GFP_KERNEL);
+		mutex_unlock(&rbd_dev->pr_mutex);
+		if( !pr_xattr ) 
+			return -ENOMEM;
+	
+		pr_xattr_len = strlen(pr_xattr) + 1;
+	}
+	else {
+		rc = rbd_dev_getxattr(tcm_rbd_dev->rbd_dev, TCM_RBD_PR_INFO_XATTR_KEY,
+			      TCM_RBD_PR_INFO_XATTR_MAX_SIZE,
+			      (void **)&pr_xattr, &pr_xattr_len);
+		if( rc && rc != -ENODATA ) {
+			pr_warn("failed to obtain PR xattr: %d\n", rc);
+			mutex_unlock(&rbd_dev->pr_mutex);
+			return rc;
+		}
+
+		if( rbd_dev->pr_cached )
+			kfree(rbd_dev->pr_cached);
+
+		if( rc == -ENODATA ) {
+			rbd_dev->pr_cached = NULL;
+			rbd_dev->pr_dirty = false;
+			rbd_dev->pr_cache_ts = jiffies;
+			mutex_unlock(&rbd_dev->pr_mutex);
+			return rc;
+		}
+
+		if (pr_xattr) {
+			rbd_dev->pr_cached = kstrdup(pr_xattr, GFP_KERNEL);
+			if ( rbd_dev->pr_cached) {  
+				rbd_dev->pr_dirty = false;
+				rbd_dev->pr_cache_ts = jiffies;
+			}
+		} 
+		else {
+			rbd_dev->pr_cached = NULL;
+			rbd_dev->pr_dirty = false;
+			rbd_dev->pr_cache_ts = jiffies;
+		}
+
+		mutex_unlock(&rbd_dev->pr_mutex);
+	}
+	
+	if (_pr_xattr) {
+		/* dup before decode, which trashes @pr_xattr */
+		dup_xattr = kstrdup(pr_xattr, GFP_KERNEL);
+		if (!dup_xattr) {
+			rc = -ENOMEM;
+			goto err_xattr_free;
+		}
+	}
+
+	rc = tcm_rbd_pr_info_decode(pr_xattr, pr_xattr_len, &pr_info);
+	if (rc) {
+		pr_warn("failed to decode PR xattr: %d\n", rc);
+		goto err_dup_xattr_free;
+	}
+
+	if (_pr_xattr) {
+		BUG_ON(!_pr_xattr_len);
+		*_pr_xattr = dup_xattr;
+		*_pr_xattr_len = pr_xattr_len;
+	}
+	kfree(pr_xattr);
+	*_pr_info = pr_info;
+	dout("successfully obtained PR info\n");
+
+	return 0;
+
+err_dup_xattr_free:
+	kfree(dup_xattr);
+err_xattr_free:
+	kfree(pr_xattr);
+	return rc;
+}
+
+static int
+tcm_rbd_pr_info_replace(struct tcm_rbd_dev *tcm_rbd_dev,
+			char *pr_xattr_old, int pr_xattr_len_old,
+			struct tcm_rbd_pr_info *pr_info_new)
+{
+	int rc;
+	char *pr_xattr_new = NULL;
+	int pr_xattr_len_new = 0;
+	struct rbd_device *rbd_dev = tcm_rbd_dev->rbd_dev;
+
+	BUG_ON(!pr_xattr_old || !pr_info_new);
+
+	/* bump seqnum prior to xattr write. Not rolled back on failure */
+	pr_info_new->seq++;
+	rc = tcm_rbd_pr_info_encode(pr_info_new, &pr_xattr_new,
+				    &pr_xattr_len_new);
+	if (rc) {
+		pr_warn("failed to encode PR xattr: %d\n", rc);
+		return rc;
+	}
+
+	if (pr_xattr_len_new > TCM_RBD_PR_INFO_XATTR_MAX_SIZE) {
+		pr_err("unable to store oversize (%d) PR info: %s\n",
+		       pr_xattr_len_new, pr_xattr_new);
+		rc = -E2BIG;
+		goto err_xattr_new_free;
+	}
+
+	mutex_lock(&rbd_dev->pr_mutex);
+	if( rbd_dev->pr_cached )
+		kfree(rbd_dev->pr_cached);
+	rbd_dev->pr_cached = NULL;
+	rbd_dev->pr_dirty = true;
+	mutex_unlock(&rbd_dev->pr_mutex);
+
+	rc = rbd_dev_cmpsetxattr(tcm_rbd_dev->rbd_dev,
+				 TCM_RBD_PR_INFO_XATTR_KEY,
+				 pr_xattr_old, pr_xattr_len_old,
+				 pr_xattr_new, pr_xattr_len_new);
+	if (rc) {
+		pr_warn("failed to set PR xattr: %d\n", rc);
+		goto err_xattr_new_free;
+	}
+
+	mutex_lock(&rbd_dev->pr_mutex);
+	rbd_dev->pr_cached = kstrdup(pr_xattr_new, GFP_KERNEL);
+	if (rbd_dev->pr_cached) {
+		rbd_dev->pr_dirty = false;
+		rbd_dev->pr_cache_ts = jiffies;
+	}
+	rbd_notify_scsi_pr_update(rbd_dev);
+
+	mutex_unlock(&rbd_dev->pr_mutex);
+
+	dout("successfully replaced PR info\n");
+	rc = 0;
+err_xattr_new_free:
+	kfree(pr_xattr_new);
+
+	return 0;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_read_keys(struct se_cmd *cmd, unsigned char *buf,
+			     u32 buf_len)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	struct tcm_rbd_pr_info *pr_info = NULL;
+	struct tcm_rbd_pr_reg *reg;
+	u32 add_len = 0, off = 8;
+	int rc;
+
+	BUG_ON(buf_len < 8);
+
+	dout("getting pr_info for buf: %p, %u\n", buf, buf_len);
+
+	rc = tcm_rbd_pr_info_get(tcm_rbd_dev, &pr_info, NULL, NULL);
+	if (rc == -ENODATA) {
+		dout("PR info not present for read, mocking empty\n");
+		rc = tcm_rbd_pr_info_mock_empty(tcm_rbd_dev, &pr_info);
+	}
+	if (rc < 0)
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+
+	dout("packing read_keys response buf: %p, %u\n", buf, buf_len);
+
+	buf[0] = ((pr_info->gen >> 24) & 0xff);
+	buf[1] = ((pr_info->gen >> 16) & 0xff);
+	buf[2] = ((pr_info->gen >> 8) & 0xff);
+	buf[3] = (pr_info->gen & 0xff);
+
+	dout("packed gen %u in read_keys response\n", pr_info->gen);
+
+	list_for_each_entry(reg, &pr_info->regs, regs_node) {
+		add_len += 8;		
+		/*
+		 * Check for overflow of 8byte PRI READ_KEYS payload and
+		 * next reservation key list descriptor.
+		 */
+		if (add_len > (buf_len - 8))
+			continue;
+
+		buf[off++] = ((reg->key >> 56) & 0xff);
+		buf[off++] = ((reg->key >> 48) & 0xff);
+		buf[off++] = ((reg->key >> 40) & 0xff);
+		buf[off++] = ((reg->key >> 32) & 0xff);
+		buf[off++] = ((reg->key >> 24) & 0xff);
+		buf[off++] = ((reg->key >> 16) & 0xff);
+		buf[off++] = ((reg->key >> 8) & 0xff);
+		buf[off++] = (reg->key & 0xff);
+		dout("packed key 0x%llx in read_keys response\n", reg->key);
+	}
+
+	buf[4] = ((add_len >> 24) & 0xff);
+	buf[5] = ((add_len >> 16) & 0xff);
+	buf[6] = ((add_len >> 8) & 0xff);
+	buf[7] = (add_len & 0xff);
+	dout("packed len %u in read_keys response\n", add_len);
+	tcm_rbd_pr_info_free(pr_info);
+
+	return TCM_NO_SENSE;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_read_reservation(struct se_cmd *cmd, unsigned char *buf,
+				    u32 buf_len)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	struct tcm_rbd_pr_info *pr_info = NULL;
+	u64 pr_res_key;
+	u32 add_len = 16; /* Hardcoded to 16 when a reservation is held. */
+	int rc;
+
+	BUG_ON(buf_len < 8);
+
+	dout("getting pr_info for buf: %p, %u\n", buf, buf_len);
+
+	rc = tcm_rbd_pr_info_get(tcm_rbd_dev, &pr_info, NULL, NULL);
+	if (rc == -ENODATA) {
+		dout("PR info not present for read, mocking empty\n");
+		rc = tcm_rbd_pr_info_mock_empty(tcm_rbd_dev, &pr_info);
+	}
+	if (rc < 0)
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+
+	buf[0] = ((pr_info->gen >> 24) & 0xff);
+	buf[1] = ((pr_info->gen >> 16) & 0xff);
+	buf[2] = ((pr_info->gen >> 8) & 0xff);
+	buf[3] = (pr_info->gen & 0xff);
+
+	if (pr_info->rsv) {
+		buf[4] = ((add_len >> 24) & 0xff);
+		buf[5] = ((add_len >> 16) & 0xff);
+		buf[6] = ((add_len >> 8) & 0xff);
+		buf[7] = (add_len & 0xff);
+
+		if (buf_len < 22)
+			goto out;
+
+		if ((pr_info->rsv->type == PR_TYPE_WRITE_EXCLUSIVE_ALLREG) ||
+		    (pr_info->rsv->type == PR_TYPE_EXCLUSIVE_ACCESS_ALLREG)) {
+			/*
+			 * a) For a persistent reservation of the type Write
+			 *    Exclusive - All Registrants or Exclusive Access -
+			 *    All Registrants, the reservation key shall be set
+			 *    to zero; or
+			 */
+			pr_res_key = 0;
+		} else {
+			/*
+			 * b) For all other persistent reservation types, the
+			 *    reservation key shall be set to the registered
+			 *    reservation key for the I_T nexus that holds the
+			 *    persistent reservation.
+			 */
+			pr_res_key = pr_info->rsv->key;
+		}
+
+		buf[8] = ((pr_res_key >> 56) & 0xff);
+		buf[9] = ((pr_res_key >> 48) & 0xff);
+		buf[10] = ((pr_res_key >> 40) & 0xff);
+		buf[11] = ((pr_res_key >> 32) & 0xff);
+		buf[12] = ((pr_res_key >> 24) & 0xff);
+		buf[13] = ((pr_res_key >> 16) & 0xff);
+		buf[14] = ((pr_res_key >> 8) & 0xff);
+		buf[15] = (pr_res_key & 0xff);
+		/*
+		 * Set the SCOPE and TYPE
+		 */
+		buf[21] = (PR_SCOPE_LU_SCOPE & 0xf0) |
+			  (pr_info->rsv->type & 0x0f);
+	}
+
+out:
+	tcm_rbd_pr_info_free(pr_info);
+	return TCM_NO_SENSE;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_report_capabilities(struct se_cmd *cmd, unsigned char *buf,
+				       u32 buf_len)
+{
+	u16 add_len = 8; /* Hardcoded to 8. */
+
+	BUG_ON(buf_len < 6);
+
+	buf[0] = ((add_len >> 8) & 0xff);
+	buf[1] = (add_len & 0xff);
+	buf[2] |= 0x10; /* CRH: Compatible Reservation Handling bit. */
+	/* SIP_C=0 and ATP_C=0: no support for all_tg_pt/spec_i_pt */
+	buf[2] |= 0x01; /* PTPL_C: Persistence across Target Power Loss bit */
+	/*
+	 * We are filling in the PERSISTENT RESERVATION TYPE MASK below, so
+	 * set the TMV: Task Mask Valid bit.
+	 */
+	buf[3] |= 0x80;
+	/*
+	 * Change ALLOW COMMANDS to 0x20 or 0x40 later from Table 166
+	 */
+	buf[3] |= 0x10; /* ALLOW COMMANDS field 001b */
+	/*
+	 * PTPL_A: Persistence across Target Power Loss Active bit
+	 */
+	buf[3] |= 0x01;
+	/*
+	 * Setup the PERSISTENT RESERVATION TYPE MASK from Table 167
+	 */
+	buf[4] |= 0x80; /* PR_TYPE_EXCLUSIVE_ACCESS_ALLREG */
+	buf[4] |= 0x40; /* PR_TYPE_EXCLUSIVE_ACCESS_REGONLY */
+	buf[4] |= 0x20; /* PR_TYPE_WRITE_EXCLUSIVE_REGONLY */
+	buf[4] |= 0x08; /* PR_TYPE_EXCLUSIVE_ACCESS */
+	buf[4] |= 0x02; /* PR_TYPE_WRITE_EXCLUSIVE */
+	buf[5] |= 0x01; /* PR_TYPE_EXCLUSIVE_ACCESS_ALLREG */
+
+	return TCM_NO_SENSE;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_read_full_status(struct se_cmd *cmd, unsigned char *buf,
+				    u32 buf_len)
+{
+	pr_err("READ FULL STATUS not supported by RBD backend\n");
+	return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+}
+
+/* handle PR registration for a currently unregistered I_T nexus */
+static sense_reason_t
+tcm_rbd_execute_pr_register_new(struct tcm_rbd_pr_info *pr_info, u64 old_key,
+				u64 new_key, char *it_nexus,
+				bool ignore_existing)
+{
+	sense_reason_t ret;
+	int rc;
+
+	dout("PR registration for unregistered nexus: %s\n", it_nexus);
+
+	if (!ignore_existing && (old_key != 0)) {
+		ret = TCM_RESERVATION_CONFLICT;
+		goto out;
+	}
+	if (new_key == 0) {
+		ret = TCM_NO_SENSE;
+		goto out;
+	}
+	/*
+	 * Register the I_T nexus on which the command was received with
+	 * the value specified in the SERVICE ACTION RESERVATION KEY
+	 * field.
+	 */
+	rc = tcm_rbd_pr_info_append_reg(pr_info, it_nexus, new_key);
+	if (rc < 0) {
+		ret = TCM_OUT_OF_RESOURCES;
+		goto out;
+	}
+
+	ret = TCM_NO_SENSE;
+out:
+	return ret;
+}
+
+/* handle PR registration for a currently registered I_T nexus */
+static sense_reason_t
+tcm_rbd_execute_pr_register_existing(struct tcm_rbd_pr_info *pr_info,
+				     u64 old_key, u64 new_key, char *it_nexus,
+				     struct tcm_rbd_pr_reg *existing_reg,
+				     bool ignore_existing)
+{
+	sense_reason_t ret;
+	int rc;
+
+	dout("PR registration for registered nexus: %s\n", it_nexus);
+
+	if (!ignore_existing && (old_key != existing_reg->key)) {
+		ret = TCM_RESERVATION_CONFLICT;
+		goto out;
+	}
+
+	if (new_key == 0) {
+		/* unregister */
+		rc = tcm_rbd_pr_info_unregister_reg(pr_info,
+						    existing_reg);
+		if (rc < 0) {
+			ret = TCM_OUT_OF_RESOURCES;
+			goto out;
+		}
+	} else {
+		if( pr_info->rsv && pr_info->rsv->key == existing_reg->key ) {
+			/* update reservation key */
+			pr_info->rsv->key = new_key;
+		}
+		/* update registration key */
+		existing_reg->key = new_key;
+	}
+
+	ret = TCM_NO_SENSE;
+out:
+	return ret;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_register(struct se_cmd *cmd, u64 old_key,
+			    u64 new_key, bool aptpl, bool all_tg_pt,
+			    bool spec_i_pt, bool ignore_existing)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	char nexus_buf[TCM_RBD_PR_IT_NEXUS_MAXLEN];
+	struct tcm_rbd_pr_info *pr_info;
+	struct tcm_rbd_pr_reg *reg;
+	struct tcm_rbd_pr_reg *existing_reg;
+	char *pr_xattr;
+	int pr_xattr_len;
+	int rc;
+	sense_reason_t ret;
+	int retries = 0;
+
+	if (!cmd->se_sess || !cmd->se_lun) {
+		pr_err("SPC-3 PR: se_sess || struct se_lun is NULL!\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	if (!aptpl) {
+		/*
+		 * Currently unsupported by block layer API (hch):
+		 * reservations not persistent through a power loss are
+		 * basically useless, so I decided to force them on in the API.
+		 */
+		pr_warn("PR register with aptpl unset. Treating as aptpl=1\n");
+		aptpl = true;
+	}
+
+	if (all_tg_pt || spec_i_pt) {
+		/* TODO: Currently unsupported by block layer API. */
+		pr_err("failing PR register with all_tg_pt=%d spec_i_pt=%d\n",
+			 all_tg_pt, spec_i_pt);
+		return TCM_INVALID_CDB_FIELD;
+	}
+
+	rc = tcm_rbd_gen_it_nexus(cmd->se_sess, nexus_buf,
+				  ARRAY_SIZE(nexus_buf));
+	if (rc < 0)
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+
+	dout("generated nexus: %s\n", nexus_buf);
+
+retry:
+	pr_info = NULL;
+	pr_xattr = NULL;
+	pr_xattr_len = 0;
+	rc = tcm_rbd_pr_info_get(tcm_rbd_dev, &pr_info, &pr_xattr,
+				 &pr_xattr_len);
+	if ((rc == -ENODATA) && (retries == 0)) {
+		pr_warn("PR info not present, initializing\n");
+		rc = tcm_rbd_pr_info_init(tcm_rbd_dev, &pr_info, &pr_xattr,
+					  &pr_xattr_len);
+	}
+	if (rc < 0) {
+		pr_err("failed to obtain PR info\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	/* check for an existing registration */
+	existing_reg = NULL;
+	list_for_each_entry(reg, &pr_info->regs, regs_node) {
+		if (!strncmp(reg->it_nexus, nexus_buf, ARRAY_SIZE(nexus_buf))) {
+			dout("found existing PR reg for %s\n", nexus_buf);
+			existing_reg = reg;
+			break;
+		}
+	}
+
+	if (!existing_reg) {
+		ret = tcm_rbd_execute_pr_register_new(pr_info, old_key, new_key,
+						      nexus_buf,
+						      ignore_existing);
+	} else {
+		ret = tcm_rbd_execute_pr_register_existing(pr_info, old_key,
+							   new_key, nexus_buf,
+							   existing_reg,
+							   ignore_existing);
+	}
+	if (ret) {
+		goto err_out;
+	}
+
+	/*
+	 * The Persistent Reservations Generation (PRGENERATION) field shall
+	 * contain the value of a 32-bit wrapping counter that the device server
+	 * shall update (e.g., increment) during the processing of any
+	 * PERSISTENT RESERVE OUT command as described in table 216 (see
+	 * 6.16.2). The PRgeneration value shall not be updated by a PERSISTENT
+	 * RESERVE IN command or by a PERSISTENT RESERVE OUT command that is
+	 * terminated due to an error or reservation conflict.
+	 */
+	pr_info->gen++;
+	/*
+	 * TODO:
+	 * Regardless of the APTPL bit value the PRgeneration value shall be set
+	 * to zero by a power on.
+	 */
+
+	rc = tcm_rbd_pr_info_replace(tcm_rbd_dev, pr_xattr, pr_xattr_len,
+				     pr_info);
+	if (rc == -ECANCELED) {
+		char *pr_xattr_changed = NULL;
+		int pr_xattr_changed_len = 0;
+		/* PR info has changed since we read it */
+		rc = rbd_dev_getxattr(tcm_rbd_dev->rbd_dev,
+				      TCM_RBD_PR_INFO_XATTR_KEY,
+				      TCM_RBD_PR_INFO_XATTR_MAX_SIZE,
+				      (void **)&pr_xattr_changed,
+				      &pr_xattr_changed_len);
+		pr_warn("atomic PR info update failed due to parallel "
+			"change, expected(%d) %s, now(%d) %s\n",
+			pr_xattr_len, pr_xattr, pr_xattr_changed_len,
+			pr_xattr_changed);
+		retries++;
+		if (retries <= TCM_RBD_PR_REG_MAX_RETRIES) {
+			tcm_rbd_pr_info_free(pr_info);
+			kfree(pr_xattr);
+			goto retry;
+		}
+	}
+	if (rc < 0) {
+		pr_err("atomic PR info update failed: %d\n", rc);
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_out;
+	}
+
+	ret = TCM_NO_SENSE;
+err_out:
+	tcm_rbd_pr_info_free(pr_info);
+	kfree(pr_xattr);
+	return ret;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_reserve(struct se_cmd *cmd, int type, u64 key)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	char nexus_buf[TCM_RBD_PR_IT_NEXUS_MAXLEN];
+	struct tcm_rbd_pr_info *pr_info;
+	struct tcm_rbd_pr_reg *reg;
+	struct tcm_rbd_pr_reg *existing_reg;
+	char *pr_xattr;
+	int pr_xattr_len;
+	int rc;
+	sense_reason_t ret;
+	int retries = 0;
+
+	if (!cmd->se_sess || !cmd->se_lun) {
+		pr_err("SPC-3 PR: se_sess || struct se_lun is NULL!\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	rc = tcm_rbd_gen_it_nexus(cmd->se_sess, nexus_buf,
+				  ARRAY_SIZE(nexus_buf));
+	if (rc < 0)
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+
+retry:
+	pr_info = NULL;
+	pr_xattr = NULL;
+	pr_xattr_len = 0;
+	rc = tcm_rbd_pr_info_get(tcm_rbd_dev, &pr_info, &pr_xattr,
+				 &pr_xattr_len);
+	if (rc < 0) {
+		/* existing registration required for reservation */
+		pr_err("failed to obtain PR info\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	/* check for an existing registration */
+	existing_reg = NULL;
+	list_for_each_entry(reg, &pr_info->regs, regs_node) {
+		if (!strncmp(reg->it_nexus, nexus_buf, ARRAY_SIZE(nexus_buf))) {
+			dout("found existing PR reg for %s\n", nexus_buf);
+			existing_reg = reg;
+			break;
+		}
+	}
+
+	if (!existing_reg) {
+		pr_err("SPC-3 PR: Unable to locate registration for RESERVE\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	if (key != existing_reg->key) {
+		pr_err("SPC-3 PR RESERVE: Received res_key: 0x%016Lx"
+			" does not match existing SA REGISTER res_key:"
+			" 0x%016Lx\n", key, existing_reg->key);
+		ret = TCM_RESERVATION_CONFLICT;
+		goto err_info_free;
+	}
+
+	if (pr_info->rsv) {
+		if (!tcm_rbd_is_rsv_holder(pr_info->rsv, existing_reg, NULL)) {
+			pr_err("SPC-3 PR: Attempted RESERVE from %s while"
+			       " reservation already held by %s, returning"
+			       " RESERVATION_CONFLICT\n",
+			       nexus_buf, pr_info->rsv->it_nexus);
+			ret = TCM_RESERVATION_CONFLICT;
+			goto err_info_free;
+		}
+
+		if (pr_info->rsv->type != type) {
+			/* scope already checked */
+			pr_err("SPC-3 PR: Attempted RESERVE from %s trying to "
+			       "change TYPE, returning RESERVATION_CONFLICT\n",
+			       existing_reg->it_nexus);
+			ret = TCM_RESERVATION_CONFLICT;
+			goto err_info_free;
+		}
+
+		dout("reserve matches existing reservation, nothing to do\n");
+		goto done;
+	}
+
+	/* new reservation */
+	rc = tcm_rbd_pr_info_rsv_set(pr_info, key, nexus_buf, type);
+	if (rc < 0) {
+		pr_err("failed to set PR info reservation\n");
+		ret = TCM_OUT_OF_RESOURCES;
+		goto err_info_free;
+	}
+
+	rc = tcm_rbd_pr_info_replace(tcm_rbd_dev, pr_xattr, pr_xattr_len,
+				     pr_info);
+	if (rc == -ECANCELED) {
+		pr_warn("atomic PR info update failed due to parallel "
+			"change, expected(%d) %s. Retrying...\n",
+			pr_xattr_len, pr_xattr);
+		retries++;
+		if (retries <= TCM_RBD_PR_REG_MAX_RETRIES) {
+			tcm_rbd_pr_info_free(pr_info);
+			kfree(pr_xattr);
+			goto retry;
+		}
+	}
+	if (rc < 0) {
+		pr_err("atomic PR info update failed: %d\n", rc);
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_info_free;
+	}
+
+done:
+	ret = TCM_NO_SENSE;
+err_info_free:
+	tcm_rbd_pr_info_free(pr_info);
+	kfree(pr_xattr);
+	return ret;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_release(struct se_cmd *cmd, int type, u64 key)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	char nexus_buf[TCM_RBD_PR_IT_NEXUS_MAXLEN];
+	struct tcm_rbd_pr_info *pr_info;
+	struct tcm_rbd_pr_reg *reg;
+	struct tcm_rbd_pr_reg *existing_reg;
+	char *pr_xattr;
+	int pr_xattr_len;
+	int rc;
+	sense_reason_t ret;
+	int retries = 0;
+
+	if (!cmd->se_sess || !cmd->se_lun) {
+		pr_err("SPC-3 PR: se_sess || struct se_lun is NULL!\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	rc = tcm_rbd_gen_it_nexus(cmd->se_sess, nexus_buf,
+				  ARRAY_SIZE(nexus_buf));
+	if (rc < 0)
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+
+retry:
+	pr_info = NULL;
+	pr_xattr = NULL;
+	pr_xattr_len = 0;
+	rc = tcm_rbd_pr_info_get(tcm_rbd_dev, &pr_info, &pr_xattr,
+				 &pr_xattr_len);
+	if (rc < 0) {
+		/* existing registration required for release */
+		pr_err("failed to obtain PR info\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	if (!pr_info->rsv) {
+		/* no persistent reservation, return GOOD status */
+		goto done;
+	}
+
+	/* check for an existing registration */
+	existing_reg = NULL;
+	list_for_each_entry(reg, &pr_info->regs, regs_node) {
+		if (!strncmp(reg->it_nexus, nexus_buf, ARRAY_SIZE(nexus_buf))) {
+			dout("found existing PR reg for %s\n", nexus_buf);
+			existing_reg = reg;
+			break;
+		}
+	}
+
+	if (!existing_reg) {
+		pr_err("SPC-3 PR: Unable to locate registration for RELEASE\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	if (!tcm_rbd_is_rsv_holder(pr_info->rsv, existing_reg, NULL)) {
+		/* registered but not a persistent reservation holder */
+		goto done;
+	}
+
+	if (key != existing_reg->key) {
+		pr_err("SPC-3 PR RELEASE: Received res_key: 0x%016Lx"
+			" does not match existing SA REGISTER res_key:"
+			" 0x%016Lx\n", key, existing_reg->key);
+		ret = TCM_RESERVATION_CONFLICT;
+		goto err_info_free;
+	}
+
+	if (pr_info->rsv->type != type) {
+		pr_err("SPC-3 PR: Attempted RELEASE from %s with different "
+		       "TYPE, returning RESERVATION_CONFLICT\n",
+		       existing_reg->it_nexus);
+		ret = TCM_RESERVATION_CONFLICT;
+		goto err_info_free;
+	}
+
+	/* release the persistent reservation */
+	tcm_rbd_pr_info_rsv_clear(pr_info);
+
+	/*
+	 * TODO:
+	 * c) If the released persistent reservation is a registrants only type
+	 * or all registrants type persistent reservation,
+	 *    the device server shall establish a unit attention condition for
+	 *    the initiator port associated with every regis-
+	 *    tered I_T nexus other than I_T nexus on which the PERSISTENT
+	 *    RESERVE OUT command with RELEASE service action was received,
+	 *    with the additional sense code set to RESERVATIONS RELEASED
+	 */
+
+	rc = tcm_rbd_pr_info_replace(tcm_rbd_dev, pr_xattr, pr_xattr_len,
+				     pr_info);
+	if (rc == -ECANCELED) {
+		pr_warn("atomic PR info update failed due to parallel "
+			"change, expected(%d) %s. Retrying...\n",
+			pr_xattr_len, pr_xattr);
+		retries++;
+		if (retries <= TCM_RBD_PR_REG_MAX_RETRIES) {
+			tcm_rbd_pr_info_free(pr_info);
+			kfree(pr_xattr);
+			goto retry;
+		}
+	}
+	if (rc < 0) {
+		pr_err("atomic PR info update failed: %d\n", rc);
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_info_free;
+	}
+
+done:
+	ret = TCM_NO_SENSE;
+err_info_free:
+	tcm_rbd_pr_info_free(pr_info);
+	kfree(pr_xattr);
+	return ret;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_clear(struct se_cmd *cmd, u64 key)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	char nexus_buf[TCM_RBD_PR_IT_NEXUS_MAXLEN];
+	struct tcm_rbd_pr_info *pr_info;
+	struct tcm_rbd_pr_reg *reg;
+	struct tcm_rbd_pr_reg *existing_reg;
+	char *pr_xattr;
+	int pr_xattr_len;
+	int rc;
+	sense_reason_t ret;
+	int retries = 0;
+
+	if (!cmd->se_sess || !cmd->se_lun) {
+		pr_err("SPC-3 PR: se_sess || struct se_lun is NULL!\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	rc = tcm_rbd_gen_it_nexus(cmd->se_sess, nexus_buf,
+				  ARRAY_SIZE(nexus_buf));
+	if (rc < 0)
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+
+retry:
+	pr_info = NULL;
+	pr_xattr = NULL;
+	pr_xattr_len = 0;
+	rc = tcm_rbd_pr_info_get(tcm_rbd_dev, &pr_info, &pr_xattr,
+				 &pr_xattr_len);
+	if (rc < 0) {
+		/* existing registration required for clear */
+		pr_err("failed to obtain PR info\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	/* check for an existing registration */
+	existing_reg = NULL;
+	list_for_each_entry(reg, &pr_info->regs, regs_node) {
+		if (!strncmp(reg->it_nexus, nexus_buf, ARRAY_SIZE(nexus_buf))) {
+			dout("found existing PR reg for %s\n", nexus_buf);
+			existing_reg = reg;
+			break;
+		}
+	}
+
+	if (!existing_reg) {
+		pr_err("SPC-3 PR: Unable to locate registration for CLEAR\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	if (key != existing_reg->key) {
+		pr_err("SPC-3 PR CLEAR: Received res_key: 0x%016Lx"
+			" does not match existing SA REGISTER res_key:"
+			" 0x%016Lx\n", key, existing_reg->key);
+		ret = TCM_RESERVATION_CONFLICT;
+		goto err_info_free;
+	}
+
+	/* release the persistent reservation, if any */
+	if (pr_info->rsv)
+		tcm_rbd_pr_info_rsv_clear(pr_info);
+
+	/* remove all registrations */
+	list_for_each_entry_safe(existing_reg, reg, &pr_info->regs, regs_node) {
+		tcm_rbd_pr_info_clear_reg(pr_info, existing_reg);
+	}
+
+	/*
+	 * TODO:
+	 * e) Establish a unit attention condition for the initiator
+	 *    port associated with every registered I_T nexus other
+	 *    than the I_T nexus on which the PERSISTENT RESERVE OUT
+	 *    command with CLEAR service action was received, with the
+	 *    additional sense code set to RESERVATIONS PREEMPTED.
+	 */
+
+	/* PR generation must be incremented on successful CLEAR */
+	pr_info->gen++;
+
+	rc = tcm_rbd_pr_info_replace(tcm_rbd_dev, pr_xattr, pr_xattr_len,
+				     pr_info);
+	if (rc == -ECANCELED) {
+		pr_warn("atomic PR info update failed due to parallel "
+			"change, expected(%d) %s. Retrying...\n",
+			pr_xattr_len, pr_xattr);
+		retries++;
+		if (retries <= TCM_RBD_PR_REG_MAX_RETRIES) {
+			tcm_rbd_pr_info_free(pr_info);
+			kfree(pr_xattr);
+			goto retry;
+		}
+	}
+	if (rc < 0) {
+		pr_err("atomic PR info update failed: %d\n", rc);
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_info_free;
+	}
+
+	ret = TCM_NO_SENSE;
+err_info_free:
+	tcm_rbd_pr_info_free(pr_info);
+	kfree(pr_xattr);
+	return ret;
+}
+
+static int
+tcm_rbd_pr_info_rm_regs_key(struct tcm_rbd_pr_info *pr_info,
+			    struct tcm_rbd_pr_reg *existing_reg,
+			    u64 new_key)
+{
+	struct tcm_rbd_pr_reg *reg;
+	struct tcm_rbd_pr_reg *reg_n;
+	bool found = false;
+
+	if (new_key == 0) {
+		dout("removing all non-nexus regs\n");
+	}
+
+	list_for_each_entry_safe(reg, reg_n, &pr_info->regs, regs_node) {
+		if (new_key && (reg->key != new_key))
+			continue;
+
+		found = true;
+		if (reg == existing_reg)
+			continue;
+
+		tcm_rbd_pr_info_clear_reg(pr_info, reg);
+
+		/* TODO flag UA if different IT nexus */
+	}
+
+	if (!found) {
+		return -ENOENT;
+	}
+
+	return 0;
+}
+
+/*
+ * Preempt logic is pretty complex. This implementation attempts to resemble
+ * SPC4r37 Figure 9 — Device server interpretation of PREEMPT service action.
+ */
+static sense_reason_t
+tcm_rbd_execute_pr_preempt(struct se_cmd *cmd, u64 old_key, u64 new_key,
+			   int type, bool abort)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	char nexus_buf[TCM_RBD_PR_IT_NEXUS_MAXLEN];
+	struct tcm_rbd_pr_info *pr_info;
+	struct tcm_rbd_pr_rsv *rsv;
+	struct tcm_rbd_pr_reg *reg;
+	struct tcm_rbd_pr_reg *existing_reg;
+	bool all_reg;
+	char *pr_xattr;
+	int pr_xattr_len;
+	int rc;
+	sense_reason_t ret;
+	int retries = 0;
+
+	if (!cmd->se_sess || !cmd->se_lun) {
+		pr_err("SPC-3 PR: se_sess || struct se_lun is NULL!\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	if (abort) {
+		pr_err("PR PREEMPT AND ABORT not supported by RBD backend\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	rc = tcm_rbd_gen_it_nexus(cmd->se_sess, nexus_buf,
+				  ARRAY_SIZE(nexus_buf));
+	if (rc < 0)
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+
+retry:
+	pr_info = NULL;
+	pr_xattr = NULL;
+	pr_xattr_len = 0;
+	all_reg = false;
+	rc = tcm_rbd_pr_info_get(tcm_rbd_dev, &pr_info, &pr_xattr,
+				 &pr_xattr_len);
+	if (rc == -ENODATA) {
+		pr_err("SPC-3 PR: no registrations for PREEMPT\n");
+		return TCM_RESERVATION_CONFLICT;
+	} else if (rc < 0) {
+		pr_err("failed to obtain PR info\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	/* check for an existing registration */
+	existing_reg = NULL;
+	list_for_each_entry(reg, &pr_info->regs, regs_node) {
+		if (!strncmp(reg->it_nexus, nexus_buf, ARRAY_SIZE(nexus_buf))) {
+			dout("found existing PR reg for %s\n", nexus_buf);
+			existing_reg = reg;
+			break;
+		}
+	}
+
+	if (!existing_reg) {
+		pr_err("SPC-3 PR: Unable to locate registration for PREEMPT\n");
+		ret = TCM_RESERVATION_CONFLICT;
+		goto err_info_free;
+	}
+
+	if (old_key != existing_reg->key) {
+		pr_err("SPC-3 PR PREEMPT: Received res_key: 0x%016Lx"
+			" does not match existing SA REGISTER res_key:"
+			" 0x%016Lx\n", old_key, existing_reg->key);
+		ret = TCM_RESERVATION_CONFLICT;
+		goto err_info_free;
+	}
+
+	if (!pr_info->rsv) {
+		/* no reservation, remove regs indicated by new_key */
+		if (new_key == 0) {
+			ret = TCM_INVALID_PARAMETER_LIST;
+			goto err_info_free;
+		}
+		rc = tcm_rbd_pr_info_rm_regs_key(pr_info, existing_reg,
+						 new_key);
+		if (rc == -ENOENT) {
+			ret = TCM_RESERVATION_CONFLICT;
+			goto err_info_free;
+		} else if (rc < 0) {
+			ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+			goto err_info_free;
+		}
+		goto commit;
+	}
+
+
+	rsv = pr_info->rsv;
+	if ((rsv->type == PR_TYPE_WRITE_EXCLUSIVE_ALLREG)
+			|| (rsv->type == PR_TYPE_EXCLUSIVE_ACCESS_ALLREG)) {
+		all_reg = true;
+	}
+
+	if (all_reg) {
+		/* if key is zero, then remove all non-nexus regs */
+		rc = tcm_rbd_pr_info_rm_regs_key(pr_info, existing_reg,
+						 new_key);
+		if (rc == -ENOENT) {
+			ret = TCM_RESERVATION_CONFLICT;
+			goto err_info_free;
+		} else if (rc < 0) {
+			ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+			goto err_info_free;
+		}
+
+		if (new_key == 0) {
+			tcm_rbd_pr_info_rsv_clear(pr_info);
+			rsv = NULL;
+			rc = tcm_rbd_pr_info_rsv_set(pr_info, existing_reg->key,
+						     existing_reg->it_nexus, type);
+			if (rc < 0) {
+				pr_err("failed to set PR info reservation\n");
+				ret = TCM_OUT_OF_RESOURCES;
+				goto err_info_free;
+			}
+		}
+		goto commit;
+	}
+
+	if (rsv->key != new_key) {
+		if (new_key == 0) {
+			ret = TCM_INVALID_PARAMETER_LIST;
+			goto err_info_free;
+		}
+		rc = tcm_rbd_pr_info_rm_regs_key(pr_info, existing_reg,
+						 new_key);
+		if (rc == -ENOENT) {
+			ret = TCM_RESERVATION_CONFLICT;
+			goto err_info_free;
+		} else if (rc < 0) {
+			ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+			goto err_info_free;
+		}
+		goto commit;
+	}
+
+	rc = tcm_rbd_pr_info_rm_regs_key(pr_info, existing_reg, new_key);
+	/* Allow pre-emption by current reservation holder
+	if (rc == -ENOENT) {
+		ret = TCM_RESERVATION_CONFLICT;
+		goto err_info_free;
+	} else if (rc < 0) {
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_info_free;
+	}
+	*/
+	tcm_rbd_pr_info_rsv_clear(pr_info);
+	rsv = NULL;
+	rc = tcm_rbd_pr_info_rsv_set(pr_info, existing_reg->key,
+				     existing_reg->it_nexus, type);
+	if (rc < 0) {
+		pr_err("failed to set PR info reservation\n");
+		ret = TCM_OUT_OF_RESOURCES;
+		goto err_info_free;
+	}
+commit:
+	/* PR generation must be incremented on successful PREEMPT */
+	pr_info->gen++;
+
+	rc = tcm_rbd_pr_info_replace(tcm_rbd_dev, pr_xattr, pr_xattr_len,
+				     pr_info);
+	if (rc == -ECANCELED) {
+		pr_warn("atomic PR info update failed due to parallel "
+			"change, expected(%d) %s. Retrying...\n",
+			pr_xattr_len, pr_xattr);
+		retries++;
+		if (retries <= TCM_RBD_PR_REG_MAX_RETRIES) {
+			tcm_rbd_pr_info_free(pr_info);
+			kfree(pr_xattr);
+			goto retry;
+		}
+	}
+	if (rc < 0) {
+		pr_err("atomic PR info update failed: %d\n", rc);
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_info_free;
+	}
+
+	ret = TCM_NO_SENSE;
+err_info_free:
+	tcm_rbd_pr_info_free(pr_info);
+	kfree(pr_xattr);
+	return ret;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_register_and_move(struct se_cmd *cmd, u64 old_key,
+				     u64 new_key, bool aptpl, int unreg)
+{
+	pr_err("REGISTER AND MOVE not supported by RBD backend\n");
+	return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_scsi2_check_scsi3_conflict(struct tcm_rbd_pr_info *pr_info,
+					      char *it_nexus)
+{
+	struct tcm_rbd_pr_rsv *rsv = pr_info->rsv;
+
+	if (rsv) {
+		struct tcm_rbd_pr_reg *reg;
+
+		/*
+		 * spc4r17
+		 * 5.12.3 Exceptions to SPC-2 RESERVE and RELEASE behavior
+		 * A RESERVE(6) command or RESERVE(10) command shall complete
+		 * with GOOD status, but no reservation shall be established and
+		 * the persistent reservation shall not be changed, if the
+		 * command is received from:
+		 * a) an I_T nexus that is a persistent reservation holder; or
+		 * b) an I_T nexus that is registered if a registrants only or
+		 *    all registrants type persistent reservation is present.
+		 */
+		list_for_each_entry(reg, &pr_info->regs, regs_node) {
+			if (strncmp(reg->it_nexus, it_nexus,
+						ARRAY_SIZE(reg->it_nexus))) {
+				continue;
+			}
+			dout("SCSI2 RESERVE from PR registrant: %s\n",
+			     it_nexus);
+			/* ALLREG types checked by tcm_rbd_is_rsv_holder() */
+			if (tcm_rbd_is_rsv_holder(rsv, reg, NULL)
+			      || ((rsv->type == PR_TYPE_WRITE_EXCLUSIVE_REGONLY)
+			  || (rsv->type == PR_TYPE_EXCLUSIVE_ACCESS_REGONLY))) {
+				return 1;
+			}
+		}
+	}
+
+	if (pr_info->num_regs > 0) {
+		/*
+		 * Following spc2r20 5.5.1 Reservations overview:
+		 *
+		 * If a logical unit has executed a PERSISTENT RESERVE OUT
+		 * command with the REGISTER or the REGISTER AND IGNORE
+		 * EXISTING KEY service action and is still registered by any
+		 * initiator, all RESERVE commands and all RELEASE commands
+		 * regardless of initiator shall conflict and shall terminate
+		 * with a RESERVATION CONFLICT status.
+		 */
+		pr_err("Received legacy SPC-2 RESERVE/RELEASE"
+			" while active SPC-3 registrations exist,"
+			" returning RESERVATION_CONFLICT\n");
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_scsi2_reserve(struct se_cmd *cmd)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	char nexus_buf[TCM_RBD_PR_IT_NEXUS_MAXLEN];
+	struct tcm_rbd_pr_info *pr_info;
+	char *pr_xattr;
+	int pr_xattr_len;
+	int rc;
+	sense_reason_t ret;
+	int retries = 0;
+
+	if (!cmd->se_sess || !cmd->se_lun) {
+		pr_err("SCSI2 RESERVE: se_sess || struct se_lun is NULL!\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	rc = tcm_rbd_gen_it_nexus(cmd->se_sess, nexus_buf,
+				  ARRAY_SIZE(nexus_buf));
+	if (rc < 0)
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+
+retry:
+	pr_info = NULL;
+	pr_xattr = NULL;
+	pr_xattr_len = 0;
+	rc = tcm_rbd_pr_info_get(tcm_rbd_dev, &pr_info, &pr_xattr,
+				 &pr_xattr_len);
+	if ((rc == -ENODATA) && (retries == 0)) {
+		pr_warn("PR info not present, initializing\n");
+		rc = tcm_rbd_pr_info_init(tcm_rbd_dev, &pr_info, &pr_xattr,
+					  &pr_xattr_len);
+	}
+	if (rc < 0) {
+		pr_err("failed to obtain PR info\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	rc = tcm_rbd_execute_pr_scsi2_check_scsi3_conflict(pr_info, nexus_buf);
+	if (rc == -EBUSY) {
+		ret = TCM_RESERVATION_CONFLICT;
+		goto err_info_free;
+	} else if (rc < 0) {
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_info_free;
+	} else if (rc == 1) {
+		/* return GOOD without processing request */
+		goto done;
+	}
+
+	if (pr_info->scsi2_rsv) {
+		if (strncmp(pr_info->scsi2_rsv->it_nexus,
+			    nexus_buf, ARRAY_SIZE(nexus_buf))) {
+			dout("SCSI2 reservation conflict: held by %s\n",
+			     pr_info->scsi2_rsv->it_nexus);
+			ret = TCM_RESERVATION_CONFLICT;
+			goto err_info_free;
+		}
+		dout("SCSI2 reservation already held by %s\n",
+		     nexus_buf);
+		goto done;
+	}
+
+	dout("new SCSI2 reservation\n");
+	ret = tcm_rbd_pr_info_scsi2_rsv_set(pr_info, nexus_buf);
+	if (ret < 0) {
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_info_free;
+	}
+
+	rc = tcm_rbd_pr_info_replace(tcm_rbd_dev, pr_xattr, pr_xattr_len,
+				     pr_info);
+	if (rc == -ECANCELED) {
+		pr_warn("atomic PR info update failed due to parallel "
+			"change, expected(%d) %s. Retrying...\n",
+			pr_xattr_len, pr_xattr);
+		retries++;
+		if (retries <= TCM_RBD_PR_REG_MAX_RETRIES) {
+			tcm_rbd_pr_info_free(pr_info);
+			kfree(pr_xattr);
+			goto retry;
+		}
+	}
+	if (rc < 0) {
+		pr_err("atomic PR info update failed: %d\n", rc);
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_info_free;
+	}
+
+done:
+	ret = TCM_NO_SENSE;
+err_info_free:
+	tcm_rbd_pr_info_free(pr_info);
+	kfree(pr_xattr);
+	return ret;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_scsi2_release(struct se_cmd *cmd)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	char nexus_buf[TCM_RBD_PR_IT_NEXUS_MAXLEN];
+	struct tcm_rbd_pr_info *pr_info;
+	char *pr_xattr;
+	int pr_xattr_len;
+	int rc;
+	sense_reason_t ret;
+	int retries = 0;
+
+	if (!cmd->se_sess || !cmd->se_lun) {
+		pr_err("SCSI2 RESERVE: se_sess || struct se_lun is NULL!\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	rc = tcm_rbd_gen_it_nexus(cmd->se_sess, nexus_buf,
+				  ARRAY_SIZE(nexus_buf));
+	if (rc < 0)
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+
+retry:
+	pr_info = NULL;
+	pr_xattr = NULL;
+	pr_xattr_len = 0;
+	rc = tcm_rbd_pr_info_get(tcm_rbd_dev, &pr_info, &pr_xattr,
+				 &pr_xattr_len);
+	if ((rc == -ENODATA) && (retries == 0)) {
+		dout("PR info not present for SCSI2 release\n");
+		return TCM_NO_SENSE;
+	}
+	if (rc < 0) {
+		pr_err("failed to obtain PR info\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	rc = tcm_rbd_execute_pr_scsi2_check_scsi3_conflict(pr_info, nexus_buf);
+	if (rc == -EBUSY) {
+		ret = TCM_RESERVATION_CONFLICT;
+		goto err_info_free;
+	} else if (rc < 0) {
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_info_free;
+	} else if (rc == 1) {
+		/* return GOOD without processing request */
+		goto done;
+	}
+
+	if (!pr_info->scsi2_rsv || strncmp(pr_info->scsi2_rsv->it_nexus,
+					   nexus_buf, ARRAY_SIZE(nexus_buf))) {
+		dout("SCSI2 release against non-matching reservation\n");
+		goto done;
+	}
+
+	tcm_rbd_pr_info_scsi2_rsv_clear(pr_info);
+
+	rc = tcm_rbd_pr_info_replace(tcm_rbd_dev, pr_xattr, pr_xattr_len,
+				     pr_info);
+	if (rc == -ECANCELED) {
+		pr_warn("atomic PR info update failed due to parallel "
+			"change, expected(%d) %s. Retrying...\n",
+			pr_xattr_len, pr_xattr);
+		retries++;
+		if (retries <= TCM_RBD_PR_REG_MAX_RETRIES) {
+			tcm_rbd_pr_info_free(pr_info);
+			kfree(pr_xattr);
+			goto retry;
+		}
+	}
+	if (rc < 0) {
+		pr_err("atomic PR info update failed: %d\n", rc);
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_info_free;
+	}
+
+done:
+	ret = TCM_NO_SENSE;
+err_info_free:
+	tcm_rbd_pr_info_free(pr_info);
+	kfree(pr_xattr);
+	return ret;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_check_scsi2_conflict(struct tcm_rbd_pr_info *pr_info,
+					char *it_nexus,
+					enum target_pr_check_type type)
+{
+	if (!pr_info->scsi2_rsv) {
+		dout("no SCSI2 reservation\n");
+		return TCM_NO_SENSE;
+	}
+
+	if (type == TARGET_PR_CHECK_SCSI2_ANY) {
+		dout("SCSI2 reservation conflict: %s with ANY\n",
+		     it_nexus);
+		return TCM_RESERVATION_CONFLICT;
+	}
+
+	if (strncmp(pr_info->scsi2_rsv->it_nexus, it_nexus,
+		    ARRAY_SIZE(pr_info->scsi2_rsv->it_nexus))) {
+		dout("SCSI2 reservation conflict: %s with %s holder\n",
+		     it_nexus, pr_info->scsi2_rsv->it_nexus);
+		return TCM_RESERVATION_CONFLICT;
+	}
+
+	return TCM_NO_SENSE;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_check_scsi3_conflict(struct se_cmd *cmd,
+					struct tcm_rbd_pr_info *pr_info,
+					char *it_nexus)
+{
+	struct tcm_rbd_pr_rsv *rsv;
+	struct tcm_rbd_pr_reg *reg;
+	bool registered_nexus;
+	int rc;
+
+	if (!pr_info->rsv) {
+		dout("no SCSI3 persistent reservation\n");
+		return TCM_NO_SENSE;
+	}
+
+	rsv = pr_info->rsv;
+	dout("PR reservation holder: %s, us: %s\n", rsv->it_nexus, it_nexus);
+
+	if (!strncmp(rsv->it_nexus, it_nexus, ARRAY_SIZE(rsv->it_nexus))) {
+		dout("cmd is from reservation holder\n");
+		return TCM_NO_SENSE;
+	}
+
+	registered_nexus = false;
+	list_for_each_entry(reg, &pr_info->regs, regs_node) {
+		if (!strncmp(reg->it_nexus, it_nexus,
+						ARRAY_SIZE(reg->it_nexus))) {
+			dout("cmd is from PR registrant: %s\n", it_nexus);
+			registered_nexus = true;
+			break;
+		}
+	}
+	rc = core_scsi3_pr_seq_non_holder(cmd, rsv->type, it_nexus,
+					  registered_nexus);
+	if (rc == 1) {
+		dout("SCSI3 reservation conflict\n");
+		return TCM_RESERVATION_CONFLICT;
+	} else if (rc < 0) {
+		pr_warn("SCSI3 PR non-holder check failed\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	return TCM_NO_SENSE;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_check_conflict(struct se_cmd *cmd,
+				  enum target_pr_check_type type)
+{
+	struct se_device *dev = cmd->se_dev;
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	struct tcm_rbd_pr_info *pr_info;
+	char nexus_buf[TCM_RBD_PR_IT_NEXUS_MAXLEN];
+	int rc;
+	sense_reason_t ret;
+
+	switch (cmd->t_task_cdb[0]) {
+	case INQUIRY:
+	case RELEASE:
+	case RELEASE_10:
+		/* always allow cdb execution */
+		return TCM_NO_SENSE;
+	default:
+		break;
+	}
+
+	rc = tcm_rbd_pr_info_get(tcm_rbd_dev, &pr_info, NULL, NULL);
+	if (rc == -ENODATA) {
+		dout("no PR info, can't conflict\n");
+		return TCM_NO_SENSE;
+	}
+	if (rc < 0) {
+		/* existing registration required for reservation */
+		pr_err("failed to obtain PR info\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	rc = tcm_rbd_gen_it_nexus(cmd->se_sess, nexus_buf,
+				  ARRAY_SIZE(nexus_buf));
+	if (rc < 0) {
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto out_info_free;
+	}
+
+	ret = tcm_rbd_execute_pr_check_scsi2_conflict(pr_info, nexus_buf, type);
+	if (ret || (type == TARGET_PR_CHECK_SCSI2_ANY)) {
+		/* SCSI2 conflict/failure, or caller only interested in SCSI2 */
+		goto out_info_free;
+	}
+
+	ret = tcm_rbd_execute_pr_check_scsi3_conflict(cmd, pr_info, nexus_buf);
+	if (ret)
+		goto out_info_free;
+
+	ret = TCM_NO_SENSE;
+out_info_free:
+	tcm_rbd_pr_info_free(pr_info);
+	return ret;
+}
+
+static sense_reason_t
+tcm_rbd_execute_pr_reset(struct se_device *dev)
+{
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	struct tcm_rbd_pr_info *pr_info;
+	char *pr_xattr;
+	int pr_xattr_len;
+	int rc;
+	sense_reason_t ret;
+	int retries = 0;
+
+retry:
+	pr_info = NULL;
+	pr_xattr = NULL;
+	pr_xattr_len = 0;
+	rc = tcm_rbd_pr_info_get(tcm_rbd_dev, &pr_info, &pr_xattr,
+				 &pr_xattr_len);
+	if ((rc == -ENODATA) && (retries == 0)) {
+		dout("PR info not present for reset\n");
+		return TCM_NO_SENSE;
+	}
+	if (rc < 0) {
+		pr_err("failed to obtain PR info\n");
+		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+	}
+
+	if (!pr_info->scsi2_rsv) {
+		dout("no SCSI2 reservation to clear for reset");
+		goto done;
+	}
+
+	tcm_rbd_pr_info_scsi2_rsv_clear(pr_info);
+
+	rc = tcm_rbd_pr_info_replace(tcm_rbd_dev, pr_xattr, pr_xattr_len,
+				     pr_info);
+	if (rc == -ECANCELED) {
+		pr_warn("atomic PR info update failed due to parallel "
+			"change, expected(%d) %s. Retrying...\n",
+			pr_xattr_len, pr_xattr);
+		retries++;
+		if (retries <= TCM_RBD_PR_REG_MAX_RETRIES) {
+			tcm_rbd_pr_info_free(pr_info);
+			kfree(pr_xattr);
+			goto retry;
+		}
+	}
+	if (rc < 0) {
+		pr_err("atomic PR info update failed: %d\n", rc);
+		ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+		goto err_info_free;
+	}
+
+	dout("cleared SCSI2 reservation on reset\n");
+
+done:
+	ret = TCM_NO_SENSE;
+err_info_free:
+	tcm_rbd_pr_info_free(pr_info);
+	kfree(pr_xattr);
+	return ret;
+}
+
+void petasan_pr_debug(char* command,struct tcm_rbd_pr_info *pr_info,char* nexus_buf,int val, u64 key)
+{
+	struct tcm_rbd_pr_reg *reg;
+	pr_debug("petasan_pr_debug ----------------------------------------- \n");
+	pr_debug("petasan_pr_debug command:%s   \n",command);
+	pr_debug("petasan_pr_debug nexus_buf:%s   \n",nexus_buf);
+	pr_debug("petasan_pr_debug val:%d   \n",val);
+	pr_debug("petasan_pr_debug key:0x%016Lx   \n",key);
+	if ( pr_info->rsv) {
+		pr_debug("petasan_pr_debug existing pr type:%d it_nexus:%s  key:0x%016Lx \n",pr_info->rsv->type,pr_info->rsv->it_nexus,pr_info->rsv->key);
+	}
+
+	list_for_each_entry(reg, &pr_info->regs, regs_node) {
+		pr_debug("petasan_pr_debug existing reg it_nexus:%s  key:0x%016Lx   \n",reg->it_nexus, reg->key);
+	}
+
+	pr_debug("petasan_pr_debug ----------------------------------------- \n\n");
+}
+
+static struct target_pr_ops tcm_rbd_pr_ops = {
+	.check_conflict		= tcm_rbd_execute_pr_check_conflict,
+	.scsi2_reserve		= tcm_rbd_execute_pr_scsi2_reserve,
+	.scsi2_release		= tcm_rbd_execute_pr_scsi2_release,
+	.reset			= tcm_rbd_execute_pr_reset,
+
+	.pr_read_keys		= tcm_rbd_execute_pr_read_keys,
+	.pr_read_reservation	= tcm_rbd_execute_pr_read_reservation,
+	.pr_report_capabilities	= tcm_rbd_execute_pr_report_capabilities,
+	.pr_read_full_status	= tcm_rbd_execute_pr_read_full_status,
+
+	.pr_register		= tcm_rbd_execute_pr_register,
+	.pr_reserve		= tcm_rbd_execute_pr_reserve,
+	.pr_release		= tcm_rbd_execute_pr_release,
+	.pr_clear		= tcm_rbd_execute_pr_clear,
+	.pr_preempt		= tcm_rbd_execute_pr_preempt,
+	.pr_register_and_move	= tcm_rbd_execute_pr_register_and_move,
+};
+
 static struct target_backend_ops tcm_rbd_ops = {
 	.name			= "rbd",
 	.inquiry_prod		= "RBD",
@@ -913,6 +3399,7 @@ static struct target_backend_ops tcm_rbd
 	.get_io_min		= tcm_rbd_get_io_min,
 	.get_io_opt		= tcm_rbd_get_io_opt,
 	.get_write_cache	= tcm_rbd_get_write_cache,
+	.pr_ops				= &tcm_rbd_pr_ops,
 };
 
 static ssize_t tcm_rbd_emulate_legacy_capacity_show(struct config_item *item,
@@ -948,6 +3435,135 @@ static ssize_t tcm_rbd_emulate_legacy_ca
 }
 CONFIGFS_ATTR(tcm_rbd_, emulate_legacy_capacity);
 
+/* detects legacy capacity block -1:error 0:not found 1:found*/
+static int tcm_rbd_detect_legacy_capacity(struct se_device *dev) 
+{
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	struct rbd_device *rbd_dev = tcm_rbd_dev->rbd_dev;
+    
+	struct rbd_img_request *img_request;
+	u64 offset;    
+	u64 length;    
+	u64 mapping_size;
+	int result;    
+	struct page *page;
+	struct bio_vec bvec;
+	struct ceph_file_extent img_extent;
+	struct tcm_rbd_sync_notify sync_notify = {
+		0,
+		COMPLETION_INITIALIZER_ONSTACK(sync_notify.c),
+	};
+	int ret;
+    
+	down_read(&rbd_dev->header_rwsem);
+	mapping_size = rbd_dev->mapping.size;
+	up_read(&rbd_dev->header_rwsem); 
+	offset = mapping_size;   
+	length = dev->dev_attrib.block_size;
+	if( length == 0)
+		/* dev_attrib.block_size may not be configured yet */
+		length = 4096;
+    
+	pr_debug("%s mapping_size::%llu offset:%llu length:%llu PAGE_SIZE:%lu\n",__func__, mapping_size, 
+			 offset, length, PAGE_SIZE);    
+        
+	if( PAGE_SIZE < length) {
+		pr_err("%s insufficent page size\n",__func__);
+		return -1;
+	}    
+	img_extent.fe_off = offset;
+	img_extent.fe_len = length;
+    
+	page = alloc_page(GFP_KERNEL);
+	bvec.bv_page = page;
+	bvec.bv_offset = 0;
+	bvec.bv_len = length;
+
+	img_request = rbd_img_request_create(rbd_dev, OBJ_OP_READ, tcm_rbd_sync_callback);
+	result = rbd_img_fill_from_bvecs(img_request,&img_extent, 1,&bvec);
+	img_request->lio_cmd_data = &sync_notify;
+	rbd_img_handle_request(img_request, 0);
+	wait_for_completion(&sync_notify.c);
+	result = sync_notify.result;
+	pr_debug("%s result:%d obj_read_enoent:%d\n",__func__,result, img_request->obj_read_enoent);
+        
+	if( result == 0 ) {
+		struct ceph_object_extent *ex;
+		int obj_req_count = 0;
+		void *p = page_address(page);
+		__u8 *bytes = (__u8 *)p;
+		int debug_print_len = 16;
+
+		list_for_each_entry(ex, &img_request->object_extents, oe_item) {
+			obj_req_count++;
+			pr_debug("%s oe_objno:%llu oe_off:%llu oe_len:%llu\n",__func__, 
+					ex->oe_objno, ex->oe_off, ex->oe_len);
+		}   
+        
+		pr_debug("%s data read:",__func__);
+		while(debug_print_len) {
+			pr_cont(" %02x", *bytes);
+			bytes++;
+			debug_print_len--;
+		}
+		pr_cont("\n");
+    
+		if( img_request->obj_read_enoent == obj_req_count )
+			ret = 0; /* all obj read requests returned ENOENT*/
+		else
+			ret = 1;           
+	}
+	else
+		ret = -1;
+
+	__free_page(page);    
+	rbd_img_request_destroy(img_request);
+    
+	pr_debug("%s ret:%d\n",__func__,ret);
+	return ret;
+}
+
+
+static int tcm_rbd_read_legacy_capacity_xattr(struct se_device *dev)
+{
+	#define TCM_RBD_LEGACY_CAPACITY_XATTR_KEY "legacy_capacity"
+	#define TCM_RBD_LEGACY_CAPACITY_XATTR_MAX_SIZE 80
+	
+	struct tcm_rbd_dev *tcm_rbd_dev = TCM_RBD_DEV(dev);
+	struct rbd_device *rbd_dev = tcm_rbd_dev->rbd_dev;
+	char buf[80];
+	char *p_xattr = NULL;
+	int p_xattr_len;
+	int rc;
+	
+	rc = rbd_dev_getxattr(rbd_dev, TCM_RBD_LEGACY_CAPACITY_XATTR_KEY, 
+			TCM_RBD_LEGACY_CAPACITY_XATTR_MAX_SIZE -1, (void **)&p_xattr, &p_xattr_len);
+	if( rc ) {
+		if( rc == -ENODATA ) {
+			pr_debug("%s No xattr found\n",__func__);
+			return 0;
+		}
+		pr_warn("failed to obtain legacy_capacity xattr: %d\n", rc);
+		return -1;
+	}
+
+	if( p_xattr_len < 1 ||  TCM_RBD_LEGACY_CAPACITY_XATTR_MAX_SIZE -1 < p_xattr_len ) {
+		pr_warn("legacy_capacity xattr incorrect length: %d\n", p_xattr_len);
+		return -1;
+	}
+	
+	if(!strncmp(p_xattr,"true",p_xattr_len) ) {
+		pr_debug("setting legacy_capacity xattr to true\n");		
+		tcm_rbd_dev->emulate_legacy_capacity = true;
+	}
+	else if(!strncmp(p_xattr,"false",p_xattr_len) ) {
+		pr_debug("setting legacy_capacity xattr to false\n");		
+		tcm_rbd_dev->emulate_legacy_capacity = false;
+	}
+	kfree(p_xattr);
+	return 0;
+}
+
 static struct configfs_attribute *tcm_rbd_attrib_attrs[] = {
 	&tcm_rbd_attr_emulate_legacy_capacity,
 	NULL,
@@ -993,6 +3609,23 @@ static void __exit tcm_rbd_module_exit(v
 	kfree(tcm_rbd_ops.tb_dev_attrib_attrs);
 }
 
+static void tcm_rbd_warn(struct rbd_device *rbd_dev, const char *fmt, ...)
+{
+	struct va_format vaf;
+	va_list args;
+	char rbd_dev_name[80];
+	/* struct rbd_device *rbd_dev = TCM_RBD_DEV(dev)->rbd_dev; */
+	va_start(args, fmt);
+	vaf.fmt = fmt;
+	vaf.va = &args;	
+	
+	rbd_dev_name[0] = 0;	
+	if (rbd_dev && rbd_dev->spec && rbd_dev->spec->image_name) 
+		snprintf(rbd_dev_name, sizeof(rbd_dev_name), "%s", rbd_dev->spec->image_name);	
+	printk(KERN_WARNING "image %s: %pV\n",rbd_dev->spec->image_name, &vaf);
+	va_end(args);
+}
+
 MODULE_DESCRIPTION("TCM Ceph RBD subsystem plugin");
 MODULE_AUTHOR("Mike Christie");
 MODULE_LICENSE("GPL");
diff -uprN a/drivers/target/target_core_tmr.c b/drivers/target/target_core_tmr.c
--- a/drivers/target/target_core_tmr.c	2024-04-24 01:22:34.572816065 +0200
+++ b/drivers/target/target_core_tmr.c	2024-04-24 01:23:17.431494041 +0200
@@ -391,15 +391,23 @@ int core_tmr_lun_reset(
 	 * Clear any legacy SPC-2 reservation when called during
 	 * LOGICAL UNIT RESET
 	 */
-	if (!preempt_and_abort_list &&
-	     (dev->dev_reservation_flags & DRF_SPC2_RESERVATIONS)) {
-		spin_lock(&dev->dev_reservation_lock);
-		dev->reservation_holder = NULL;
-		dev->dev_reservation_flags &= ~DRF_SPC2_RESERVATIONS;
-		spin_unlock(&dev->dev_reservation_lock);
-		pr_debug("LUN_RESET: SCSI-2 Released reservation\n");
-	}
+	if (!preempt_and_abort_list) {
+		if (dev->transport->pr_ops && dev->transport->pr_ops->reset) {
+			sense_reason_t ret;
 
+			ret = dev->transport->pr_ops->reset(dev);
+			if (ret != TCM_NO_SENSE) {
+				pr_err("LUN_RESET: failed to release "
+				       "reservations: %u\n", ret);
+			}
+		} else if (dev->dev_reservation_flags & DRF_SPC2_RESERVATIONS) {
+			spin_lock(&dev->dev_reservation_lock);
+			dev->reservation_holder = NULL;
+			dev->dev_reservation_flags &= ~DRF_SPC2_RESERVATIONS;
+			spin_unlock(&dev->dev_reservation_lock);
+			pr_debug("LUN_RESET: SCSI-2 Released reservation\n");
+		}
+	}
 	atomic_long_inc(&dev->num_resets);
 
 	pr_debug("LUN_RESET: %s for [%s] Complete\n",
diff -uprN a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c
--- a/drivers/target/target_core_transport.c	2024-04-24 01:22:34.572816065 +0200
+++ b/drivers/target/target_core_transport.c	2024-04-24 01:23:17.571502673 +0200
@@ -752,8 +752,7 @@ static void target_complete_failure_work
 {
 	struct se_cmd *cmd = container_of(work, struct se_cmd, work);
 
-	transport_generic_request_failure(cmd,
-			TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE);
+	transport_generic_request_failure(cmd, cmd->sense_reason);
 }
 
 /*
@@ -871,7 +870,8 @@ static bool target_cmd_interrupted(struc
 }
 
 /* May be called from interrupt context so must not sleep. */
-void target_complete_cmd(struct se_cmd *cmd, u8 scsi_status)
+static void __target_complete_cmd(struct se_cmd *cmd, u8 scsi_status,
+				  sense_reason_t sense_reason)
 {
 	struct se_wwn *wwn = cmd->se_sess->se_tpg->se_tpg_wwn;
 	int success, cpu;
@@ -881,7 +881,8 @@ void target_complete_cmd(struct se_cmd *
 		return;
 
 	cmd->scsi_status = scsi_status;
-
+	cmd->sense_reason = sense_reason;
+	
 	spin_lock_irqsave(&cmd->t_state_lock, flags);
 	switch (cmd->scsi_status) {
 	case SAM_STAT_CHECK_CONDITION:
@@ -909,8 +910,22 @@ void target_complete_cmd(struct se_cmd *
 
 	queue_work_on(cpu, target_completion_wq, &cmd->work);
 }
+
+void target_complete_cmd(struct se_cmd *cmd, u8 scsi_status)
+{
+	__target_complete_cmd(cmd, scsi_status, scsi_status ?
+			     TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE :
+			     TCM_NO_SENSE);
+}
 EXPORT_SYMBOL(target_complete_cmd);
 
+void target_complete_cmd_with_sense(struct se_cmd *cmd,
+				    sense_reason_t sense_reason)
+{
+	__target_complete_cmd(cmd, SAM_STAT_CHECK_CONDITION, sense_reason);
+}
+EXPORT_SYMBOL(target_complete_cmd_with_sense);
+
 void target_set_cmd_data_length(struct se_cmd *cmd, int length)
 {
 	if (length < cmd->data_length) {
@@ -2029,6 +2044,7 @@ void transport_generic_request_failure(s
 	case TCM_TOO_MANY_SEGMENT_DESCS:
 	case TCM_UNSUPPORTED_SEGMENT_DESC_TYPE_CODE:
 	case TCM_INVALID_FIELD_IN_COMMAND_IU:
+	case TCM_MISCOMPARE_VERIFY:
 		break;
 	case TCM_OUT_OF_RESOURCES:
 		cmd->scsi_status = SAM_STAT_TASK_SET_FULL;
diff -uprN a/drivers/target/target_core_xcopy.c b/drivers/target/target_core_xcopy.c
--- a/drivers/target/target_core_xcopy.c	2024-04-24 01:22:34.572816065 +0200
+++ b/drivers/target/target_core_xcopy.c	2024-04-24 01:23:17.675509086 +0200
@@ -33,16 +33,16 @@ static struct workqueue_struct *xcopy_wq
 
 static sense_reason_t target_parse_xcopy_cmd(struct xcopy_op *xop);
 
-/**
- * target_xcopy_locate_se_dev_e4_iter - compare XCOPY NAA device identifiers
- *
- * @se_dev: device being considered for match
- * @dev_wwn: XCOPY requested NAA dev_wwn
- * @return: 1 on match, 0 on no-match
- */
+struct xcopy_dev_search_info {
+	const unsigned char *dev_wwn;
+	struct se_device *found_dev;
+};
+
+
 static int target_xcopy_locate_se_dev_e4_iter(struct se_device *se_dev,
-					      const unsigned char *dev_wwn)
+					      void *data)
 {
+	struct xcopy_dev_search_info *info = data;
 	unsigned char tmp_dev_wwn[XCOPY_NAA_IEEE_REGEX_LEN];
 	int rc;
 
@@ -53,63 +53,46 @@ static int target_xcopy_locate_se_dev_e4
 
 	memset(&tmp_dev_wwn[0], 0, XCOPY_NAA_IEEE_REGEX_LEN);
 	spc_gen_naa_6h_vendor_specific(se_dev, &tmp_dev_wwn[0]);
-
-	rc = memcmp(&tmp_dev_wwn[0], dev_wwn, XCOPY_NAA_IEEE_REGEX_LEN);
+	
+	rc = memcmp(&tmp_dev_wwn[0], info->dev_wwn, XCOPY_NAA_IEEE_REGEX_LEN);
 	if (rc != 0) {
 		pr_debug("XCOPY: skip non-matching: %*ph\n",
 			 XCOPY_NAA_IEEE_REGEX_LEN, tmp_dev_wwn);
 		return 0;
 	}
+
+	info->found_dev = se_dev;
 	pr_debug("XCOPY 0xe4: located se_dev: %p\n", se_dev);
 
+	rc = target_depend_item(&se_dev->dev_group.cg_item);
+	if (rc != 0) {
+		pr_err("configfs_depend_item attempt failed: %d for se_dev: %p\n",
+		       rc, se_dev);
+		return rc;
+	}
+
+	pr_debug("Called configfs_depend_item for se_dev: %p se_dev->se_dev_group: %p\n",
+		 se_dev, &se_dev->dev_group);
 	return 1;
 }
 
-static int target_xcopy_locate_se_dev_e4(struct se_session *sess,
-					const unsigned char *dev_wwn,
-					struct se_device **_found_dev,
-					struct percpu_ref **_found_lun_ref)
-{
-	struct se_dev_entry *deve;
-	struct se_node_acl *nacl;
-	struct se_lun *this_lun = NULL;
-	struct se_device *found_dev = NULL;
-
-	/* cmd with NULL sess indicates no associated $FABRIC_MOD */
-	if (!sess)
-		goto err_out;
-
-	pr_debug("XCOPY 0xe4: searching for: %*ph\n",
-		 XCOPY_NAA_IEEE_REGEX_LEN, dev_wwn);
-
-	nacl = sess->se_node_acl;
-	rcu_read_lock();
-	hlist_for_each_entry_rcu(deve, &nacl->lun_entry_hlist, link) {
-		struct se_device *this_dev;
-		int rc;
-
-		this_lun = rcu_dereference(deve->se_lun);
-		this_dev = rcu_dereference_raw(this_lun->lun_se_dev);
-
-		rc = target_xcopy_locate_se_dev_e4_iter(this_dev, dev_wwn);
-		if (rc) {
-			if (percpu_ref_tryget_live(&this_lun->lun_ref))
-				found_dev = this_dev;
-			break;
-		}
+static int target_xcopy_locate_se_dev_e4(const unsigned char *dev_wwn,
+					struct se_device **found_dev)
+{
+	struct xcopy_dev_search_info info;
+	int ret;
+
+	memset(&info, 0, sizeof(info));
+	info.dev_wwn = dev_wwn;
+
+	ret = target_for_each_device(target_xcopy_locate_se_dev_e4_iter, &info);
+	if (ret == 1) {
+		*found_dev = info.found_dev;
+		return 0;
+	} else {
+		pr_debug_ratelimited("Unable to locate 0xe4 descriptor for EXTENDED_COPY\n");
+		return -EINVAL;
 	}
-	rcu_read_unlock();
-	if (found_dev == NULL)
-		goto err_out;
-
-	pr_debug("lun_ref held for se_dev: %p se_dev->se_dev_group: %p\n",
-		 found_dev, &found_dev->dev_group);
-	*_found_dev = found_dev;
-	*_found_lun_ref = &this_lun->lun_ref;
-	return 0;
-err_out:
-	pr_debug_ratelimited("Unable to locate 0xe4 descriptor for EXTENDED_COPY\n");
-	return -EINVAL;
 }
 
 static int target_xcopy_parse_tiddesc_e4(struct se_cmd *se_cmd, struct xcopy_op *xop,
@@ -256,16 +239,12 @@ static int target_xcopy_parse_target_des
 
 	switch (xop->op_origin) {
 	case XCOL_SOURCE_RECV_OP:
-		rc = target_xcopy_locate_se_dev_e4(se_cmd->se_sess,
-						xop->dst_tid_wwn,
-						&xop->dst_dev,
-						&xop->remote_lun_ref);
+		rc = target_xcopy_locate_se_dev_e4(xop->dst_tid_wwn,
+						&xop->dst_dev);
 		break;
 	case XCOL_DEST_RECV_OP:
-		rc = target_xcopy_locate_se_dev_e4(se_cmd->se_sess,
-						xop->src_tid_wwn,
-						&xop->src_dev,
-						&xop->remote_lun_ref);
+		rc = target_xcopy_locate_se_dev_e4(xop->src_tid_wwn,
+						&xop->src_dev);
 		break;
 	default:
 		pr_err("XCOPY CSCD descriptor IDs not found in CSCD list - "
@@ -405,12 +384,18 @@ static int xcopy_pt_get_cmd_state(struct
 
 static void xcopy_pt_undepend_remotedev(struct xcopy_op *xop)
 {
+	struct se_device *remote_dev;
+
 	if (xop->op_origin == XCOL_SOURCE_RECV_OP)
-		pr_debug("putting dst lun_ref for %p\n", xop->dst_dev);
+		remote_dev = xop->dst_dev;
 	else
-		pr_debug("putting src lun_ref for %p\n", xop->src_dev);
+		remote_dev = xop->src_dev;
 
-	percpu_ref_put(xop->remote_lun_ref);
+	pr_debug("Calling configfs_undepend_item for"
+		  " remote_dev: %p remote_dev->dev_group: %p\n",
+		  remote_dev, &remote_dev->dev_group.cg_item);
+
+	target_undepend_item(&remote_dev->dev_group.cg_item);
 }
 
 static void xcopy_pt_release_cmd(struct se_cmd *se_cmd)
@@ -675,12 +660,17 @@ static void target_xcopy_do_work(struct
 	int rc = 0;
 	unsigned short nolb, max_nolb, copied_nolb = 0;
 
-	if (target_parse_xcopy_cmd(xop) != TCM_NO_SENSE)
-		goto err_free;
-
-	if (WARN_ON_ONCE(!xop->src_dev) || WARN_ON_ONCE(!xop->dst_dev))
-		goto err_free;
-
+	sense_reason_t sense_rc;
+ 
+	sense_rc = target_parse_xcopy_cmd(xop);
+	if (sense_rc != TCM_NO_SENSE)
+ 		goto err_free;
+ 
+	if (WARN_ON_ONCE(!xop->src_dev) || WARN_ON_ONCE(!xop->dst_dev)) {
+		sense_rc = TCM_INVALID_PARAMETER_LIST;
+ 		goto err_free;
+	}
+ 
 	src_dev = xop->src_dev;
 	dst_dev = xop->dst_dev;
 	src_lba = xop->src_lba;
@@ -762,20 +752,20 @@ static void target_xcopy_do_work(struct
 	return;
 
 out:
+	/*
+	 * The XCOPY command was aborted after some data was transferred.
+	 * Terminate command with CHECK CONDITION status, with the sense key
+	 * set to COPY ABORTED.
+	 */
+	sense_rc = TCM_COPY_TARGET_DEVICE_NOT_REACHABLE;
 	xcopy_pt_undepend_remotedev(xop);
 	target_free_sgl(xop->xop_data_sg, xop->xop_data_nents);
 
 err_free:
 	kfree(xop);
-	/*
-	 * Don't override an error scsi status if it has already been set
-	 */
-	if (ec_cmd->scsi_status == SAM_STAT_GOOD) {
-		pr_warn_ratelimited("target_xcopy_do_work: rc: %d, Setting X-COPY"
-			" CHECK_CONDITION -> sending response\n", rc);
-		ec_cmd->scsi_status = SAM_STAT_CHECK_CONDITION;
-	}
-	target_complete_cmd(ec_cmd, ec_cmd->scsi_status);
+	pr_warn_ratelimited("target_xcopy_do_work: rc: %d, sense: %u, XCOPY operation failed\n",
+			   rc, sense_rc);
+	target_complete_cmd_with_sense(ec_cmd, sense_rc);
 }
 
 /*
diff -uprN a/drivers/target/target_core_xcopy.h b/drivers/target/target_core_xcopy.h
--- a/drivers/target/target_core_xcopy.h	2024-04-24 01:22:34.572816065 +0200
+++ b/drivers/target/target_core_xcopy.h	2024-04-24 01:23:17.555501687 +0200
@@ -5,7 +5,7 @@
 #define XCOPY_TARGET_DESC_LEN		32
 #define XCOPY_SEGMENT_DESC_LEN		28
 #define XCOPY_NAA_IEEE_REGEX_LEN	16
-#define XCOPY_MAX_SECTORS		4096
+#define XCOPY_MAX_SECTORS		8192
 
 /*
  * SPC4r37 6.4.6.1
diff -uprN a/include/linux/ceph/librbd.h b/include/linux/ceph/librbd.h
--- a/include/linux/ceph/librbd.h	2024-04-24 01:22:34.564815558 +0200
+++ b/include/linux/ceph/librbd.h	2024-04-24 01:23:17.715511552 +0200
@@ -129,6 +129,7 @@ struct rbd_img_request {
 	struct pending_result	pending;
 	struct work_struct	work;
 	int			work_result;
+	int obj_read_enoent;
 };
 
 enum rbd_watch_state {
@@ -227,6 +228,16 @@ struct rbd_device {
 	/* sysfs related */
 	struct device		dev;
 	unsigned long		open_count;	/* protected by lock */
+
+	/* pr */
+	struct mutex		pr_mutex;
+	char			*pr_cached;	/* cached pr string */ 
+	bool			pr_dirty;
+	unsigned long 		pr_cache_ts;		
+	
+	/* block writes during snapshots */
+	atomic_t		inflight_write_requests;
+	atomic_t		block_writes;	
 };
 
 /*
@@ -256,4 +267,19 @@ extern int rbd_img_fill_cmp_and_write_fr
 extern void rbd_img_handle_request(struct rbd_img_request *img_req, int result);
 extern void rbd_img_request_destroy(struct rbd_img_request *img_request);
 
+extern int rbd_dev_setxattr(struct rbd_device *rbd_dev, char *key, void *val,
+			    int val_len);
+extern int rbd_dev_cmpsetxattr(struct rbd_device *rbd_dev, char *key,
+			       void *oldval, int oldval_len, void *newval,
+			       int newval_len);
+extern int rbd_dev_getxattr(struct rbd_device *rbd_dev, char *key, int max_val_len,
+			    void **_val, int *val_len);
+extern void rbd_notify_scsi_pr_update(struct rbd_device *rbd_dev);
+
+inline bool rbd_dev_block_writes(struct rbd_device *rbd_dev)
+{
+	return atomic_read(&rbd_dev->block_writes);
+}
+extern void rbd_dev_wait_on_bloked_writes(struct rbd_device *rbd_dev, u32 sec);
+
 #endif
diff -uprN a/include/linux/ceph/osd_client.h b/include/linux/ceph/osd_client.h
--- a/include/linux/ceph/osd_client.h	2024-04-24 01:22:34.568815811 +0200
+++ b/include/linux/ceph/osd_client.h	2024-04-24 01:23:17.387491328 +0200
@@ -105,7 +105,8 @@ struct ceph_osd_req_op {
 			u32 value_len;
 			__u8 cmp_op;       /* CEPH_OSD_CMPXATTR_OP_* */
 			__u8 cmp_mode;     /* CEPH_OSD_CMPXATTR_MODE_* */
-			struct ceph_osd_data osd_data;
+			struct ceph_osd_data request_data;
+			struct ceph_osd_data response_data;
 		} xattr;
 		struct {
 			const char *class_name;
@@ -562,5 +563,11 @@ int ceph_osdc_list_watchers(struct ceph_
 			    struct ceph_object_locator *oloc,
 			    struct ceph_watch_item **watchers,
 			    u32 *num_watchers);
+
+extern void osd_req_op_xattr_response_data_pages(struct ceph_osd_request *,
+					unsigned int which,
+					struct page **pages, u64 length,
+					u32 alignment, bool pages_from_pool,
+					bool own_pages);
 #endif
 
diff -uprN a/include/target/target_core_backend.h b/include/target/target_core_backend.h
--- a/include/target/target_core_backend.h	2024-04-24 01:22:34.564815558 +0200
+++ b/include/target/target_core_backend.h	2024-04-24 01:23:17.571502673 +0200
@@ -17,6 +17,44 @@
 struct request_queue;
 struct scatterlist;
 
+enum target_pr_check_type {
+	/* check for *any* SCSI2 reservations, including own */
+	TARGET_PR_CHECK_SCSI2_ANY,
+	/* check for conflicting SCSI2 or SCSI3 reservation */
+	TARGET_PR_CHECK_SCSI2_SCSI3,
+};
+
+struct target_pr_ops {
+	sense_reason_t (*check_conflict)(struct se_cmd *cmd,
+					 enum target_pr_check_type);
+	sense_reason_t (*scsi2_reserve)(struct se_cmd *cmd);
+	sense_reason_t (*scsi2_release)(struct se_cmd *cmd);
+	sense_reason_t (*reset)(struct se_device *dev);
+	/* persistent reservation (out) API attempts to mirror block layer */
+	sense_reason_t (*pr_register)(struct se_cmd *cmd, u64 old_key,
+				      u64 new_key, bool aptpl, bool all_tg_pt,
+				      bool spec_i_pt, bool ignore_existing);
+	sense_reason_t (*pr_reserve)(struct se_cmd *cmd, int type, u64 key);
+	sense_reason_t (*pr_release)(struct se_cmd *cmd, int type, u64 key);
+	sense_reason_t (*pr_clear)(struct se_cmd *cmd, u64 key);
+	sense_reason_t (*pr_preempt)(struct se_cmd *cmd, u64 old_key,
+				     u64 new_key, int type, bool abort);
+	sense_reason_t (*pr_register_and_move)(struct se_cmd *cmd, u64 old_key,
+					       u64 new_key, bool aptpl,
+					       int unreg);
+	/* persistent reservation (in) API not proposed for block layer yet */
+	sense_reason_t (*pr_read_keys)(struct se_cmd *cmd, unsigned char *buf,
+				       u32 buf_len);
+	sense_reason_t (*pr_read_reservation)(struct se_cmd *cmd,
+					      unsigned char *buf, u32 buf_len);
+	sense_reason_t (*pr_report_capabilities)(struct se_cmd *cmd,
+						 unsigned char *buf,
+						 u32 buf_len);
+	sense_reason_t (*pr_read_full_status)(struct se_cmd *cmd,
+					      unsigned char *buf, u32 buf_len);
+};
+
+
 struct target_backend_ops {
 	char name[16];
 	char inquiry_prod[16];
@@ -57,6 +95,9 @@ struct target_backend_ops {
 	int (*format_prot)(struct se_device *);
 	void (*free_prot)(struct se_device *);
 
+	/* backend reservation hooks */
+	struct target_pr_ops *pr_ops;
+
 	struct configfs_attribute **tb_dev_attrib_attrs;
 	struct configfs_attribute **tb_dev_action_attrs;
 };
@@ -76,6 +117,7 @@ void	target_backend_unregister(const str
 void	target_complete_cmd(struct se_cmd *, u8);
 void	target_set_cmd_data_length(struct se_cmd *, int);
 void	target_complete_cmd_with_length(struct se_cmd *, u8, int);
+void	target_complete_cmd_with_sense(struct se_cmd *, sense_reason_t);
 
 void	transport_copy_sense_to_cmd(struct se_cmd *, unsigned char *);
 
diff -uprN a/include/target/target_core_base.h b/include/target/target_core_base.h
--- a/include/target/target_core_base.h	2024-04-24 01:22:34.564815558 +0200
+++ b/include/target/target_core_base.h	2024-04-24 01:23:17.575502919 +0200
@@ -542,6 +542,7 @@ struct se_cmd {
 	unsigned int		t_prot_nents;
 	sense_reason_t		pi_err;
 	u64			sense_info;
+	sense_reason_t		sense_reason;	
 	/*
 	 * CPU LIO will execute the cmd on. Defaults to the CPU the cmd is
 	 * initialized on. Drivers can override.
diff -uprN a/net/ceph/osd_client.c b/net/ceph/osd_client.c
--- a/net/ceph/osd_client.c	2024-04-24 01:22:34.572816065 +0200
+++ b/net/ceph/osd_client.c	2024-04-24 01:23:17.387491328 +0200
@@ -331,6 +331,18 @@ void osd_req_op_cls_response_data_pages(
 }
 EXPORT_SYMBOL(osd_req_op_cls_response_data_pages);
 
+void osd_req_op_xattr_response_data_pages(struct ceph_osd_request *osd_req,
+			unsigned int which, struct page **pages, u64 length,
+			u32 alignment, bool pages_from_pool, bool own_pages)
+{
+	struct ceph_osd_data *osd_data;
+
+	osd_data = osd_req_op_data(osd_req, which, xattr, response_data);
+	ceph_osd_data_pages_init(osd_data, pages, length, alignment,
+				pages_from_pool, own_pages);
+}
+EXPORT_SYMBOL(osd_req_op_xattr_response_data_pages);
+
 static u64 ceph_osd_data_length(struct ceph_osd_data *osd_data)
 {
 	switch (osd_data->type) {
@@ -388,7 +400,11 @@ static void osd_req_op_data_release(stru
 		break;
 	case CEPH_OSD_OP_SETXATTR:
 	case CEPH_OSD_OP_CMPXATTR:
-		ceph_osd_data_release(&op->xattr.osd_data);
+		ceph_osd_data_release(&op->xattr.request_data);
+		break;
+	case CEPH_OSD_OP_GETXATTR:
+		ceph_osd_data_release(&op->xattr.request_data);
+		ceph_osd_data_release(&op->xattr.response_data);
 		break;
 	case CEPH_OSD_OP_STAT:
 		ceph_osd_data_release(&op->raw_data_in);
@@ -676,6 +692,7 @@ static void get_num_data_items(struct ce
 			break;
 
 		/* both */
+		case CEPH_OSD_OP_GETXATTR:
 		case CEPH_OSD_OP_NOTIFY:
 			*num_request_data_items += 1;
 			*num_reply_data_items += 1;
@@ -847,37 +864,42 @@ int osd_req_op_xattr_init(struct ceph_os
 {
 	struct ceph_osd_req_op *op = osd_req_op_init(osd_req, which,
 						     opcode, 0);
-	struct ceph_pagelist *pagelist;
+	struct ceph_pagelist *req_pagelist;
 	size_t payload_len;
 	int ret;
 
-	BUG_ON(opcode != CEPH_OSD_OP_SETXATTR && opcode != CEPH_OSD_OP_CMPXATTR);
+	BUG_ON(opcode != CEPH_OSD_OP_SETXATTR
+		&& opcode != CEPH_OSD_OP_CMPXATTR
+		&& opcode != CEPH_OSD_OP_GETXATTR);
 
-	pagelist = ceph_pagelist_alloc(GFP_NOFS);
-	if (!pagelist)
+	req_pagelist = ceph_pagelist_alloc(GFP_NOFS);
+	if (!req_pagelist)
 		return -ENOMEM;
 
 	payload_len = strlen(name);
 	op->xattr.name_len = payload_len;
-	ret = ceph_pagelist_append(pagelist, name, payload_len);
-	if (ret)
+	ret = ceph_pagelist_append(req_pagelist, name, payload_len);
+	if (ret) {
 		goto err_pagelist_free;
-
-	op->xattr.value_len = size;
-	ret = ceph_pagelist_append(pagelist, value, size);
-	if (ret)
-		goto err_pagelist_free;
-	payload_len += size;
+	}
+	
+	if (value) {
+		op->xattr.value_len = size;
+		ret = ceph_pagelist_append(req_pagelist, value, size);
+		if (ret)
+			goto err_pagelist_free;
+		payload_len += size;
+	}
 
 	op->xattr.cmp_op = cmp_op;
 	op->xattr.cmp_mode = cmp_mode;
 
-	ceph_osd_data_pagelist_init(&op->xattr.osd_data, pagelist);
+	ceph_osd_data_pagelist_init(&op->xattr.request_data, req_pagelist);
 	op->indata_len = payload_len;
 	return 0;
 
 err_pagelist_free:
-	ceph_pagelist_release(pagelist);
+	ceph_pagelist_release(req_pagelist);
 	return ret;
 }
 EXPORT_SYMBOL(osd_req_op_xattr_init);
@@ -1007,6 +1029,7 @@ static u32 osd_req_encode_op(struct ceph
 		break;
 	case CEPH_OSD_OP_SETXATTR:
 	case CEPH_OSD_OP_CMPXATTR:
+	case CEPH_OSD_OP_GETXATTR:
 		dst->xattr.name_len = cpu_to_le32(src->xattr.name_len);
 		dst->xattr.value_len = cpu_to_le32(src->xattr.value_len);
 		dst->xattr.cmp_op = src->xattr.cmp_op;
@@ -2007,7 +2030,7 @@ static void setup_request_data(struct ce
 			WARN_ON(op->indata_len != op->xattr.name_len +
 						  op->xattr.value_len);
 			ceph_osdc_msg_data_add(request_msg,
-					       &op->xattr.osd_data);
+					       &op->xattr.request_data);
 			break;
 		case CEPH_OSD_OP_NOTIFY_ACK:
 			ceph_osdc_msg_data_add(request_msg,
@@ -2052,6 +2075,12 @@ static void setup_request_data(struct ce
 			ceph_osdc_msg_data_add(reply_msg,
 					       &op->notify.response_data);
 			break;
+		case CEPH_OSD_OP_GETXATTR:
+			WARN_ON(op->indata_len != op->xattr.name_len);
+			ceph_osdc_msg_data_add(request_msg,
+					       &op->xattr.request_data);
+			ceph_osdc_msg_data_add(reply_msg,
+					       &op->xattr.response_data);
 		}
 	}
 }
